Pattern changes caused by commit: 21aef92905678f98675fc34aeb4edb0e34a1d6af

From: Decorator-1
To:   Decorator-2

From: Flyweight-1
To:   Flyweight-2

From: Mediator-1
To:   Mediator-3

From: Strategy-1
To:   Strategy-0

From: Template Method-2
To:   Template Method-3


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6960.txt 

commit 21aef92905678f98675fc34aeb4edb0e34a1d6af
Author: Yuki Morishita <yukim@apache.org>

    directly stream compressed sstable with java nio. patch by yukim,
    reveiwed by jbellis for CASSANDRA-4297



==================================
 Issue CASSANDRA-4297 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4297] Use java NIO as much as possible when streaming compressed SSTables
-----------------

-----------------
Summary: Use java NIO as much as possible when streaming compressed SSTables
-----------------

-----------------
Issue type: Improvement
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Wed, 30 May 2012 17:50:49 +0000
-----------------

-----------------
Resolved at: Wed, 13 Jun 2012 17:28:30 +0000
-----------------

-----------------
Assigned to: Yuki Morishita
-----------------

-----------------
Description: 

Back in 0.8, streaming uses java NIO (FileChannel#transferTo/transferFrom) to perform
zero copy file transfer between nodes. Since 1.0, in order to add new features like
sstable compression and internode encryption we had to switch to java IO
Input/OutputStreams. What we currently do to transfer compressed SSTable is, in source
node, 1) decompress chunk in SSTable, 2) compress using LZF for network, and in
destination node, 3) decompress using LZF as reading from socket, 4) compress for SSTable
on disk.

Now, 1.1 comes out with SSTable compression turned on by default. It is
reasonable to transfer compressed file as is using NIO instead of decompress/compress in
source node.
 

-----------------

-----------------
Comments: 

New Comment: 
I've pushed working commit to <a href="https://github.com/yukim/cassandra/tree/4297"
class="external-link" rel="nofollow">https://github.com/yukim/cassandra/tree/4297</a>.When
streaming compressed files, source node appends compression info to stream header, and
dest node uses that info to decompress data from stream.<br/>If inter-node encryption is
turned on, then zero copy transfer cannot be performed, so in that case we fall back to
current way of streaming.I ran simple bulk loading test which transfers several compressed
SSTables between nodes. Although overall throughput and time took to complete streaming is
about the same, patched version reduced CPU usage (20% -&gt; 2%) on source node. Most of
the time was spent on source node waiting for dest node to decompress and write to disk.I
still don't know if this is useful in production, so if someone can perform more realistic
tests, I'm greatly appreciated. 


New Comment: 
<blockquote>Most of the time was spent on source node waiting for dest node to decompress
and write to disk</blockquote>It looks like the only reason to decompress is to compare
crc32...  is that right?  Why did we crc uncompressed data instead of compressed?  Should
we introduce a new version of snappy compression that CRCs the compressed data instead? 


New Comment: 
<blockquote>It looks like the only reason to decompress is to compare crc32... is that
right?</blockquote>No. We decompress because we need to build secondary indexes, compute
the sstable stats, clean counters delta, etc... 


New Comment: 
Can you break this down a bit for me?  CompressedFST looks straightforward, but what is
CompressedDIS doing? 


New Comment: 
CompressedDIS is DataInputStream version of CompressedRandomAccessReader. It reads
compressed chunks directly from stream and provides decompressed data  while reading from
stream.  CRC check is also performed after decompressing chunk based on crc_chance setting
in compression option, which is default to 1.0 or 100%, as done in CRAR. 


New Comment: 
I have to brush up my patch around progress  to update correctly. Will post updated
version soon. 


New Comment: 
Attaching patch for review(also updated <a
href="https://github.com/yukim/cassandra/tree/4297" class="external-link"
rel="nofollow">https://github.com/yukim/cassandra/tree/4297</a>).<ul>	<li>removed
unnecessary changes from trunk</li>	<li>corrected progress reporting on dest
node</li>	<li>added some comments</li></ul> 


New Comment: 
Comments:<ul class="alternate" type="square">	<li>Would prefer to have CDIS just implement
IS, and let callers wrap in DIS when desired, similar to how we use SnappyInputStream in
IncomingTcpConnection, or LZFInputStream in ISR</li>	<li>Why the changes to
OutboundTcpConnection?</li>	<li>Re MS changes: when would header.file be
null?</li>	<li>Chunk[] sort can use Guava Longs.compare</li>	<li>I suggest adding a
comment to explain why sort is necessary (b/c ranges are from replication strategy, so may
not be sorted?)</li>	<li>Instead of using Set + copy into array, why not use an ArrayList
+ trimToSize()</li>	<li>is the FST comment <tt>// TODO just use a raw RandomAccessFile
since we're managing our own buffer here</tt> obsolete?</li>	<li>is the
CompressedRandomAccessReader path used at all in FST anymore?</li>	<li>Nit: avoid double
negation in if statements with else clauses, e.g. instead of<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">. 
         <span class="code-keyword">if</span> (remoteFile.compressionInfo != <span
class="code-keyword">null</span>)                dis = <span
class="code-keyword">new</span> CompressedDataInputStream(socket.getInputStream(),
remoteFile.compressionInfo);            <span class="code-keyword">else</span>            
   dis = <span class="code-keyword">new</span> DataInputStream(<span
class="code-keyword">new</span>
LZFInputStream(socket.getInputStream()));</pre></div></div>prefer<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">[ 
         <span class="code-keyword">if</span> (remoteFile.compressionInfo == <span
class="code-keyword">null</span>)                dis = <span
class="code-keyword">new</span> DataInputStream(<span class="code-keyword">new</span>
LZFInputStream(socket.getInputStream()));            <span
class="code-keyword">else</span>                dis = <span
class="code-keyword">new</span> CompressedDataInputStream(socket.getInputStream(),
remoteFile.compressionInfo);</pre></div></div></li>	<li>Nit: suggest moving serialization
code for Chunk and CompressionParameters into ChunkSerializer and
ChunkParametersSerializer classes, respectively, just to make the code discoverable for
re-use later</li></ul>At a higher level,<ul class="alternate" type="square">	<li>Should we
make nio transfer the default for uncompressed sstables as well, and add an option to
enable compression?  Alternatively, now that compression is the default for new sstables,
I'd be okay with removing LZF stream compression entirely</li>	<li>Does this over-transfer
data on chunk boundaries?  Put another way, do we stream data that doesn't actually belong
on the target node?  (I'm okay with this, just want to be clear about what's
happening.)</li></ul> 


New Comment: 
V2 attached based on the review + some test related change.<blockquote>Would prefer to
have CDIS just implement IS, and let callers wrap in DIS when desired, similar to how we
use SnappyInputStream in IncomingTcpConnection, or LZFInputStream in ISR</blockquote>CDIS
now implements InputStream only and renamed to CompressedInputStream.<blockquote>Why the
changes to OutboundTcpConnection?</blockquote>The changes are made in order to obtain
nio.SocketChannel, socket has to be created using SocketChannel.open.<blockquote>Re MS
changes: when would header.file be null?</blockquote>When a node requests range but target
node doesn't have corresponding data. I reverted the change in MS to send at least send
streaming header when header.file is null. It seems redundant but for now, it's necessary
to terminate stream session of requesting node.<blockquote>Chunk[] sort can use Guava
Longs.compare</blockquote>done.<blockquote>I suggest adding a comment to explain why sort
is necessary (b/c ranges are from replication strategy, so may not be sorted?) Instead of
using Set + copy into array, why not use an ArrayList + trimToSize()</blockquote>The
reason why I use Set here is to eliminate duplicate chunks. Given two different file
section can be mapped to just one chunk.<blockquote>is the FST comment // TODO just use a
raw RandomAccessFile since we're managing our own buffer here obsolete? is the
CompressedRandomAccessReader path used at all in FST anymore?</blockquote>I removed CRAR
from FST in v2. Even if nio is not available (in case of inter-node SSL), streaming uses
CompressedFileStreamTask with socket's InputStream to transfer file
directly.<blockquote>Nit: avoid double negation in if statements with else
clauses<br/>Nit: suggest moving serialization code for Chunk and CompressionParameters
into ChunkSerializer and ChunkParametersSerializer classes, respectively, just to make the
code discoverable for re-use later</blockquote>done.<blockquote>Should we make nio
transfer the default for uncompressed sstables as well, and add an option to enable
compression? Alternatively, now that compression is the default for new sstables, I'd be
okay with removing LZF stream compression entirely</blockquote>I don't do any benchmark,
but I think always using LZF compression is fine when transferring uncompressed
data.<blockquote>Does this over-transfer data on chunk boundaries? Put another way, do we
stream data that doesn't actually belong on the target node? (I'm okay with this, just
want to be clear about what's happening.)</blockquote>Source node can send unrelated range
of data inside chunk, but receiving node ignores (or skips) that part when reading from
socket, so, the answer is no. 


New Comment: 
LGTM, +1.<blockquote>The reason why I use Set here is to eliminate duplicate chunks. Given
two different file section can be mapped to just one chunk</blockquote>Can you expand the
"since sections are not guaranteed to be sorted" comment to elaborate on that?  (Still
might be a bit cleaner to just new ArrayList(set) instead of manually copying to array;
performance difference would be negligible.) 


New Comment: 
Why didn't I use SortedSet/TreeSet to eliminate dups and sort? <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/sad.png" height="16" width="16"
align="absmiddle" alt="" border="0"/><br/>I will update patch with more detailed comment. 


New Comment: 
I attached the part I modified since v2 patch. Basically just switched to use TreeSet
instead of Set and Arrays.sort. 


New Comment: 
+1 


