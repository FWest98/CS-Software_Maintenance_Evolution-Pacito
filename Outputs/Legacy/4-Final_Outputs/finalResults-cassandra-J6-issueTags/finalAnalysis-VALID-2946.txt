Pattern changes caused by commit: 29a373bb798ca5dd5a3beb62ea5cc7879a707d5a

From: Template Method-1
To:   Template Method-2


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-2946.txt 

commit 29a373bb798ca5dd5a3beb62ea5cc7879a707d5a
Author: T Jake Luciani <jake@apache.org>

    zero-copy reads
    Patch by Pavel Yaskevich; Reviewed by Jake Luciani for CASSANDRA-1714



==================================
 Issue CASSANDRA-1714 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-1714] zero-copy reads
-----------------

-----------------
Summary: zero-copy reads
-----------------

-----------------
Issue type: Improvement
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Fri, 5 Nov 2010 18:48:05 +0000
-----------------

-----------------
Resolved at: Sat, 15 Jan 2011 02:10:41 +0000
-----------------

-----------------
Assigned to: Pavel Yaskevich
-----------------

-----------------
Description: 

Since we are already using mmap'd ByteBuffers in MappedFileDataInput we should be able to
do zero-copy reads (via buffer.slice()), which would give us better performance than <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1651" title="Improve read
performance by using byte array slabs" class="issue-link"
data-issue-key="CASSANDRA-1651"><del>CASSANDRA-1651</del></a> without having to worry
about buffer management.
 

-----------------

-----------------
Comments: 

New Comment: 
patch implements the main zero-copy method (FileDataInput.readBytes) but there are details
left to solve:1. LazilyCompactedRowTest deals with inputstreams which conflicts w/ our
need to deal w/ FDI.  I refactored out AbstractDataInput so we could implement a
ByteBufferFileDataInput relatively easily but maybe changing the test is better.<br/>2. We
make a ton of calls to ByteBuffer.array() which is invalid with a direct buffer.  I
started cleaning these up but there are more left.  (Some calls may be okay if we know the
BB involved is always allocated on the heap; relying on the test suite may be less work
than changing <b>every</b> array() call.)<br/>3. AFAIK the only performant way to write
the contents of a direct buffer is with FileChannel, which conflicts with our
ICompactSerializer<span class="error">&#91;2&#93;</span> code that deals with
DataInput<span class="error">&#91;Stream&#93;</span>.  Only the Column stuff cares deeply
about this I think (since nothing else deals w/ direct buffers) but if we start using a
FileChannel for our sockets then that probably forces a cascading change everywhere else. 


New Comment: 
Regarding 2)  How do you plan on removing .array() while keeping the buffers immutable? 
every .get() call will increment the position.  Just call duplicate() before everything
like you did with message digest? 


New Comment: 
that's one approach (when the method knows how to take a ByteBuffer object), the other is
replacing with get<img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/information.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> like I did for the compareUnsigned 


New Comment: 
branch: trunk (latest commit 0888d35fa933e873f29b2b2daa645a71f995b7f3)Additional: major
refactoring for buffer.array() usages and FBUtility/ByteBufferUtil classes and marshal
types, TBinaryProtocol added to org.apache.cassandra.thrift to override method writeBinary
which will now work fine with buffers without backing array. 


New Comment: 
Could you submit your direct friendly protocol changes here? <a
href="https://issues.apache.org/jira/browse/THRIFT-883" class="external-link"
rel="nofollow">https://issues.apache.org/jira/browse/THRIFT-883</a> 


New Comment: 
Sure, I will submit change for TBinaryProtocol.writeBinary(ByteBuffer) this is only change
I need in Thrift. 


New Comment: 
Done. 


New Comment: 
Two issues I see:1) writeShortByteArray/readShortByteArray changes still make copies of
direct buffers. A more fundamental change to IColumnSerializer2 to use FileChannels is
needed, see original (3)<br/>2) Objects going into rowCache or keyCache that contain
direct buffers need to be deepCopied first otherwise the OS will never release these
underlying buffers. 


New Comment: 
stress benchmarks don't give any major difference in speed. I think I will rework
everything and start from moving ICS2 to use FileChannel. 


New Comment: 
ICS2 FileChannel will <b>only</b> help multinode clusters.  So if you're not seeing any
speed difference on a single node cluster, then that's going to be a waste of time.Let's
speed up local reads first, then worry about the filechannel serialization later.  And if
it turns out we can't do the first, then let's not bother with the second. 


New Comment: 
Ok, I will work on improving BRAF.readBytes() method and see if I can further refactor
buffer.array() usages. 


New Comment: 
After re-looking at the patch surprised local reads aren't faster, your changes to
FileDataInput should make SSTable reads zero-copy for column data. Did you try
benchmarking with large columns? -S 10k? 


New Comment: 
I have tried with -S 2048 but will try with 16384 soon, I have discovered strange issue
with my changes - stress can't read portion of the inserted keys after one of the inserts,
working on that now. 


New Comment: 
branch: trunk (latest commit d8c4f7ce5fb2b5d2d1e75aee99426eec77ceadc2)benchmark using
stress.py (-S 8192 -n 100000): writes are equal, reads are 16 secs. faster (250 comparing
to 266) 


New Comment: 
I think it's worth adding then, and do ICS2 in another ticket +1 


New Comment: 
I concur! Can you create that task and assign it on me, please? 


New Comment: 
we need to clean the renames and other unrelated changes out of this patch and submit
separately. 


New Comment: 
The biggest of unrelated changes in removal of "_" in attributes of SuperColumn class and
few code style tweaks... Can't we just commit it as is because it will cost me good time
of injecting them off, if not - where can I post a patch with those refactorings? 


New Comment: 
Sorry; it really makes it difficult when reading e.g. svn annotate to see what was
actually relevant to the zero-copy read change.  Please re-submit with the other changes
backed out.You're welcome to create a ticket for the refactorings.(FWIW, a git-based
workflow like the one I described in <a
href="http://spyced.blogspot.com/2009/06/patch-oriented-development-made-sane.html"
class="external-link"
rel="nofollow">http://spyced.blogspot.com/2009/06/patch-oriented-development-made-sane.html</a>
makes it much easier to keep these kinds of changes separated out.) 


New Comment: 
branch: trunk (latest commit d8c4f7ce5fb2b5d2d1e75aee99426eec77ceadc2)version 3 with
refactoring and renames removed. 


New Comment: 
needs rebase to 0.7 branch 


New Comment: 
(trunk is <em>probably</em> close enough &#8211; this doesn't apply to trunk either,
post-<a href="https://issues.apache.org/jira/browse/CASSANDRA-1939" title="Misuses of
ByteBuffer absolute get (wrongfully adding arrayOffset to the index)" class="issue-link"
data-issue-key="CASSANDRA-1939"><del>CASSANDRA-1939</del></a> &#8211; but if you can test
against 0.7 to be sure that would be ideal) 


New Comment: 
the latest commit c4150961a85d8e12636b7b080a0ce162d8a4bcef (merge from 0.7) 


New Comment: 
Is this ok or should I provide version for 0.7 also? 


New Comment: 
sorry, needs a separate patch for the *Column differences 


New Comment: 
I don't get it... If you mean changes for digest.update(...) there those were in your
original patch too... 


New Comment: 
I think we can do the following thing - I or you can create a task "clean all
ByteBuffer.array()/arrayOffset() usages except in ByteBufferUtil" and I will provide a
separate patch for that and then I will submit a patch for this task, because all those
modifications I made in Columns/Serializers are needed to work with updated model aka
readBytes. WDYT? 


New Comment: 
<blockquote>I don't get it... If you mean changes for digest.update(...) there those were
in your original patch too... </blockquote>I mean the patch does not apply to 0.7 because
of the changes to some Column classes from <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1072" title="Increment counters"
class="issue-link" data-issue-key="CASSANDRA-1072"><del>CASSANDRA-1072</del></a> that are
not in 0.7 


New Comment: 
I got it now, thanks for explanation! I will make a patch for 0.7 


New Comment: 
the latest commit 28394978ca72a8d1f88422cb7858fa200fc470d5, branch cassandra-0.7 (merge
from 0.6) 


New Comment: 
I see around a 20% increase in reads against a single node with this patch. 


New Comment: 
We need to be sure we aren't storing MmappedByteBuffers in the RowCache or KeyCache.  I
think with this patch that's now the case. 


New Comment: 
This still does a ton of copies by calling getArray unnecessarily.  I have a patch fixing
this but I broke some tests &#8211; will upload soon. 


New Comment: 
The idea behind .getArray was to keep all array manipulation in one place to refactor all
.array() calls everywhere . Would be great to take a look at your patch, Jonathan. 


New Comment: 
v5 attached against 0.7, w/ tests passing.Almost all the getArray calls were for data
about to be written to a DataOutput.  Replaced these with BBU.write or BBU.writeWithLength
calls (moved &amp; renamed from FBU). 


New Comment: 
LGTM. 


New Comment: 
Still needs to copy MappedByteBuffers  before row or key cachingSee <a
href="http://download.oracle.com/javase/1.4.2/docs/api/java/nio/MappedByteBuffer.html"
class="external-link"
rel="nofollow">http://download.oracle.com/javase/1.4.2/docs/api/java/nio/MappedByteBuffer.html</a>"All
or part of a mapped byte buffer may become inaccessible at any time, for example if the
mapped file is truncated. An attempt to access an inaccessible region of a mapped byte
buffer will not change the buffer's content and will cause an unspecified exception to be
thrown either at the time of the access or at some later time. It is therefore strongly
recommended that appropriate precautions be taken to avoid the manipulation of a mapped
file by this program, or by a concurrently running program, except to read or write the
file's content." 


New Comment: 
I'm on it. 


New Comment: 
deep copies key before storing to the keyCache, rowCache is not affected by modifications
in MappedFileDataInput. 


New Comment: 
won't the buffers for column name / value in row cache be coming from MFDI?(we don't care
about truncate et al, but we don't want to keep the mappedbytebuffers alive
post-compaction, so we do want to copy for that reason) 


New Comment: 
latest patch looks right, tested row cache before and after compaction, all ok.+1. I'll
commit tonight unless someone chimes in with anything else 


New Comment: 
committed nice work! 


New Comment: 
Integrated in Cassandra-0.7 #160 (See <a
href="https://hudson.apache.org/hudson/job/Cassandra-0.7/160/" class="external-link"
rel="nofollow">https://hudson.apache.org/hudson/job/Cassandra-0.7/160/</a>)<br/>   
zero-copy reads<br/>Patch by Pavel Yaskevich; Reviewed by Jake Luciani for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1714" title="zero-copy reads"
class="issue-link" data-issue-key="CASSANDRA-1714"><del>CASSANDRA-1714</del></a> 


New Comment: 
I think we need a trunk version of the v6 patch. 


New Comment: 
I already upmerged to trunk and fixed the conflicts related to 1072. Pavel can you check
that trunk isn't missing anything? 


New Comment: 
Missing changes for trunk version 


