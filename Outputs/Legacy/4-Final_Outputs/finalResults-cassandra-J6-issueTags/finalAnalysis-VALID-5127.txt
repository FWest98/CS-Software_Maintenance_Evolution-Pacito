Pattern changes caused by commit: e5f7b37e001a76ca0e53b07a8ff8f37e20c69ca9

From: Decorator-0
To:   Decorator-1

From: Flyweight-5
To:   Flyweight-4

From: Strategy-0
To:   Strategy-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-5127.txt 

commit e5f7b37e001a76ca0e53b07a8ff8f37e20c69ca9
Author: Brandon Williams <brandonwilliams@apache.org>

    Fix bug where the FailureDetector can take a very long time to mark a
    host down.
    Patch by brandonwilliams, reviewed by Paul Cannon for CASSANDRA-3273



==================================
 Issue CASSANDRA-3273 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3273] FailureDetector can take a very long time to mark a host down
-----------------

-----------------
Summary: FailureDetector can take a very long time to mark a host down
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 29 Sep 2011 04:21:50 +0000
-----------------

-----------------
Resolved at: Mon, 3 Oct 2011 20:37:05 +0000
-----------------

-----------------
Assigned to: Brandon Williams
-----------------

-----------------
Description: 

There are two ways to trigger this:
<ul>	<li>Bring a node up very briefly in a
mixed-version cluster and then terminate it</li>	<li>Bring a node up, terminate it for a
very long time, then bring it back up and take it down again</li></ul>
In the first case,
what can happen is a very short interval arrival time is recorded by the versioning logic
which requires reconnecting and can happen very quickly. This can easily be solved by
rejecting any intervals within a reasonable bound, for instance the gossiper
interval.

The second instance is harder to solve, because what is happening is that an
extremely large interval is recorded, which is the time the node was left dead the first
time.  This throws off the mean of the intervals and causes it to take a much longer time
than it should to mark it down the second time.
 

-----------------

-----------------
Comments: 

New Comment: 
<blockquote>a very short interval arrival time is recorded by the versioning logic which
requires reconnecting and can happen very quickly</blockquote>I thought only gossip
heartbeats generate interval measurements, is that incorrect?<blockquote>an extremely
large interval is recorded, which is the time the node was left dead the first
time</blockquote>What if we reset the intervals when we get a node back-from-the-dead? 


New Comment: 
As a side note, this is a big pain in the ass for (distributed) testing, where you
start/stop node quickly. 


New Comment: 
<blockquote>I thought only gossip heartbeats generate interval measurements, is that
incorrect?</blockquote>Heartbeats and generation changes.  I take back what I said though,
it's not the versioning reconnection, and it's not a problem with regard to making the FD
take a long time to mark a host down.It is, however, possible to receive two intervals in
a short amount of time, just due to timer skew between the two hosts, but it can only
happen once since after that they will be in sync from the FD's perspective.The net effect
of this in the pathological case would be that the FD causes a host to be marked down if
the host suddenly becomes silent for a period of 4-5s after the FD receives the initial
(500ms) interval and then the short (1ms) one only. 


New Comment: 
So it sounds like only case 2 is worth bothering with? 


New Comment: 
<blockquote>What if we reset the intervals when we get a node
back-from-the-dead?</blockquote>That makes sense if we're observing a generation change,
the node either rebooted or was taken over by a new machine, so relearning the network
characteristics is a good idea.In the case that there was only a heartbeat change, that
indicates there was something bad (most likely in the network) and we should remember that
for next time to avoid flapping.  However, in the case of a long partition where the
generation won't change, we don't want to record the partition time as an interval since
if the partition reoccurs soon, it will take us a very long time to mark the host down
again.This patch clears the intervals on a generation change, and handles the long
partition case by defining a reasonable maximum to record, in this case the rpc timeout,
since adapting beyond this rather than failing quickly doesn't make much sense that I can
think of, but I'll entertain a higher hard set default if anyone disagrees. 


New Comment: 
smoke test only: so far so goodi have a node (6-node cluster) that was down for a LONG
time (700 PHI), then start that node for about 30 sec before stopping itring shows that
node is down in about 20-30secs, gives or takes<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>TRACE
[GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 703.9568334429565TRACE [GossipTasks:1] 2011-09-30 00:14:58,727
FailureDetector.java (line 160) notifying listeners that /10.40.22.186 is downTRACE
[GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 161) intervals: 1027.0
1904.0 2153.0 951.0 215.0 1788.0 1002.0 1002.0 895.0 1133.0 1869.0 mean:
1267.1818181818182DEBUG [GossipStage:1] 2011-09-30 00:14:58,728 Gossiper.java (line 661)
Clearing interval times for /10.40.22.186 due to generation changeDEBUG [GossipStage:1]
2011-09-30 00:14:58,728 FailureDetector.java (line 242) Ignoring interval time of
2054002.0TRACE [GossipTasks:1] 2011-09-30 00:14:59,729 FailureDetector.java (line 156) PHI
for /10.40.22.186 : 0.0TRACE [GossipTasks:1] 2011-09-30 00:15:00,730 FailureDetector.java
(line 156) PHI for /10.40.22.186 : 0.43429448190325176TRACE [GossipTasks:1] 2011-09-30
00:15:01,732 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.8690228244277856TRACE [GossipTasks:1] 2011-09-30 00:15:02,733 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.2890479080886867TRACE [GossipTasks:1] 2011-09-30
00:15:03,734 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.19662520906271305TRACE [GossipTasks:1] 2011-09-30 00:15:04,735 FailureDetector.java
(line 156) PHI for /10.40.22.186 : 0.20189636121957935TRACE [GossipTasks:1] 2011-09-30
00:15:05,737 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.5977870734348798TRACE [GossipTasks:1] 2011-09-30 00:15:06,738 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.20802340729819624TRACE [GossipTasks:1] 2011-09-30
00:15:07,739 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.6139326289463335TRACE [GossipTasks:1] 2011-09-30 00:15:08,740 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.21152308625862737TRACE [GossipTasks:1] 2011-09-30
00:15:09,741 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.21261773854488178TRACE [GossipTasks:1] 2011-09-30 00:15:10,743 FailureDetector.java
(line 156) PHI for /10.40.22.186 : 0.6270982327510521TRACE [GossipTasks:1] 2011-09-30
00:15:11,744 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.1968065146773795TRACE [GossipTasks:1] 2011-09-30 00:15:12,745 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.579337235438655TRACE [GossipTasks:1] 2011-09-30
00:15:13,746 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.37217274142982526TRACE [GossipTasks:1] 2011-09-30 00:15:14,747 FailureDetector.java
(line 156) PHI for /10.40.22.186 : 0.7443454828596505TRACE [GossipTasks:1] 2011-09-30
00:15:15,757 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.3555955505071756TRACE [GossipTasks:1] 2011-09-30 00:15:16,758 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.7083717111193488TRACE [GossipTasks:1] 2011-09-30
00:15:17,759 FailureDetector.java (line 156) PHI for /10.40.22.186 :
1.061147871731522TRACE [GossipTasks:1] 2011-09-30 00:15:18,760 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.3194684082936909TRACE [GossipTasks:1] 2011-09-30
00:15:19,762 FailureDetector.java (line 156) PHI for /10.40.22.186 :
0.6395757534039692TRACE [GossipTasks:1] 2011-09-30 00:15:20,763 FailureDetector.java (line
156) PHI for /10.40.22.186 : 0.9593636301059537TRACE [GossipTasks:1] 2011-09-30
00:15:21,764 FailureDetector.java (line 156) PHI for /10.40.22.186 :
1.2791515068079384TRACE [GossipTasks:1] 2011-09-30 00:15:22,765 FailureDetector.java (line
156) PHI for /10.40.22.186 : 1.598939383509923TRACE [GossipTasks:1] 2011-09-30
00:15:23,767 FailureDetector.java (line 156) PHI for /10.40.22.186 :
1.919046728620201TRACE [GossipTasks:1] 2011-09-30 00:15:24,768 FailureDetector.java (line
156) PHI for /10.40.22.186 : 2.238834605322186TRACE [GossipTasks:1] 2011-09-30
00:15:25,769 FailureDetector.java (line 156) PHI for /10.40.22.186 :
2.5586224820241705TRACE [GossipTasks:1] 2011-09-30 00:15:26,771 FailureDetector.java (line
156) PHI for /10.40.22.186 : 2.8787298271344484TRACE [GossipTasks:1] 2011-09-30
00:15:27,772 FailureDetector.java (line 156) PHI for /10.40.22.186 :
3.198517703836433TRACE [GossipTasks:1] 2011-09-30 00:15:28,773 FailureDetector.java (line
156) PHI for /10.40.22.186 : 3.518305580538418TRACE [GossipTasks:1] 2011-09-30
00:15:29,774 FailureDetector.java (line 156) PHI for /10.40.22.186 :
3.838093457240402TRACE [GossipTasks:1] 2011-09-30 00:15:30,776 FailureDetector.java (line
156) PHI for /10.40.22.186 : 4.158200802350681TRACE [GossipTasks:1] 2011-09-30
00:15:31,777 FailureDetector.java (line 156) PHI for /10.40.22.186 :
4.4779886790526655TRACE [GossipTasks:1] 2011-09-30 00:15:32,778 FailureDetector.java (line
156) PHI for /10.40.22.186 : 4.79777655575465TRACE [GossipTasks:1] 2011-09-30 00:15:33,779
FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.117564432456635TRACE
[GossipTasks:1] 2011-09-30 00:15:34,781 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 5.437671777566913TRACE [GossipTasks:1] 2011-09-30 00:15:35,782
FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.757459654268897TRACE
[GossipTasks:1] 2011-09-30 00:15:36,783 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 6.077247530970882TRACE [GossipTasks:1] 2011-09-30 00:15:37,784
FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.397035407672866TRACE
[GossipTasks:1] 2011-09-30 00:15:38,785 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 6.7168232843748505TRACE [GossipTasks:1] 2011-09-30 00:15:39,786
FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.036611161076836TRACE
[GossipTasks:1] 2011-09-30 00:15:40,788 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 7.356718506187114TRACE [GossipTasks:1] 2011-09-30 00:15:41,789
FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.676506382889099TRACE
[GossipTasks:1] 2011-09-30 00:15:42,790 FailureDetector.java (line 156) PHI for
/10.40.22.186 : 7.996294259591083TRACE [GossipTasks:1] 2011-09-30 00:15:43,791
FailureDetector.java (line 156) PHI for /10.40.22.186 : 8.316082136293067TRACE
[GossipTasks:1] 2011-09-30 00:15:43,792 FailureDetector.java (line 160) notifying
listeners that /10.40.22.186 is downTRACE [GossipTasks:1] 2011-09-30 00:15:43,792
FailureDetector.java (line 161) intervals: 1001.0 2004.0 1011.0 481.0 999.0 1514.0 487.0
1551.0 450.0 1001.0 2002.0 1516.0 2003.0 3012.0 mean: 1359.4285714285713</pre></div></div> 


New Comment: 
+1 


New Comment: 
Committed. 


