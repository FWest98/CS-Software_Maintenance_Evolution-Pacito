Pattern changes caused by commit: 83418fa80613c37461c47ee61e02b6c489bbb6c8

From: Decorator-1
To:   Decorator-0

From: Flyweight-4
To:   Flyweight-5

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-4908.txt 

commit 83418fa80613c37461c47ee61e02b6c489bbb6c8
Author: Sylvain Lebresne <slebresne@apache.org>

    Randomize choice of first replica for counter increments
    patch by slebresne; reviewed by jbellis for CASSANDRA-2890



==================================
 Issue CASSANDRA-2890 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-2890] Randomize (to some extend) the choice of the first replica for counter increment
-----------------

-----------------
Summary: Randomize (to some extend) the choice of the first replica for counter increment
-----------------

-----------------
Issue type: Improvement
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Tue, 12 Jul 2011 19:28:38 +0000
-----------------

-----------------
Resolved at: Tue, 13 Sep 2011 16:03:43 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

Right now, we choose the first replica for a counter increments based solely on what the
snitch returns. If the clients requests are well balanced over the cluster and the snitch
not ill configured, this should not be a problem, but this is probably too strong an
assumption to make.

The goal of this ticket is to change this to choose a random replica
in the current data center instead.
 

-----------------

-----------------
Comments: 

New Comment: 
David Hawthorne reports on the mailing list that he ran into this in the
wild:<blockquote>It was exactly due to 2890, and the fact that the first replica is always
the one with the lowest value IP address.  I patched cassandra to pick a random node out
of the replica set in StorageProxy.java findSuitableEndpoint:Random rng = new
Random();return endpoints.get(rng.nextInt(endpoints.size()));  // instead of return
endpoints.get(0);Now work load is evenly balanced among all 5 nodes and I'm getting 2.5x
the inserts/sec throughput.Here's the behavior I saw, and "disk work" refers to the
ReplicateOnWrite load of a counter insert:One node will get RF/n of the disk work.  Two
nodes will always get 0 disk work.in a 3 node cluster, 1 node gets disk hit really hard. 
You get the performance of a one-node cluster.<br/>in a 6 node cluster, 1 node gets hit
with 50% of the disk work, giving you the performance of ~2 node cluster.<br/>in a 10 node
cluster, 1 node gets 30% of the disk work, giving you the performance of a ~3 node
cluster.I confirmed this behavior with a 3, 4, and 5 node cluster size.</blockquote> 


New Comment: 
Randomizing the first replica will break dynamic snitch behaviour. If we're
assuming:<br/>1) everyone uses the dynamic snitch<br/>2) some folks perform nearly 100%
writes, and very few client reads<br/>...would it be better to start recording the
replicate-on-writes latencies in the dynamic snitch, so that the first replica will be
chosen based on latency, rather than raw snitch scores? 


New Comment: 
Ok, I wasn't understanding that "the lowest value IP address" is always chosen<br/>thing,
but as it turns out the DynamicSnitch uses the underlying snitch compareEndpoints() method
if two endpoints have the same score (which includes the case where they have no scores at
all, because no reads have been done for instance). And the SimpleSnitch compareEndpoints
method happens to compare the endpoint by IP address. I'm not sure this is a really good
choice because<ol>	<li>this is not coherent with the sortEndpointByProximity
sorting</li>	<li>this doesn't correspond to how the NetworkTopolySnitch compareEndpoints
work when restricted to only one datacenter (it sets all node equal).<br/>So I think there
may be something to change in there, but anyway that is not completely relevant to this
issue.</li></ol>On this issue, I agree with Stu than tracking the latency and using that
is probably the best solution (though that does suppose the use of the DynamicSnitch).
However, this is not so simple, because the latency we have (on the coordinator) is the
latency of the whole counter write, that is, it does not only include the read by the
first replica, but also the local write (not a big deal) and the latencies of the writes
to the other replica. Those last ones depends on the consistency level for instance. Also,
a TimeoutException does not necessarily means that the first replica is to blame, it could
be that enough other replica timed out (so that the consistency level wasn't
achieved).There may be solutions to those problem, but I don't see any simple ones. Now,
as there is reports this is a problem "in the wild", I propose we go for the simple
"randomize" solution for now and push this directly to the 0.8 series.  Attaching a patch
(against 0.8) for this. Then we can open another ticket to improve over that. 


New Comment: 
How necessary is this if we change SSnitch to match ANTS? 


New Comment: 
<blockquote>How necessary is this if we change SSnitch to match ANTS?</blockquote>That
would restore what I though was the previous behavior, i.e, for equal score the first
alive replica in ring order will be picked. Which means that provided the ring is well
balanced and the client query are well balanced too, the load will be uniform. But that
suppose that<ol>	<li>client queries are well balanced. True, this is already a requirement
to get the best out of your cluster. But it would make it a much stronger requirement as
more would depend on it.</li>	<li>this is for when the dynamic snitch returns equal
scores. I think that is actually the stronger motivation for this: relying on the scores
of the dynamic snitch when those scores are not influenced by the operation we're doing is
likely a bad idea. Clusters that have a very low read rate (compared to the write one)
will likely still experience poor load balancing.</li></ol> 


New Comment: 
That being said, I do think we should probably change SSnitch to match ANTS. 


New Comment: 
Sounds reasonable.Should we move threadlocal&lt;random&gt; to FBUtilities since we have
one in CounterMutation as well? 


New Comment: 
<blockquote>Should we move threadlocal&lt;random&gt; to FBUtilities since we have one in
CounterMutation as well?</blockquote>We have one in ReadCallback too, and one or two
places where we create one shot random object. We could move all that to
FBUtilities?<br/>Note though that is won't save us much allocation since all those uses of
Random are done in different thread pools (and the one in CounterMutation may go away with
<a href="https://issues.apache.org/jira/browse/CASSANDRA-3178" title="Counter shard
merging is not thread safe" class="issue-link"
data-issue-key="CASSANDRA-3178"><del>CASSANDRA-3178</del></a> anyway). 


New Comment: 
Would just like to consolidate the code, like we did w/ ThreadLocal&lt;MessageDigest&gt;. 


New Comment: 
Attaching v2 that moves ThreadLocal&lt;Random&gt; uses to FBUtilities. 


New Comment: 
+1 


New Comment: 
Committed, thanks 


