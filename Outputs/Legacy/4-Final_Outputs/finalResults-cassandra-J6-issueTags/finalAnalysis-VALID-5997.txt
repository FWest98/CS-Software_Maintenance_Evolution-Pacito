Pattern changes caused by commit: c3dc7894159ad413f9c8fa0cc0024c6ed0984831

From: Abstract Factory-3
To:   Abstract Factory-2

From: Factory Method-3
To:   Factory Method-2

From: Facade-1
To:   Facade-0

From: Flyweight-3
To:   Flyweight-4


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-5997.txt 

commit c3dc7894159ad413f9c8fa0cc0024c6ed0984831
Author: Jonathan Ellis <jbellis@apache.org>

    avoid including non-queried nodes in rangeslice read repair
    patch by jbellis; reviewed by Vijay for CASSANDRA-3843



==================================
 Issue CASSANDRA-3843 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3843] Unnecessary  ReadRepair request during RangeScan
-----------------

-----------------
Summary: Unnecessary  ReadRepair request during RangeScan
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Fri, 3 Feb 2012 11:40:44 +0000
-----------------

-----------------
Resolved at: Thu, 9 Feb 2012 21:43:48 +0000
-----------------

-----------------
Assigned to: Jonathan Ellis
-----------------

-----------------
Description: 

During reading with Quorum level and replication factor greater then 2, Cassandra sends
at least one ReadRepair, even if there is no need to do that. 

With the fact that read
requests await until ReadRepair will finish it slows down requsts a lot, up to the Timeout
<img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.png"
height="16" width="16" align="absmiddle" alt="" border="0"/>

It seems that the problem
has been introduced by the <a href="https://issues.apache.org/jira/browse/CASSANDRA-2494"
title="Quorum reads are not monotonically consistent" class="issue-link"
data-issue-key="CASSANDRA-2494"><del>CASSANDRA-2494</del></a>, unfortunately I have no
enought knowledge of Cassandra internals to fix the problem and do not broke <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2494" title="Quorum reads are not
monotonically consistent" class="issue-link"
data-issue-key="CASSANDRA-2494"><del>CASSANDRA-2494</del></a> functionality, so my report
without a patch.

Code explanations:
<div class="code panel" style="border-style:
solid;border-width: 1px;"><div class="codeHeader panelHeader" style="border-bottom-width:
1px;border-bottom-style: solid;"><b>RangeSliceResponseResolver.java</b></div><div
class="codeContent panelContent"><pre class="code-java"><span class="code-keyword">class
</span>RangeSliceResponseResolver {    <span class="code-comment">// ....</span>    <span
class="code-keyword">private</span> <span class="code-keyword">class </span>Reducer <span
class="code-keyword">extends</span> MergeIterator.Reducer&lt;Pair&lt;Row,InetAddress&gt;,
Row&gt;    {    <span class="code-comment">// ....</span>        <span
class="code-keyword">protected</span> Row getReduced()        {            ColumnFamily
resolved = versions.size() &gt; 1                                  ?
RowRepairResolver.resolveSuperset(versions)                                  :
versions.get(0);            <span class="code-keyword">if</span> (versions.size() &lt;
sources.size())            {                <span class="code-keyword">for</span>
(InetAddress source : sources)                {                    <span
class="code-keyword">if</span> (!versionSources.contains(source))                    {    
                                             <span class="code-comment">// [PA] Here we
are adding <span class="code-keyword">null</span> ColumnFamily.</span>                    
   <span class="code-comment">// later it will be compared with the <span
class="code-quote">"desired"</span></span>                        <span
class="code-comment">// version and will give us <span class="code-quote">"fake"</span>
difference which</span>                        <span class="code-comment">// forces
Cassandra to send ReadRepair to a given source</span>                       
versions.add(<span class="code-keyword">null</span>);                       
versionSources.add(source);                    }                }            }           
<span class="code-comment">// ....</span>            <span class="code-keyword">if</span>
(resolved != <span class="code-keyword">null</span>)               
repairResults.addAll(RowRepairResolver.scheduleRepairs(resolved, table, key, versions,
versionSources));            <span class="code-comment">// ....</span>        }   
}}</pre></div></div><div class="code panel" style="border-style: solid;border-width:
1px;"><div class="codeHeader panelHeader" style="border-bottom-width:
1px;border-bottom-style: solid;"><b>RowRepairResolver.java</b></div><div
class="codeContent panelContent"><pre class="code-java"><span
class="code-keyword">public</span> <span class="code-keyword">class
</span>RowRepairResolver <span class="code-keyword">extends</span> AbstractRowResolver {  
 <span class="code-comment">// ....</span>    <span class="code-keyword">public</span>
<span class="code-keyword">static</span> List&lt;IAsyncResult&gt;
scheduleRepairs(ColumnFamily resolved, <span class="code-object">String</span> table,
DecoratedKey&lt;?&gt; key, List&lt;ColumnFamily&gt; versions, List&lt;InetAddress&gt;
endpoints)    {        List&lt;IAsyncResult&gt; results = <span
class="code-keyword">new</span> ArrayList&lt;IAsyncResult&gt;(versions.size());       
<span class="code-keyword">for</span> (<span class="code-object">int</span> i = 0; i &lt;
versions.size(); i++)        {            <span class="code-comment">// On some iteration
we have to compare <span class="code-keyword">null</span> and resolved which are
obviously</span>            <span class="code-comment">// not equals, so it will fire a
ReadRequest, however it is not needed here</span>            ColumnFamily diffCf =
ColumnFamily.diff(versions.get(i), resolved);            <span
class="code-keyword">if</span> (diffCf == <span class="code-keyword">null</span>)         
      <span class="code-keyword">continue</span>;        <span class="code-comment">//
.... </span></pre></div></div>
Imagine the following situation:<br/>NodeA has X.1 // row X
with the version 1<br/>NodeB has X.2 <br/>NodeC has X.? // Unknown version, but because
write was with Quorum it is 1 or 2

During the Quorum read from nodes A and B, Cassandra
creates version 12 and send ReadRepair, so now nodes has the following content:<br/>NodeA
has X.12<br/>NodeB has X.12

which is correct, however Cassandra also will fire ReadRepair
to NodeC. There is no need to do that, the next consistent read have a chance to be served
by nodes 
{A, B}
 (no ReadRepair) or by pair {?, C} and in that case ReadRepair will be
fired and brings nodeC to the consistent state

Right now we are reading from the Index a
lot and starting from some point in time we are getting TimeOutException because cluster
is overloaded by the ReadRepairRequests <b>even</b> if all nodes has the same data <img
class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.png"
height="16" width="16" align="absmiddle" alt="" border="0"/>
 

-----------------

-----------------
Comments: 

New Comment: 
The null version was added for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2680" title="range scan doesn&#39;t
repair missing rows" class="issue-link"
data-issue-key="CASSANDRA-2680"><del>CASSANDRA-2680</del></a>.  I think the core problem
here is that the RSRR is being created with <b>all</b> the replica endpoints
(<tt>liveEndpoints</tt>), not just the ones being asked to respond to the query
(<tt>handler.endpoints</tt>).  This is a bit tricky since the handler wants to know its
resolver at creation time, and unlike RowDigestResolver, RSRR wants to initialize its
endpoints at creation time too.  Kind of hackish patch attached. 


New Comment: 
+1 


New Comment: 
&gt; The null version was added for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2680" title="range scan doesn&#39;t
repair missing rows" class="issue-link"
data-issue-key="CASSANDRA-2680"><del>CASSANDRA-2680</del></a>.<br/>Oh, good point. Sorry,
I've should pay more attention on git history, not only on annotations <img
class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.png"
height="16" width="16" align="absmiddle" alt="" border="0"/>Anyway, thanks for the patch,
now we could apply correct patch on our servers. 


New Comment: 
committed, with the same fix for the 2nd occurrence of the RSRR in 1.0 StorageProxy. v2
attached in case that makes it easyer for anyone to test. 


New Comment: 
We'll be upgrading to 1.0.8 as soon as we can, but this seems like a significant issue for
anyone doing range scans - does it make sense to backport to 0.8.x? 


New Comment: 
It's a relatively small patch, but StorageProxy and its callbacks can be fragile...  I
almost didn't commit it to 1.0 either.  Tell you what though, I'll post a backported patch
here and if you want you can run with it. <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
Looks to me like the 1.0 code changes from v2 apply cleanly to 0.8.  (CHANGES diff does
not apply but can be ignored.) 


New Comment: 
I patched the version of 0.8.4 that we use with the change.  I applied it to all of our
staging nodes.  However, the problem with writes on the column family it was simply doing
range scans of still persists.  I had major compacted a column family on all of the nodes,
then did a simple pig job to read the contents of that CF, then I got a lot of minor
compactions for that column family. 


New Comment: 
I suggest testing with a single range scan at debug level.  Too much hay to see the needle
when you're doing 100s or 1000s of scans. 


New Comment: 
... You did patch with v2, right? 


New Comment: 
I did patch with v2.  Doing more testing today and it appears that there are writes
occurring but it looks like a definite reduction.  It could be a valid repair thing.  I'll
do some more testing and hopefully repair every node and compact every node and then do a
scan across a large column family and see what happens. 


New Comment: 
I did repairs on all the nodes and then compacts on all the nodes. Then I did a pig job to
simply count the number of rows in the column family. Again I think the overall writes
were reduced but there are writes going on. I need to turn debug on and do the same test
again. I did the compactions at 6:42 and the range scans at 14:16:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">-rw-r--r-- 1 root root 40106228511 Feb 21 06:42
account_snapshot-g-792-Data.db-rw-r--r-- 1 root root   206884816 Feb 21 06:42
account_snapshot-g-792-Filter.db-rw-r--r-- 1 root root  2913796038 Feb 21 06:42
account_snapshot-g-792-Index.db-rw-r--r-- 1 root root        4276 Feb 21 06:42
account_snapshot-g-792-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-793-Compacted-rw-r--r-- 1 root root      287286 Feb 21 14:16
account_snapshot-g-793-Data.db-rw-r--r-- 1 root root         976 Feb 21 14:16
account_snapshot-g-793-Filter.db-rw-r--r-- 1 root root       20857 Feb 21 14:16
account_snapshot-g-793-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:16
account_snapshot-g-793-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-794-Compacted-rw-r--r-- 1 root root    87770771 Feb 21 14:17
account_snapshot-g-794-Data.db-rw-r--r-- 1 root root      293944 Feb 21 14:17
account_snapshot-g-794-Filter.db-rw-r--r-- 1 root root     6377968 Feb 21 14:17
account_snapshot-g-794-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:17
account_snapshot-g-794-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-795-Compacted-rw-r--r-- 1 root root    78459166 Feb 21 14:17
account_snapshot-g-795-Data.db-rw-r--r-- 1 root root      262600 Feb 21 14:17
account_snapshot-g-795-Filter.db-rw-r--r-- 1 root root     5698156 Feb 21 14:17
account_snapshot-g-795-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:17
account_snapshot-g-795-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-796-Compacted-rw-r--r-- 1 root root    69838937 Feb 21 14:17
account_snapshot-g-796-Data.db-rw-r--r-- 1 root root      234000 Feb 21 14:17
account_snapshot-g-796-Filter.db-rw-r--r-- 1 root root     5077447 Feb 21 14:17
account_snapshot-g-796-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:17
account_snapshot-g-796-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-797-Compacted-rw-r--r-- 1 root root    68094433 Feb 21 14:17
account_snapshot-g-797-Data.db-rw-r--r-- 1 root root      227808 Feb 21 14:17
account_snapshot-g-797-Filter.db-rw-r--r-- 1 root root     4943098 Feb 21 14:17
account_snapshot-g-797-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:17
account_snapshot-g-797-Statistics.db-rw-r--r-- 1 root root   304163307 Feb 21 14:20
account_snapshot-g-798-Data.db-rw-r--r-- 1 root root     1019776 Feb 21 14:20
account_snapshot-g-798-Filter.db-rw-r--r-- 1 root root    22096669 Feb 21 14:20
account_snapshot-g-798-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:20
account_snapshot-g-798-Statistics.db-rw-r--r-- 1 root root    65874829 Feb 21 14:18
account_snapshot-g-799-Data.db-rw-r--r-- 1 root root      220192 Feb 21 14:18
account_snapshot-g-799-Filter.db-rw-r--r-- 1 root root     4777809 Feb 21 14:18
account_snapshot-g-799-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:18
account_snapshot-g-799-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-800-Compacted-rw-r--r-- 1 root root    50067413 Feb 21 14:18
account_snapshot-g-800-Data.db-rw-r--r-- 1 root root      167416 Feb 21 14:18
account_snapshot-g-800-Filter.db-rw-r--r-- 1 root root     3632313 Feb 21 14:18
account_snapshot-g-800-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:18
account_snapshot-g-800-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-801-Compacted-rw-r--r-- 1 root root    50575719 Feb 21 14:18
account_snapshot-g-801-Data.db-rw-r--r-- 1 root root      169160 Feb 21 14:18
account_snapshot-g-801-Filter.db-rw-r--r-- 1 root root     3669880 Feb 21 14:18
account_snapshot-g-801-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:18
account_snapshot-g-801-Statistics.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-g-802-Compacted-rw-r--r-- 1 root root    41788766 Feb 21 14:19
account_snapshot-g-802-Data.db-rw-r--r-- 1 root root      139776 Feb 21 14:19
account_snapshot-g-802-Filter.db-rw-r--r-- 1 root root     3033069 Feb 21 14:19
account_snapshot-g-802-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:19
account_snapshot-g-802-Statistics.db-rw-r--r-- 1 root root    46547146 Feb 21 14:19
account_snapshot-g-803-Data.db-rw-r--r-- 1 root root      155720 Feb 21 14:19
account_snapshot-g-803-Filter.db-rw-r--r-- 1 root root     3378457 Feb 21 14:19
account_snapshot-g-803-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:19
account_snapshot-g-803-Statistics.db-rw-r--r-- 1 root root   142719184 Feb 21 14:20
account_snapshot-g-804-Data.db-rw-r--r-- 1 root root      478576 Feb 21 14:19
account_snapshot-g-804-Filter.db-rw-r--r-- 1 root root    10356119 Feb 21 14:20
account_snapshot-g-804-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:20
account_snapshot-g-804-Statistics.db-rw-r--r-- 1 root root    55373874 Feb 21 14:19
account_snapshot-g-805-Data.db-rw-r--r-- 1 root root      185160 Feb 21 14:19
account_snapshot-g-805-Filter.db-rw-r--r-- 1 root root     4017391 Feb 21 14:19
account_snapshot-g-805-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:19
account_snapshot-g-805-Statistics.db-rw-r--r-- 1 root root    46399227 Feb 21 14:19
account_snapshot-g-806-Data.db-rw-r--r-- 1 root root      155120 Feb 21 14:19
account_snapshot-g-806-Filter.db-rw-r--r-- 1 root root     3365947 Feb 21 14:19
account_snapshot-g-806-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:19
account_snapshot-g-806-Statistics.db-rw-r--r-- 1 root root    58491393 Feb 21 14:19
account_snapshot-g-807-Data.db-rw-r--r-- 1 root root      196048 Feb 21 14:19
account_snapshot-g-807-Filter.db-rw-r--r-- 1 root root     4253922 Feb 21 14:19
account_snapshot-g-807-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:19
account_snapshot-g-807-Statistics.db-rw-r--r-- 1 root root    47609635 Feb 21 14:20
account_snapshot-g-808-Data.db-rw-r--r-- 1 root root      159320 Feb 21 14:20
account_snapshot-g-808-Filter.db-rw-r--r-- 1 root root     3456985 Feb 21 14:20
account_snapshot-g-808-Index.db-rw-r--r-- 1 root root        4276 Feb 21 14:20
account_snapshot-g-808-Statistics.db-rw-r--r-- 1 root root    46923060 Feb 21 14:20
account_snapshot-tmp-g-809-Data.db-rw-r--r-- 1 root root           0 Feb 21 14:20
account_snapshot-tmp-g-809-Index.db-rw-r--r-- 1 root root    49693602 Feb 21 14:20
account_snapshot-tmp-g-810-Data.db-rw-r--r-- 1 root root      166600 Feb 21 14:20
account_snapshot-tmp-g-810-Filter.db-rw-r--r-- 1 root root     3614750 Feb 21 14:20
account_snapshot-tmp-g-810-Index.db</pre></div></div> 


New Comment: 
I'm unable to repro against 1.0 HEAD. 


