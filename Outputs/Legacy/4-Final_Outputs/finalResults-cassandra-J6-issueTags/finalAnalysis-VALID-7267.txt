Pattern changes caused by commit: a075385d05c3e1d26475f448363958bad4645f17

From: Decorator-1
To:   Decorator-2

From: Flyweight-1
To:   Flyweight-2

From: Mediator-1
To:   Mediator-3

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-7267.txt 

commit a075385d05c3e1d26475f448363958bad4645f17
Author: Yuki Morishita <yukim@apache.org>

    Fix ScrubTest after file format change in CASSANDRA-4436



==================================
 Issue CASSANDRA-4436 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4436] Counters in columns don't preserve correct values after cluster restart
-----------------

-----------------
Summary: Counters in columns don't preserve correct values after cluster restart
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 12 Jul 2012 13:32:47 +0000
-----------------

-----------------
Resolved at: Thu, 26 Jul 2012 16:07:12 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

Similar to #3821. but affecting normal columns. 

Set up a 2-node cluster with
rf=2.<br/>1. Create a counter column family and increment a 100 keys in loop 5000 times.
<br/>2. Then make a rolling restart to cluster. <br/>3. Again increment another 5000
times.<br/>4. Make a rolling restart to cluster.<br/>5. Again increment another 5000
times.<br/>6. Make a rolling restart to cluster.

After step 6 we were able to reproduce
bug with bad counter values. <br/>Expected values were 15 000. Values returned from
cluster are higher then 15000 + some random number.<br/>Rolling restarts are done with
nodetool drain. Always waiting until second node discover its down then kill java process.

 

-----------------

-----------------
Comments: 

New Comment: 
Can you reproduce every time with those steps? I tried reproducing with those exact steps
(as far as I can tell) a few times on both 1.0 and 1.1 (the counter code didn't change
much between 1.0 and 1.1) and wasn't able to reproduce. 


New Comment: 
create keyspace test_old<br/>  with placement_strategy = 'SimpleStrategy'<br/>  and
strategy_options = {replication_factor : 2}  and durable_writes = true;use test_old;create
column family cf1_increment<br/>  with column_type = 'Standard'<br/>  and comparator =
'BytesType'<br/>  and default_validation_class = 'CounterColumnType'<br/>  and
key_validation_class = 'BytesType'<br/>  and read_repair_chance = 1.0<br/>  and
dclocal_read_repair_chance = 0.0<br/>  and gc_grace = 864000<br/>  and
min_compaction_threshold = 4<br/>  and max_compaction_threshold = 32<br/>  and
replicate_on_write = true<br/>  and compaction_strategy =
'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'<br/>  and caching =
'KEYS_ONLY'<br/>  and compression_options = {'sstable_compression' :
'org.apache.cassandra.io.compress.SnappyCompressor'};In version 1.0.10 am always able to
reproduce with this steps.. but its not reproducible in 1.1.2 .When I stop writing and
shutdown node with "nodetool drain" there are some small commitlog files, but I don't
bother to delete them just restart cassandra process. Maybe this is case ? 


New Comment: 
The only difference I could see with the test I ran previously was the use of compression.
So while I strongly doubt compression can have anything to do with that in any way, I
rerun the test against 1.0 a bunch of time but I was still not able to reproduce any
error.Since you seem to be able to reproduce easily, would you mind sharing the scripts
you use to reproduce? I.e. mainly the code you use for insertion, preferably in plain
thrift or CQL2 as this would eliminate the possibility of a client library bug. 


New Comment: 
You are right its not affected by compression.<br/>I was just curious if its problem with
our python code using pycassa ... <br/>So I created increments.cql containing 100k lines
with 1000 increments for each of 100 key values.<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B &gt;/dev/<span
class="code-keyword">null</span> </pre></div></div>after 3 rolling restarts each value was
correct with value 3000 <br/>after 4 rolling restart values are incorrect see bellow<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">col1	5479col10	5507col100	5531col11	5480col12	5501col13	5499col14	5516</pre></div></div>Its
2 node cluster with replication=2. <div class="code panel" style="border-width: 1px;"><div
class="codeContent panelContent"><pre class="code-java">[root@cass-bug1 ~]#
/opt/apache-cassandra-1.0.10/bin/cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B
&gt;/dev/<span class="code-keyword">null</span> [root@cass-bug1 ~]#
/opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain[root@cass-bug2 ~]#
/opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ringAddress         DC         
Rack        Status State   Load            Owns    Token                                  
                                                                                  
85070591730234615865843651857942052864      10.20.30.160    datacenter1 rack1       Down  
Normal  97.67 KB        50.00%  0                                           10.20.30.161  
 datacenter1 rack1       Up     Normal  113.45 KB       50.00% 
85070591730234615865843651857942052864  [root@cass-bug1 ~]# killall java[root@cass-bug1
~]# /opt/apache-cassandra-1.0.10/bin/cassandra[root@cass-bug2 ~]#
/opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain[root@cass-bug1 ~]#
/opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ringAddress         DC         
Rack        Status State   Load            Owns    Token                                  
                                                                                  
85070591730234615865843651857942052864      10.20.30.160    datacenter1 rack1       Up    
Normal  97.67 KB        50.00%  0                                           10.20.30.161  
 datacenter1 rack1       Down   Normal  86.13 KB        50.00% 
85070591730234615865843651857942052864 [root@cass-bug2 ~]# killall java[root@cass-bug2 ~]#
/opt/apache-cassandra-1.0.10/bin/cassandra</pre></div></div>Here is dump of keyspace and
CF <div class="code panel" style="border-width: 1px;"><div class="codeContent
panelContent"><pre class="code-java">create keyspace inc_test  with placement_strategy =
<span class="code-quote">'SimpleStrategy'</span>  and strategy_options =
{replication_factor : 2}  and durable_writes = <span class="code-keyword">true</span>;use
inc_test;create column family cf1_increment  with column_type = <span
class="code-quote">'Standard'</span>  and comparator = <span
class="code-quote">'BytesType'</span>  and default_validation_class = <span
class="code-quote">'CounterColumnType'</span>  and key_validation_class = <span
class="code-quote">'BytesType'</span>  and rows_cached = 0.0  and row_cache_save_period =
0  and row_cache_keys_to_save = 2147483647  and keys_cached = 200000.0  and
key_cache_save_period = 14400  and read_repair_chance = 1.0  and gc_grace = 864000  and
min_compaction_threshold = 4  and max_compaction_threshold = 32  and replicate_on_write =
<span class="code-keyword">true</span>  and row_cache_provider = <span
class="code-quote">'SerializingCacheProvider'</span>  and compaction_strategy = <span
class="code-quote">'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'</span>;</pre></div></div>Hope
that helps you reproduce .. 


New Comment: 
Increment for batch loading through cassandra-cli. 


New Comment: 
Thanks a lot Peter for helping out reproducing this issue.The problem is that when a node
stops (or is drained for that matter, we don't wait for all compaction to end during drain
as this could mean waiting for a very long time, at least with SizeTieredCompaction) just
when a compaction is finishing, it is possible for some of the compacted file to not have
-Compacted components even if the compacted file is not temporary anymore. In other words,
it is possible that when the node is restart, it will load both the compacted files and
some of the file used to compact it. While this is harmless (though inefficient) for
normal column family, this means overcounting for counters.I'll note that even though I
can't reproduce the counter bug on 1.1 with the test case above, it is just "luck" as 1.1
is affected as well.What we need to guarantee is that we will never use both a compacted
file and one of it's ancestor. One way to ensure that is to keep in the metadata of the
compacted file, the list of it's ancestors (we only need to keep the generation). Then
when a node start, it can gather all the ancestors of all the sstable in the data dir, and
delete all those sstable that are in this ancestor set. Since we don't want to keep ever
going list of ancestors however, a newly compacted sstable only need to keep the list of
it's still live ancestor (which 99% of the time means keeping only the generation of the
file that were compacted to obtain it). I note that if we do that, we don't need to
generate -Compacted components.Attaching patch to implement this. Attaching a patch for
1.0 and 1.1 (which aren't very different). I wrote the 1.0 version because it's on this
version that I knew how to reproduce the counter bug reliably, and I've checked that this
patch does fix the issue. However, this patch doesn't only affect counter code and is not
trivial per se, so I don't know how I feel about risking to breaking things on 1.0 for
non-counter user at this point. I think it might me wiser to put this in 1.1.3 only and
say that counter users should either apply the attached patch at their own risk or upgrade
to 1.1.3. 


New Comment: 
Thanks for your interest and time to fix it. We currently move to 1.1.2 version to avoid
some random aws failure and patiently waiting for 1.1.3 release. 


New Comment: 
Looks like skipCompacted in Directories.SSTableLister can be removed (since we
scrubDataDirectories on startup and no new compacted components will be created).Using a
List means we can add an ancestor multiple times.  Suggest using a Set instead.Nits:<ul
class="alternate" type="square">	<li>would prefer Ancestor to LiveAncestor, since we only
check liveness at creation time, so "Live" is misleading when iterating over them
later.</li>	<li>the deleting code feels more at home in CFS constructor than
addInitialSSTables.</li>	<li>tracker parameter is unused now in SSTR.open</li></ul> 


New Comment: 
<blockquote>Looks like skipCompacted in Directories.SSTableLister can be removed (since we
scrubDataDirectories on startup and no new compacted components will be
created).</blockquote>True, though there is the (arguably remote) possibility that people
call loadNewSSTables() (or the offline scrub from <a
href="https://issues.apache.org/jira/browse/CASSANDRA-4441" title="Fix validation of
dates" class="issue-link" data-issue-key="CASSANDRA-4441"><del>CASSANDRA-4441</del></a>)
on sstables having some -Compacted components. So I would prefer leaving it in 1.1 and
removing it during the merge to trunk, just to be sure minor upgrade are as little
disrupting as can be.<blockquote>Using a List means we can add an ancestor multiple times.
Suggest using a Set instead.</blockquote>But we won't have the same ancestor multiple
times. Otherwise that would be a bug (and at least for counters, a particularly bad one).
But for sanity I've added an assertion to check this doesn't happen (I've a list however,
I figured that since the list will be small, the difference between List.contains() and
Set.contains() will be negligeable, and it's checked in an assertion and only once a the
sstable creation. On the other Lists have a smaller memory footprint. Though I admit in
either case we're talked minor differences).<blockquote>would prefer Ancestor to
LiveAncestor, since we only check liveness at creation time, so "Live" is misleading when
iterating over them later.</blockquote>Renamed.<blockquote>the deleting code feels more at
home in CFS constructor than addInitialSSTables.</blockquote>Moved.<blockquote>tracker
parameter is unused now in SSTR.open</blockquote>Removed. I realized that setTrackedBy was
already always call through the DataTracker.addNewSSTablesSize, so I also removed the call
duplication. 


New Comment: 
<blockquote>But we won't have the same ancestor multiple times</blockquote>I don't think
that's true.  Suppose for instance we have leveled compaction with A and B in L0.  They
are larger than 5MB so we split the result into X, Y, and Z.  Next we flush C to L0.  It
overlaps with Y and Z, so we're compacting C, Y, and Z.  Now we have Y and Z both with A
and B as ancestors.(Switching from LCS back to STCS is another way you could get duplicate
ancestors.) 


New Comment: 
You're completely right, I'm still thinking too much in terms of
SizeTieredCompaction.Updated patches to use a Set. 


New Comment: 
+1 


New Comment: 
Committed (to &gt;= 1.1 as per my earlier comment), thanks.I've also removed
Directories.skipCompacted() while merging to trunk. 


