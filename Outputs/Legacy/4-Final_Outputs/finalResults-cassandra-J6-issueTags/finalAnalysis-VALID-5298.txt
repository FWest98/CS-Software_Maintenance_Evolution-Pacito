Pattern changes caused by commit: 5a439b23b19e4b73ac4e0511b619886cc57fc6f4

From: Decorator-1
To:   Decorator-0

From: Flyweight-4
To:   Flyweight-5

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-5298.txt 

commit 5a439b23b19e4b73ac4e0511b619886cc57fc6f4
Author: Jonathan Ellis <jbellis@apache.org>

    remove incorrect optimization from slice read path
    patch by jbellis; reviewed by slebresne for CASSANDRA-3390



==================================
 Issue CASSANDRA-3390 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3390] ReadResponseSerializer.serializedSize() calculation is wrong
-----------------

-----------------
Summary: ReadResponseSerializer.serializedSize() calculation is wrong
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 20 Oct 2011 19:44:42 +0000
-----------------

-----------------
Resolved at: Mon, 24 Oct 2011 20:24:32 +0000
-----------------

-----------------
Assigned to: Jonathan Ellis
-----------------

-----------------
Description: 

in ReadResponse.java

the following code

    public long serializedSize(ReadResponse
response, int version)
    {        int size = DBConstants.intSize;        size +=
(response.isDigestQuery() ? response.digest() :
ByteBufferUtil.EMPTY_BYTE_BUFFER).remaining();        size += DBConstants.boolSize;       
if (response.isDigestQuery())            size += response.digest().remaining();       
else            size += Row.serializer().serializedSize(response.row(), version);       
return size;    }
adds the digest size 2 times

this triggers assertion error in at least
ReadVerbHandler
 

-----------------

-----------------
Comments: 

New Comment: 
should add the digest size only once 


New Comment: 
seems there are still other places where the count is wrongERROR <span
class="error">&#91;ReadStage:129&#93;</span> 2011-10-20 15:54:50,932
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[ReadStage:1<br/>29,5,main]<br/>java.lang.AssertionError: Final buffer length 1778
to accomodate data size of 969 (predicted 888)<br/>        at
org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)<br/>        at
org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)<br/>        at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)<br/>       
at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)<br/>  
     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)<br/>      
 at java.lang.Thread.run(Thread.java:662) 


New Comment: 
it seems that with the remaining errorbuffer.getData().length = size*2 + 1
buffer.getLength() = size + 81this may be of some help in debugging 


New Comment: 
the part i fixed is on the isDigest() path,I'm suspecting that the remaining error is on
the non-digest path, since the sizes are pretty large 


New Comment: 
The first one was fixed in <a href="https://issues.apache.org/jira/browse/CASSANDRA-3373"
title="ReadResponseSerializer doesn&#39;t compute serialized size correctly"
class="issue-link" data-issue-key="CASSANDRA-3373"><del>CASSANDRA-3373</del></a>.  You
should probably retest on latest trunk to make sure the other isn't fixed too. 


New Comment: 
tested with latest on 1.0 branch (I checked that it does have the 3373 fix)still same:    
   at java.lang.Thread.run(Thread.java:662)<br/>ERROR <span
class="error">&#91;ReadStage:57&#93;</span> 2011-10-20 16:48:50,117
AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread<span
class="error">&#91;ReadStage:57,5,main&#93;</span><br/>java.lang.AssertionError: Final
buffer length 560 to accomodate data size of 360 (predicted 279)<br/>        at
org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)<br/>        at
org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)<br/>        at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)<br/>       
at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)<br/>  
     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)<br/>      
 at java.lang.Thread.run(Thread.java:662)<br/>ERROR <span
class="error">&#91;ReadStage:45&#93;</span> 2011-10-20 16:48:50,200
AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread<span
class="error">&#91;ReadStage:45,5,main&#93;</span><br/>java.lang.AssertionError: Final
buffer length 1002 to accomodate data size of 574 (predicted 500)<br/>        at
org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)<br/>        at
org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)<br/>        at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)<br/>       
at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)<br/>  
     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)<br/>      
 at java.lang.Thread.run(Thread.java:662) 


New Comment: 
btw how can I find the github commit corresponding to the 1.0.0 official
release?<br/>thanks<br/>Yang 


New Comment: 
This is the wrong place to ask, but just switch to the 1.0.0 branch: <a
href="https://github.com/apache/cassandra/tree/cassandra-1.0.0" class="external-link"
rel="nofollow">https://github.com/apache/cassandra/tree/cassandra-1.0.0</a> 


New Comment: 
Do you have an easy way to reproduce that remaining error ? Do you use something specific:
super columns ? counters ? expiring columns ? 


New Comment: 
standard columns<br/>not counter,<br/>yes they are expiring columnsI found this seems to
have to do with thread issues, since it works fine with single client 


New Comment: 
I went through the serialize() and serializedSize() code and they seem to agree, now the
most likely cause is thread issue,  would the serialize() ever be called from multiple
threads?the buffer.data().length  should not be in the assert check, that discrepancy
comes from the result of expand(), really that is an internal thing, should not be
checked. 


New Comment: 
Sure it should be, because that's what tells us if serializedSize has a bug. 


New Comment: 
I mean  buffer.getData().length  returns the underlying byte[] length, which is always
doubled on expansion, so this will not be the same as the actual bytes written. actually
this agrees with what we are seeing, getData().length() == (size + 1 ) * 2, because the
buffer is doubled on the last write of one byte.buffer.getLength()  returns the "count"
var, yes, this should be the same as the serializedSize() calculation 


New Comment: 
But we pre-allocate the buffer size:<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">        <span
class="code-object">int</span> size = (<span class="code-object">int</span>)
serializer.serializedSize(object, version);        DataOutputBuffer buffer = <span
class="code-keyword">new</span> DataOutputBuffer(size);</pre></div></div>The point is to
avoid copies during buffer re-allocations, so again, this is a good check to have. 


New Comment: 
I added the object being serialized to the assertion failure message in r1187539.  Does
that give us anything useful? 


New Comment: 
I did this too myselfdoesn't seem particularly helpful, except for telling us that it's a
normal column, with TTLI'm intrigued by why the difference is always 81 or 77 bytesERROR
<span class="error">&#91;ReadStage:176&#93;</span> 2011-10-21 14:45:04,099
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[ReadStage:1<br/>76,5,main]<br/>java.lang.AssertionError: 
row:Row(key=DecoratedKey(58613415544222752604144955634601649391, 3164323233656134),
cf=ColumnFamily(mea<br/>suredSession
[0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656135:false:4@1319222<br/>703553!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656137:false:4@1319222703<br/>556!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656139:false:4@1319222703562<br/>!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656162:false:4@1319222703567!60<br/>0,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656164:false:4@1319222703571!600,0<br/>000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656166:false:4@1319222703584!600,0000<br/>013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656231:false:4@1319222703589!600,0000013<br/>327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656233:false:4@1319222703592!600,]))Final
b<br/>uffer length 1152 to accomodate data size of 652 (predicted 575)<br/>        at
org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:683)<br/>        at
org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)<br/>        at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)<br/>       
at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)<br/>  
     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)<br/>      
 at java.lang.Thread.run(Thread.java:662) 


New Comment: 
I think you're right about the threading being the cause.  Are you using row cache? 


New Comment: 
yes, let me disable rowcache and see if it goes away... 


New Comment: 
yeah, disabled row cache, with multiple threads running, no assertion error so far 


New Comment: 
Jonathan: I see what you mean:the returned CF from cachedRow, in CFS.java:1169 :   
/**<ul>	<li>Filter a cached row, which will not be modified by the filter, but may be
modified by throwing out</li>	<li>tombstones that are no longer relevant.</li>	<li>The
returned column family won't be thread safe.<br/>     */<br/>    ColumnFamily
filterColumnFamily(ColumnFamily cached, QueryFilter filter, int gcBefore)<br/>   
{</li></ul>it seems that for some CFs, the buffer.length() was calculated, then another
thread did the <br/>removeDeletedColumns() for expiring columns in filterColumnFamily(),
then serializedSize() was calculated again, something like that.if that is the case, would
it actually cause correctness problems, or is it just an annoying discrepancy between 2
reports on the size (which reflect the size at 2 times in history)? 


New Comment: 
well, expiring column would lead to the size to be bigger than buffer.length(), <br/>but
what we saw is the reverse, so it could be caused because new columns are added to the
cached CF. 


New Comment: 
It looks like the culprit is this part:<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">                   
<span class="code-keyword">if</span> (sliceFilter.count &gt;=
cached.getColumnCount())</pre></div></div>... then we don't clone the cached CF.  But this
is broken as-written; it's possible for new columns to be inserted, increasing the
cached's row size after the check.I think the right thing to do here is jut get rid of
this special case.  We could try to preserve it by only skipping thie copy when the
requested count == MAXINT, but that's contrary to best-practices (we strongly discourage
using MAXINT as the limit). 


New Comment: 
tried the latest patch, seems to work fine. but understandably, the performance is a bit
slower due to the extra copy (even with arraybackedSortedColumns ) 


New Comment: 
<blockquote>We could try to preserve it by only skipping thie copy when the requested
count == MAXINT</blockquote>I don't think that would work either because If I understand
correctly the problem, it's that when we serialize, we may add a column between when we
compute the serialized size and when we do the actual serialization. The request count
shouldn't make any difference here (besides, I agree with the contrary to best-practices
argument).I'll note that I think this optimization has always been wrong, because the cf
can also change between when the column count is written in the serialization form and the
actual write of the columns. If columns are added, we're kind of fine, we'll serialize
more columns that advertised but the code will be ok. But it's possible to have a race
where we actually remove columns, because a tombstone could be gced by another thread, in
which case we could have a EOFException or something like that during deserialization
(it's sufficiently unlikely that either nobody ran into it, or got it only once and never
again and so didn't report it or something).Anyway +1 on the patch, but I believe it would
be correct to commit to 0.8 too for the above reason. 


New Comment: 
<blockquote>we may add a column between when we compute the serialized size and when we do
the actual serialization</blockquote>Right, I meant that if we were willing to drop the
assert and allow the buffer to expand in the rare case of a race here.I'll commit to 0.8
and 1.0. 


New Comment: 
committed 


