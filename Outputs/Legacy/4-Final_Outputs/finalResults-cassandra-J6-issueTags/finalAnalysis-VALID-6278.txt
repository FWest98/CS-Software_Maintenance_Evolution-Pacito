Pattern changes caused by commit: 34060fc77a77846b60b146fc32af9dd58a21b545

From: Abstract Factory-3
To:   Abstract Factory-2

From: Factory Method-3
To:   Factory Method-2

From: Facade-1
To:   Facade-0

From: Flyweight-3
To:   Flyweight-4


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6278.txt 

commit 34060fc77a77846b60b146fc32af9dd58a21b545
Author: Vijay Parthasarathy <vijay2win@gmail.com>

    EC2 snitch incorrectly reports regions
    patch by Vijay; reviewed by Brandon Williams for CASSANDRA-4026



==================================
 Issue CASSANDRA-4026 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4026] EC2 snitch incorrectly reports regions
-----------------

-----------------
Summary: EC2 snitch incorrectly reports regions
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 8 Mar 2012 23:26:36 +0000
-----------------

-----------------
Resolved at: Mon, 12 Mar 2012 19:01:33 +0000
-----------------

-----------------
Assigned to: Vijay
-----------------

-----------------
Description: 

Currently the org.apache.cassandra.locator.Ec2Snitch reports "us-west" in both the oregon
and the california data centers.  This is incorrect, since they are different
regions.

California =&gt; us-west-1<br/>Oregon     =&gt; us-west-2

wget <a
href="http://169.254.169.254/latest/meta-data/placement/availability-zone"
class="external-link"
rel="nofollow">http://169.254.169.254/latest/meta-data/placement/availability-zone</a>
returns the value "us-west-2a"

After parsing this returns

DC = us-west Rack = 2a

What
it should return

DC = us-west-2 Rack = a

This makes it possible to use multi region when
both regions are in the west coast.
 

-----------------

-----------------
Comments: 

New Comment: 
I ran into this a while ago. But the problem is that we cannot change the settings on the
current live clusters which uses Ec2MultiregionSnitch or Ec2Snitch without taking a
downtime. (If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the schema
for the existing cluster).Option 1: Leave the existing snitch as it is and add a new
snitch.<br/>Option 2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as
us-west-2 is fairly new it wont affect a lot of us?Brandon, Thoughts? 


New Comment: 
Could we create a new snitch that corrects the problem and deprecate the existing
snitches?  This way people can migrate if they choose to, or keep the old snitches if it
does not affect them. 


New Comment: 
I am ok with deprecating it, but the problem is unless the users have a way out of this we
cannot remove it. 


New Comment: 
<blockquote>If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the
schema for the existing cluster</blockquote>Not the schema per se, but the datacenter
name.  This is doable though if you're willing to repair afterwards.  Another option is to
switch an entire DC's snitch at a time.<blockquote>Option 1: Leave the existing snitch as
it is and add a new snitch.</blockquote>Ugh, that will cause tremendous confusing for new
users.  It would however be nice to get rid of this wart at some point.<blockquote>Option
2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as us-west-2 is fairly
new it wont affect a lot of us?</blockquote>There aren't a lot good options here, I'm not
sure how I feel about this one since it's definitely a hack, but only appending the number
to the DC if &gt; 1 might be the least painful for existing users. 


New Comment: 
&gt;&gt;&gt; This is doable though if you're willing to repair afterwards.<br/>The problem
is that StorageProxy will start to write the data to the nodes which are not suppose to
have the data (During upgrade, and restart takes a while)... hence after recovery they
will not be able to be recovered via repair (Lets say Node A, B, C, D if B and C are
upgraded A will start to write the data to D for thinking it as this datacenters
replica).&gt;&gt;&gt; it's definitely a hack, but only appending the number to the DC if
&gt; 1 might be the least painful for existing users.<br/>Agree, and attached patch does
this.BTW: The attached patch can break after we AWS has 24 AZ's which is highly unlikely
but i will create a ticket requesting for API for Regions instead of AZ. 


New Comment: 
<blockquote>hence after recovery they will not be able to be recovered via
repair</blockquote>One replica will always be in the right spot so you can
repair.<blockquote>BTW: The attached patch can break after we AWS has 24 AZ's which is
highly unlikely but i will create a ticket requesting for API for Regions instead of
AZ.</blockquote>That would be great.  Unfortunately when we have that, we'll still have to
munge the name (and rack names) to be backwards compatible <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/sad.png" height="16" width="16"
align="absmiddle" alt="" border="0"/>This ticket makes me sad, but +1. 


New Comment: 
Committed to trunk and 1.0, Thanks! 


New Comment: 
Not to reopen ancient tickets, but code was added in <a
href="https://issues.apache.org/jira/browse/CASSANDRA-5897"
title="GossipingPropertyFileSnitch does not auto-reload local rack/dc" class="issue-link"
data-issue-key="CASSANDRA-5897"><del>CASSANDRA-5897</del></a> to pull in SnitchProperties.
Would it not be desirable to address this by looking at an additional property and
choosing between legacy naming and a full, consistent with other EC2 APIs, naming
scheme?Something similar to this commit <a
href="https://github.com/ramsperger/cassandra/commit/bf1ee0251e5cf46b8af28282e22c4fbf29d85f33?w=1"
class="external-link"
rel="nofollow">https://github.com/ramsperger/cassandra/commit/bf1ee0251e5cf46b8af28282e22c4fbf29d85f33?w=1</a>? 


New Comment: 
<a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ramsperger"
class="user-hover" rel="ramsperger">Gregory Ramsperger</a> go ahead an open a new ticket
for that. 


