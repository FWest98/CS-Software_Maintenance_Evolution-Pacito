Pattern changes caused by commit: 7b4639ab26d6c1f174bab07a32eb7838c1c51c09

From: Abstract Factory-1
To:   Abstract Factory-2

From: Factory Method-1
To:   Factory Method-2


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-8059.txt 

commit 7b4639ab26d6c1f174bab07a32eb7838c1c51c09
Author: Jonathan Ellis <jbellis@apache.org>

    improve error message on partitioner mismatch
    patch by jbellis; reviewed by slebresne for CASSANDRA-4843



==================================
 Issue CASSANDRA-4843 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4843] When upgrading from 1.1.6 to 1.20 change in partitioner causes nodes not to start
-----------------

-----------------
Summary: When upgrading from 1.1.6 to 1.20 change in partitioner causes nodes not to start
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Sun, 21 Oct 2012 14:15:29 +0000
-----------------

-----------------
Resolved at: Tue, 27 Nov 2012 15:36:18 +0000
-----------------

-----------------
Assigned to: Jonathan Ellis
-----------------

-----------------
Description: 

ERROR 10:17:20,341 Cannot open
/home/edward/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hf-1 because
partitioner does not match org.apache.cassandra.dht.RandomPartitioner !=
org.apache.cassandra.dht.Murmur3Partitioner

This is because 1.2 has a new default
partitioner, why are we changing the default? Is this wise? The current partitioner has
been rock solid for years. 

Should the previously known partition be stored in the schema
like the previously know seed nodes, and schema?
 

-----------------

-----------------
Comments: 

New Comment: 
The error message is strange as well.Changing paritioner on a existing cluster can cause
data loose, Please verify your partitioner in cassandra.yaml"Can cause data *loss" is
impossible because Cassandra will not even start. If it did start it would definitely
cause data loss. 


New Comment: 
Also the comments say, <div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre># - RandomPartitioner distributes rows
across the cluster evenly by md5.#   When in doubt, this is the best
option.</pre></div></div>If this is the best option why is another option chosen as the
default? 


New Comment: 
<blockquote>why are we changing the default?</blockquote>The reason is that md5 is a bit
cpu intensive and in some 2ndary index requests (that does a token computation for each
key on disk it scans for internal reason that can't be changed easily) this was a
bottleneck. The new default is the Murmur3Partitioner that is much cheaper to compute.
Besides, for every query we do compute a bunch of token and vnodes will probably not
reduce that, so it's a generic improvement.That being said, I fully agree that the current
upgrade experience is pretty harsh (even the NEWS file don't clearly explain the action to
take to avoid this error). And since we save the partitonner and don't start if the user
change it in the yaml, maybe it's time to change the behavior so that if a partitioner is
saved in the system table, we use that (and log a warning if it differs from the yaml
configured one). 


New Comment: 
<blockquote>maybe it's time to change the behavior so that if a partitioner is saved in
the system table, we use that</blockquote>Partitioner is saved per-sstable so we can do
this panic doublecheck, but you have to know (or think you know) the partitioner before
you can actually open up a system table.  I could be wrong, but I don't think it's a quick
fix.In the meantime, I've updated the comments and error message in
8f3d9b8371fa7c5dea83d45d83ec7fe4911a96c0. 


New Comment: 
<blockquote>but you have to know (or think you know) the partitioner before you can
actually open up a system table</blockquote>I don't think you do since the system table
keyspace uses LocalStrategy (and this is hardcoded). That being said, even if we start
saving the partitioner in the system table, it's possibly a little late for the upgrade to
1.2. I still think we should do it for 1.1.7 though (but to be clear, I'm all for keeping
the panic doublecheck when we open a sstable because that protects again a different
problem anyway). 


New Comment: 
We missed the 1.1.7 window so I think it's pretty safe to guess that most 1.2 upgraders
won't have any extra safety information we add to 1.1.8.Patch attached to clarify the
exception message. 


New Comment: 
+1 


