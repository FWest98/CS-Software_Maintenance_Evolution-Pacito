Pattern changes caused by commit: 263f192b65db18e3cfb5126e86358f374a0be5fa

From: Decorator-2
To:   Decorator-1

From: Flyweight-2
To:   Flyweight-1

From: Mediator-3
To:   Mediator-1

From: Template Method-3
To:   Template Method-2


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6693.txt 

commit 263f192b65db18e3cfb5126e86358f374a0be5fa
Author: Sylvain Lebresne <sylvain@datastax.com>

    Cleanup DTPE usage after CASSANDRA-3494



==================================
 Issue CASSANDRA-3494 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3494] Streaming is mono-threaded (the bulk loader too by extension)
-----------------

-----------------
Summary: Streaming is mono-threaded (the bulk loader too by extension)
-----------------

-----------------
Issue type: Improvement
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Tue, 15 Nov 2011 16:40:51 +0000
-----------------

-----------------
Resolved at: Tue, 6 Dec 2011 02:23:36 +0000
-----------------

-----------------
Assigned to: Peter Schuller
-----------------

-----------------
Description: 

The streamExecutor is define as:
<div class="preformatted panel" style="border-width:
1px;"><div class="preformattedContent panelContent"><pre>streamExecutor_ = new
DebuggableThreadPoolExecutor("Streaming", Thread.MIN_PRIORITY);</pre></div></div>
In the
meantime, in DebuggableThreadPoolExecutor.java:
<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>public
DebuggableThreadPoolExecutor(String threadPoolName, int priority){   this(1,
Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), new
NamedThreadFactory(threadPoolName, priority));}</pre></div></div>
In other word, since the
core pool size is 1 and the queue unbounded, tasks will always queued and the executor is
essentially mono-threaded.<br/>This is clearly not necessary since we already have stream
throttling nowadays. And it could be a limiting factor in the case of the bulk
loader.

Besides, I would venture that this maybe was not the intention, because putting
the max core size to MAX_VALUE would suggest that the intention was to spawn threads on
demand. 
 

-----------------

-----------------
Comments: 

New Comment: 
This was intentional to start with (it pre-dates the stream throttling).  And if we can
hit the throttle cap w/ a single thread (we can) then I'm not sure that part needs to be
multithreaded.The sstable building pre-stream seems like a more profitable area to
multithread for the bulkloader scenario. 


New Comment: 
In cases like decommission, or the bootstrapping of multiple nodes, multithreading would
result in distribution of the throttled bandwidth across more of your cluster, rather than
across one switch at a time, for example. 


New Comment: 
We actually just ran into this tonight. We had the network bandwidth, but because it was
monothreaded, we couldnt fully saturate the NIC with 1 thread. It hurt us pretty hard
since we needed to transfer 18TB in under 6 hours. Please increase this. 


New Comment: 
Just to confirm, you had stream_throughput_outbound_megabits_per_sec set to zero?  It
defaults to 400Mbps when not set. 


New Comment: 
We had it set very high. Throttling is not the problem here. The problem is that each node
was sending data to only one bootstrapping node at a time, and that process was
bottlenecking on the destination node's write capacity (see <a
href="https://issues.apache.org/jira/browse/CASSANDRA-3549" title="streaming is hard-coded
to be single-threaded" class="issue-link"
data-issue-key="CASSANDRA-3549"><del>CASSANDRA-3549</del></a> too).Another effect is that
nodes weren't evenly bootstrapping across the cluster; so some nodes were at 200 gigs
whiles other were at 500, because of the pseudo-random ordering restriction imposed by
this limitation (multiple destinations dogpiling on a source -&gt; some have to just wait
for a long time before they even begin receiving data). In addition to the aggregate
bandwidth limit, this caused even more delay to get to the state where all nodes were
bootstrapped. 


New Comment: 
(Part of the reason the destinations were bottlenecking were that the sstable rebuilds +
minor compactions happening during streaming are expensive and eat disk throughput. So
you'd have a bunch of machines just idling waiting for the sender to send them some data,
while the sender is blocking on a write to a TCP conenction, where on the other end some
receiver is blocking on a write to disk because that node happened to be lucky and got
some good sources and got more streams to it than it could handle in terms of throughput.) 


New Comment: 
Just had a thought (will have to check later): I hope we're not <b>also</b> waiting for
the entire streaming session to complete before the next streaming session is allowed to
run, because that would imply also waiting on the SSTable rebuild on the destination
(though no longer in 1.0). 


New Comment: 
I can submit a patch to make this a tunable, defaulting to something semi-reasonable like
10. Do we however believe there are concurrency issues here? A very very brief sifting of
the code looked to me like we're alright. 


New Comment: 
A complication is that merely upping the concurrency works, but given that you may have a
lot of files destined for one particular host, you actually need to allow a <b>lot</b> of
streams in order to ensure that you're streaming to all nodes that want files. At some
point you're gonna die seek death from the amount of readers. 


New Comment: 
I suggest having a single executor per target node, and creating the executors on-demand
(under a simple lock, initiating streaming is not a critical code path). Opinions? That
would mean one active stream per destination node. 


New Comment: 
Attaching a "principle" diff against our internal 0.8 for cursory examination. I want to
submit another one for inclusion for 1.0 later (it's blocking on some 1.0 work that I have
to do first).The idea is this: Now we have one executor per destination host. In order to
avoid complex synchronization we never bother removing an executor once created; but this
is fine because we make sure threads time out so the cost is just the executor instance
and not a thread, for destinations that are not being streamed to.Make the tracking of
active streams for throttling purposes be explicit, to avoid iterating over the O<img
class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> map. 


New Comment: 
Attaching version rebased for 1.0 (and tested). 


New Comment: 
I like Peter's idea about having one executor per destination.<br/>+1 on patch against
1.0. (Patch for 0.8 needs rebase but the change is essentially the same.) 


New Comment: 
If you want to apply this to 0.8 I can submit an 0.8 patch which is properly rebased
against the 0.8 branch instead of our internal one. I was assuming it would only be going
into 1.x. Want one? 


New Comment: 
Sure. Why not? 


New Comment: 
I'm pretty sure we don't want to commit this in 0.8. This is clearly an optimization (not
a bug or regression fix), so there is no point in taking any risks with 0.8. To be honest,
I wouldn't be opposed to putting this in trunk only. 


New Comment: 
+1 trunk-only 


New Comment: 
The patch applies cleanly to trunk. 


New Comment: 
committed to 1.1.Created <a href="https://issues.apache.org/jira/browse/CASSANDRA-3576"
title="make AbstractSSTableSimpleWriters threadsafe" class="issue-link"
data-issue-key="CASSANDRA-3576"><del>CASSANDRA-3576</del></a> to follow up on bulk
loader-specific multithreading. 


New Comment: 
Integrated in Cassandra #1238 (See <a href="https://builds.apache.org/job/Cassandra/1238/"
class="external-link"
rel="nofollow">https://builds.apache.org/job/Cassandra/1238/</a>)<br/>    multithreaded
streaming<br/>patch by Peter Schuller; reviewed by yukim for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-3494" title="Streaming is
mono-threaded (the bulk loader too by extension)" class="issue-link"
data-issue-key="CASSANDRA-3494"><del>CASSANDRA-3494</del></a>jbellis : <a
href="http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1210748"
class="external-link"
rel="nofollow">http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1210748</a><br/>Files
:
<ul>	<li>/cassandra/trunk/CHANGES.txt</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/streaming/FileStreamTask.java</li></ul> 


New Comment: 
Does this actually work?  My reading of ThreadPoolExecutor suggests that an unbounded
queue will result in no non-core threads ever being created. 


New Comment: 
I did test it, yes. Streaming does work, and it does indeed not retain an idle thread per
destination beyond end of streaming and timeout (verified with jstack). I'd have to go
back to the code again (I also looked at ThreadPoolExecutor) to address your concern
specifically. What problem are you seeing? 


New Comment: 
This work because of some details of ThreadPoolExecutor. Namely, in the sentence "When a
new task is submitted in method execute(java.lang.Runnable), and fewer than corePoolSize
threads are running, a new thread is created to handle the request, ...", the "fewer" is
meant strictly. So with a 0 for corePoolSize, it will create a thread for the first task.
It will also terminate that thread afterward because the doc says "If the pool currently
has more than corePoolSize threads, excess threads will be terminated if they have been
idle for more than the keepAliveTime".However, this is dodgy code as for instance if
maxCorePoolSize was 10, the thread pool would <b>not</b> create up to 10 threads, it would
create one and then queue up all other tasks.In fact, the documentation is actually
inconsistent, as for unbounded queues it states that "Thus, no more than corePoolSize
threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have
any effect.)". But this is not true if corePoolSize is 0.I believe the correct way to
create an executor with a max number of threads and where all threads timeout if unused
with ThreadPoolExecutor is to set corePoolSize == maxPoolSize == whateverTheMaxShouldBe (1
for that patch) and to use ThreadPoolExecutor.allowCoreTheadTimeout(). 


New Comment: 
<blockquote>Thus, no more than corePoolSize threads will ever be
created</blockquote>Right, that was the source of my confusion.<blockquote>I believe the
correct way to create an executor with a max number of threads and where all threads
timeout if unused with ThreadPoolExecutor is to set corePoolSize == maxPoolSize ==
whateverTheMaxShouldBe (1 for that patch) and to use
ThreadPoolExecutor.allowCoreTheadTimeout().</blockquote>+1 


New Comment: 
+1. I no longer remember why I wanted 0, in light of re-reading the javadocs. Unless I
observed empirically that threads were retained. I remember there was <b>some</b> reason
that caused me to start looking at the JDK source. So if changing to 1 I'd suggest just
making sure that threads do indeed die off properly. 


New Comment: 
Attaching patch implementing the change described above. 


New Comment: 
        allowCoreThreadTimeOut(true) is default behavior in DTPE 


New Comment: 
oh right, fixed. 


New Comment: 
is there still a meaningful distinction b/t createWithFixedPoolSize and
createWithMaximumPoolSize? 


New Comment: 
As much as there was before my remove of the allowCoreThreadTimeOut line <img
class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.png"
height="16" width="16" align="absmiddle" alt="" border="0"/><br/>Basically createWithFixed
doesn't timeout it's thread, while createWithMaximum does it. Not claiming the names are
perfect, though I'm not too unhappy with them. 


