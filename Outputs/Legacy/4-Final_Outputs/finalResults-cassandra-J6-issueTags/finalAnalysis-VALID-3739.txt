Pattern changes caused by commit: dc89826fd89720da0c5ad2de9fb54afeb712136a

From: Abstract Factory-2
To:   Abstract Factory-3

From: Factory Method-2
To:   Factory Method-3

From: Mediator-1
To:   Mediator-2


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-3739.txt 

commit dc89826fd89720da0c5ad2de9fb54afeb712136a
Author: Sylvain Lebresne <slebresne@apache.org>

    Make repair work on a token range instead of the full ring
    patch by slebresne; reviewed by stuhood for CASSANDRA-2324



==================================
 Issue CASSANDRA-2324 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-2324] Repair transfers more data than necessary
-----------------

-----------------
Summary: Repair transfers more data than necessary
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Mon, 14 Mar 2011 19:43:08 +0000
-----------------

-----------------
Resolved at: Sun, 10 Apr 2011 18:16:27 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

To repro: 3 node cluster, stress.java 1M rows with -x KEYS and -l 2.  The index is enough
to make some mutations drop (about 20-30k total in my tests).  Repair afterwards will
repair a large amount of ranges the first time.  However, each subsequent run will repair
the same set of small ranges every time.  INDEXED_RANGE_SLICE in stress never fully works.
 Counting rows with sstablekeys shows there are 2M rows total as expected, however when
trying to count the indexed keys, I get exceptions like:
<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>Exception in
thread "main" java.io.IOException: Key out of order!
DecoratedKey(101571366040797913119296586470838356016,
0707ab782c5b5029d28a5e6d508ef72f0222528b5e28da3b7787492679dc51b96f868e0746073e54bc173be927049d0f51e25a6a95b3268213b8969abf40cea7d7)
&gt; DecoratedKey(12639574763031545147067490818595764132,
0bc414be3093348a2ad389ed28f18f0cc9a044b2e98587848a0d289dae13ed0ad479c74654900eeffc6236)   
    at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:206)     
  at
org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:388)</pre></div></div> 

-----------------

-----------------
Comments: 

New Comment: 
Key out of order is because sstableexport doesn't know that index sstables use
LocalPartitioner instead of the cluster partitioner RP or BOPP. 


New Comment: 
It looks like INDEXED_RANGE_SLICE is broken in stress.java, so the only problem here is
repair doing superfluous work. 


New Comment: 
The problem is, the ranges repair hashes are not actual node ranges.Let's consider the
following ring (RF=2), where I consider token being in <span
class="error">&#91;0..12&#93;</span> to simplify, and where everything is consistent:<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>                  _.-""""-._ C (token: 11)  .'          `. [11,3][3,7] 
 /              \              |                |              |                | A(token:
3)              |                | [3,7],[7,11]               \              /            
   `._        _.'       B (token: 7)`-....-'       [7,11],[11,3]</pre></div></div>Now say
I run a repair on node A. The problem is that the Merkle tree ranges are built by dividing
the full range by 2 recursively. This means that in this example, the ranges in the tree
will for instance be <span class="error">&#91;0,2&#93;</span>, <span class="error">&#91;2,
4&#93;</span>, <span class="error">&#91;4, 6&#93;</span>, <span class="error">&#91;6,
8&#93;</span>, <span class="error">&#91;8,10&#93;</span> and <span
class="error">&#91;10,12&#93;</span>.If you compare the hashes for A and B on those
ranges, changes are you'll find mismatches for <span class="error">&#91;6,8&#93;</span>
and <span class="error">&#91;10,12&#93;</span> (because A don't have anyone on <span
class="error">&#91;11, 12&#93;</span> while B have, and B don't have anyone on <span
class="error">&#91;6, 7&#93;</span> while A have). As a consequence, the range <span
class="error">&#91;7,8&#93;</span> and <span class="error">&#91;10,11&#93;</span> will be
repaired, even though there is no inconsistencies.What that means in practice is that it
will be very rare for anti-antropy to actually consider the nodes in sync, it will almost
surely "repair" something, even if the nodes are perfectly consistent. It's Very easy to
check btw: with a cluster right the one above (3 nodes, RF=2), with as few as 5 keys for
the whole cluster I'm able to have a repair do repairs over and over again.Now the good
question is: how bad is it ? I'm not sure, I depends a bit.On a 3 nodes cluster (RF=2), I
tried inserting 1M keys with stress (stress -l 2) and triggered repair afterwards. The
amount of (unnecessarily) repaired keys was around 150 keys for a given node (it varies
slightly for run to run because there is some randomness in the creation of the Merkle
tree), corresponding to ~44KB streamed (that is the amount transfered to the node where
repair has been ran, so for the total operation its twice this, since we stream in both
ways). That's ~0.02% of keys (a given node have ~666 666 keys).  It's bad to do useless
work, but not a really big deal.However, the less keys we'll have, the worst it gets (and
the bigger our rows are, the more useless transfer we do). With the same experiment
inserting only 10K keys, there is 190 keys uselessly repaired. That's now close to 3% of
the load. It also gets worst with increasing replication factor.To fix this, we would need
for the range in the Merkle tree to "share" the node range boundaries. An interesting way
to do this would be to have the coordinating node give a list a range for which to
calculate Merkle trees, and the node would compute one tree by range (for the coordinating
node, that would be #RF's tree). A nice think with this is that it would leave room to
optimizing repair since a node would need to do a validation compaction only on the range
asked for, which means that only the coordinator node would validate all its data. The
neighbors would do less work. 


New Comment: 
<blockquote>To fix this, we would need for the range in the Merkle tree to "share" the
node range boundaries</blockquote>couldn't we just take the interesection of the computed
ranges w/ the range actually being repaired? 


New Comment: 
<blockquote>couldn't we just take the interesection of the computed ranges w/ the range
actually being repaired?</blockquote>We do that. But the problem is: you're node A and you
receive a merkle tree from B that in particular says that for the range <span
class="error">&#91;0..10&#93;</span> the hash is x. And on <span
class="error">&#91;0..10&#93;</span> your has is x'. The problem is when <span
class="error">&#91;0..10&#93;</span> is partly one of your range, partly not. For instance
it can be that you're a replica for <span class="error">&#91;8..10&#93;</span> but not at
all for <span class="error">&#91;0..8&#93;</span>.<br/>This is due to the fact that the
ranges for which the hashes are computed are computed without concern for actual node
ranges. So now you know there is some inconsistency on <span
class="error">&#91;0..10&#93;</span> but it may just be that B is responsible for <span
class="error">&#91;0..8&#93;</span> and have data for it (and we don't since we are not in
charge of that).<br/>In that case, the code do take the intersection of <span
class="error">&#91;0..10&#93;</span> with the local range and will stream only <span
class="error">&#91;8..10&#93;</span>. But it's still useless. 


New Comment: 
I thought repair is per-token-range, i.e., if I say "nodetool repair A" then range (11, 3]
and (3, 7] will be repaired independently. 


New Comment: 
No, not if I read this code correctly (but I think it should, that's roughly what I'm
proposing to do).Actually thinking about it, there is probably no need to construct
multiple merkle trees, it will be enough for neighbors to only add to the tree the keys
that are in the range of the node asking for the tree. 


New Comment: 
So what about this:<ul class="alternate" type="square">	<li>change the atom of repair (in
nodetool + StorageService) to be a single token range, so it's unambiguous what we're
repairing.  This has the side benefit of making it enormously easier to repair an entire
cluster w/o doing redundant work.</li>	<li>provide backwards compatibility w/ existing
repair command by splitting it into RF repair ranges and waiting on each of those futures
in StorageService mbean</li></ul> 


New Comment: 
Sounds good, will do. 


New Comment: 
Attached patch modify repair to operate on one token range at a time. Nodetool repair
schedule as many repair session than the node have ranges to perform a full node repair.
Note that this is more efficient than previously, since the neighbors of the node will
only do a validation compaction on the range they have in common with the node
coordinating the repair (instead of validating everything).This moreover makes it trivial
to add an option to nodetool so that the node only repair it's primary range. That way,
you can repair a full cluster by calling this operation on every node and there is no
duplication of work. The patch doesn't add this option yet though.The patch is against
trunk. Because the way we construct the merkleTree is fundamentally different, the trees
created by 0.7 cannot be compared to the ones created with this patch. The strategy this
patch adopts with respect to talking to 0.7 nodes is this:<ul>	<li>If a 0.7 node asks for
a merkleTree, since we are still able to do a full compaction validation, we do it and
answer with that.</li>	<li>Since a 0.7 node cannot do a merkleTree that would be ok for
us, we simply exclude 0.7 nodes from the endpoints we ask merkleTree from.</li></ul>I
don't feel this is a trivially enough patch to go to the 0.7 branch. 


New Comment: 
This change definitely makes sense: thanks for tackling it. The original implementation
was intended to take advantage of naturally occurring compactions: I would still like to
get in a position where that is possible, but living with the existing implementation
until then isn't worth it.From a quick skim: forceTableRepair incorrectly reports that the
session has failed if the client thread dies: the repair will continue in the background
(or used to). 


New Comment: 
I'll give this a more complete review over the weekend. 


New Comment: 
<ul>	<li>SSTableBoundScanner might be much simpler if it iterates within a list of file
offsets, as returned by
SSTableReader.getPositionsForRanges</li>	<li>SSTableReader.getKeySamples could perform two
binary searches for min and max rather than doing sequential comparisons to the
keys</li></ul>Thanks again Sylvain: this is great! 


New Comment: 
<blockquote>SSTableBoundScanner might be much simpler if it iterates within a list of file
offsets, as returned by SSTableReader.getPositionsForRanges</blockquote>Good call, that's
much simpler. Thanks.<blockquote>SSTableReader.getKeySamples could perform two binary
searches for min and max rather than doing sequential comparisons to the
keys</blockquote>Yeah, realized getKeySamples was buggy anyway since it wasn't handling
wrapping ranges correctly.Attaching patch that simplify the bounded scanner and fixes
getKeySamples. 


New Comment: 
+1<br/>Thanks! 


New Comment: 
Committed as r1090840. Thanks. 


