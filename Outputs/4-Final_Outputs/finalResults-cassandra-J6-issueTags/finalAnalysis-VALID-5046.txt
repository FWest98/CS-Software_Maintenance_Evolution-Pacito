Pattern changes caused by commit: b277e4f5cc48719a32ba42f61cf9efab79cbca48

From: Decorator-1
To:   Decorator-0

From: Flyweight-4
To:   Flyweight-5

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-5046.txt 

commit b277e4f5cc48719a32ba42f61cf9efab79cbca48
Author: Brandon Williams <brandonwilliams@apache.org>

    Increase FD limit to 100k in packaging.
    Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-3206



==================================
 Issue CASSANDRA-3206 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3206] increase file descriptor limit in deb, rpm packages
-----------------

-----------------
Summary: increase file descriptor limit in deb, rpm packages
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Wed, 14 Sep 2011 16:04:51 +0000
-----------------

-----------------
Resolved at: Fri, 23 Sep 2011 21:23:54 +0000
-----------------

-----------------
Assigned to: paul cannon
-----------------

-----------------
Description: 

We can use a lot of file descriptors (one per socket, 5? per sstable).  People hit this
regularly on the user list and it will get worse with Leveled compaction, which limits
sstable size to a relatively low size (currently 5MB).
 

-----------------

-----------------
Comments: 

New Comment: 
I'd suggest sizing this based on the number of sstables we need to support a "sane" amount
of storage.  1TB?  2? 


New Comment: 
<blockquote>5? per sstable</blockquote>This is incorrect &#8211; when using mmap'd I/O, we
don't consume FD-per-sstable (we use one temporarily when setting up the mmap, then
release it).  And even in buffered I/O mode we use one FD per sstable being read, per
reading thread.  (Which will be much less than one FD per sstable in most cases, although
range scans under LevelDB do not yet do a very good job of cutting back the number of
SSTables consulted.)So it sounds like we mostly need to make sure we have it high enough
that we tolerate high numbers of unpooled connections (common in PHP environments, I'm
told). 


New Comment: 
Jeremy pointed out that we can also use a large number of FDs during compaction: either
major compaction for tiered strategy, or a L0 compaction under Leveled, can open an
effectively arbitrary number of sstables if compaction is behind. 


New Comment: 
So...  64K? 


New Comment: 
My two cents: Just go wild. I really don't see the need to be conservative. On any modern
system that you run Cassandra on, the resources consumed by file descriptors is going to
be irrelevant and I don't see when you'd ever actually want Cassandra to hit the limit,
unless it's <b>completely</b> run-away and buggy in which case the limit need not be low.
Better a very high number so people don't run into it, than try to shave off.64k seems
reasonable, I'd be fine with 250k <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/wink.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
SGTM. 


New Comment: 
I concur. I'll go with 100k unless someone has a good argument for having it be higher. 


New Comment: 
Committed. 


