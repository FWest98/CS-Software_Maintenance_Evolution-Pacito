Pattern changes caused by commit: 94fa82558f198489e0eefb0c14392607d5d23224

From: Decorator-1
To:   Decorator-2

From: Facade-1
To:   Facade-0

From: Flyweight-1
To:   Flyweight-2

From: Mediator-1
To:   Mediator-3

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-7900.txt 

commit 94fa82558f198489e0eefb0c14392607d5d23224
Author: Vijay Parthasarathy <vijay2win@gmail.com>

    update measure to measureDeep
    patch by vijay reviewed by jbellis for CASSANDRA-4860



==================================
 Issue CASSANDRA-4860 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4860] Estimated Row Cache Entry size incorrect (always 24?)
-----------------

-----------------
Summary: Estimated Row Cache Entry size incorrect (always 24?)
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Wed, 24 Oct 2012 22:11:03 +0000
-----------------

-----------------
Resolved at: Mon, 13 May 2013 20:04:48 +0000
-----------------

-----------------
Assigned to: Vijay
-----------------

-----------------
Description: 

After running for several hours the RowCacheSize was suspicious low (ie 70 something MB) 
I used  <a href="https://issues.apache.org/jira/browse/CASSANDRA-4859" title="Include
number of entries in CachedService" class="issue-link"
data-issue-key="CASSANDRA-4859"><del>CASSANDRA-4859</del></a> to measure the size and
number of entries on a node:

In <span class="error">&#91;3&#93;</span>:
1560504./65021<br/>Out<span class="error">&#91;3&#93;</span>: 24.0

In <span
class="error">&#91;4&#93;</span>: 2149464./89561<br/>Out<span
class="error">&#91;4&#93;</span>: 24.0

In <span class="error">&#91;6&#93;</span>:
7216096./300785<br/>Out<span class="error">&#91;6&#93;</span>: 23.990877204647838

That's
RowCacheSize/RowCacheNumEntires  .  Just to prove I don't have crazy small rows the mean
size of the row <b>keys</b> in the saved cache is 67 and Compacted row mean size: 355.  No
jamm errors in the log

Config notes:<br/>row_cache_provider:
ConcurrentLinkedHashCacheProvider<br/>row_cache_size_in_mb: 2048

Version
info:
<ul>	<li>C*: 1.1.6</li>	<li>centos 2.6.32-220.13.1.el6.x86_64</li>	<li>java 6u31
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)</li></ul> 

-----------------

-----------------
Comments: 

New Comment: 
Simple patch to fix the issue....Looks like the issue is that we do Mesure (instead of
MeasureDeep) which will not measure the byte[] attached to the RK.*-for-11.patch is for
1.1 and *.patch is for 1.2. 


New Comment: 
+1 


New Comment: 
Committed Thanks! 


New Comment: 
I've been working on a 2.0/1.2 read performance regression analysis (<a
href="http://goo.gl/KHZfL" class="external-link" rel="nofollow">report is here</a>) and it
brought me back to this ticket.The patch as it was applied in revision
94fa82558f198489e0eefb0c14392607d5d23224 introduces a read performance penalty of about
~20%.I've tested reverting this patch and applying on top of the latest 2.0 branch and it
greatly improves read performance. I don't claim to know what this patch really does, so
not sure if that's an appropriate action to take or not, but I've attached my patch here
regardless (trunk-4860-revert.patch). 


New Comment: 
Ryan, so the results has row caching enabled? (For the record this shows up only when
caching is enabled and using InHeap RowCache).<br/>20% over head is a lot more than i
initially anticipated.I could think of one alternative: I have to dig my history, but i
had a alternative to measureDeep() which was very similar to <a href="http://goo.gl/oqD89"
class="external-link" rel="nofollow">http://goo.gl/oqD89</a>, but it doesn't cover all the
platforms and was lot more complicated so it was never committed. Should we resurrect it? 


New Comment: 
Hi <a
href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vijay2win%40yahoo.com"
class="user-hover" rel="vijay2win@yahoo.com">Vijay</a>, I have not tweaked any row cache
settings from the default. This is in my cassandra.yaml:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">#
Maximum size of the row cache in memory.# NOTE: <span class="code-keyword">if</span> you
reduce the size, you may not get you hottest keys loaded on startup.## Default value is 0,
to disable row caching.row_cache_size_in_mb: 0</pre></div></div>From the description, it
sounds like I should have row caching turned off, and yet this code is still being run as
evidenced by the change in performance by reverting your patch. I don't yet have a deep
understanding of the features involved here, so if you have any other suggestions for
things to test here, please let me know. Thanks! 


New Comment: 
Ahaa missed it, it is the key cache which is slowing down the performance. 


New Comment: 
Hi Ryan, <br/>Since you have the environment do you mind testing -v2?<br/>It is not a
final patch, I have to verify the accuracy of the estimate though. 


New Comment: 
With your v2 patch applied I get an average read rate of 14276. That's much worse actually
than the first patch. To make sure something is not amiss, I re-ran the 2.0 baseline and
got comparable results as before (22524). The number we're hoping to back to is ~28000. 


New Comment: 
Hi Ryan, can you try this one? I am really optimistic that this patch should improve the
performance without sacrificing the accuracy of measurement of the memory
footprint.<br/>Attached patch doesn't use any reflection, Micro benchmark shows a better
performance than any other approach.<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">Completed warmup!,
<span class="code-object">Number</span> of Iteratoions: 1000000Using reflection took:
8113Using 4860-v3 took: 95Using MemoryMeter meter.measure(key) took: 190Using MemoryMeter
meter.measureDeep(key) took: 982</pre></div></div>Note: We don't have this optimization
when we have a range tombstone in KeyCache (coz the code becomes really complex), and
while using RowCache.Let me know if you want me to publish the accuracy test and perf test
code. 


New Comment: 
This one (v3 patch) is much better. Average read op rate is 26484. That's the best run
I've seen on 1.2+ except for the run with the trunk-4860-revert.patch applied which has an
average read op rate of 27809 (4.8% faster). 


New Comment: 
Ryan, Micro benchmark shows v3 is better, is there any chance you are hitting the same key
in the key cache often? The reason for asking is that you might have a smaller key cache
since the calculation is more accurate. If yes then i would increase the keycache and try.
To be clear the Meter.measure() is bug and cannot be used in production and can cause OOM. 


New Comment: 
I'm hitting the same key multiple times in the write, but not the read:stress -F 2000000
-n 20000000 -i 1<br/>stress -n 2000000 -o read -i 1 


New Comment: 
This is my theory:2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 *
2000000 bytes) will fit in key cache.<br/>2M KV with measureDeep() will take 96M + 48M (48
* 2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key size.Hence
there is a eviction overhead on the keycache which you dont have in
Measure.measure().<br/>Give the above if you have the key cache of 300M and re test both
v3 should show a better performance.<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">Completed warmup!,
<span class="code-object">Number</span> of Iteratoions: 1000000Using reflection took:
8037Using 4860-v3 took: 90Using MemoryMeter meter.measure(key) took: 190Using MemoryMeter
meter.measureDeep(key) took: 1002Using 4860-v3 RowIndexEntry took: 14Using MemoryMeter
meter.measure(RowIndexEntry(i)) took: 104Using MemoryMeter
meter.measureDeep(RowIndexEntry(i)) took: 459Size of Meter.measure key: 24Size of
Meter.measure index: 24Size of Meter.measureDeep key: 48Size of Meter.measureDeep index:
24Size of key: 48Size of index: 24</pre></div></div> 


New Comment: 
Added unit test and pushed to <a
href="https://github.com/Vijay2win/cassandra/commits/4860-v4" class="external-link"
rel="nofollow">https://github.com/Vijay2win/cassandra/commits/4860-v4</a>, Thanks! 


New Comment: 
I increased my key_cache_size_in_mb and did in fact get better results:<div class="code
panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">Averages from the middle 80% of values:interval_op_rate          :
27848interval_key_rate         : 27848latency median            : 0.7latency 95th
percentile   : 1.4latency 99.9th percentile : 22.4Total operation time      :
00:01:20</pre></div></div>The old measure() way with 300M key cache:<div class="code
panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">Running stress : -n 2000000 -o read -i 1output is hidden <span
class="code-keyword">while</span> collecting stats...Averages from the middle 80% of
values:interval_op_rate          : 27877interval_key_rate         : 27877latency median   
        : 0.7latency 95th percentile   : 1.5latency 99.9th percentile : 23.4Total
operation time      : 00:01:20</pre></div></div>This is roughly equal to the original
messure() method in read performance, I'm happy with it! 


New Comment: 
<blockquote>2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 * 2000000
bytes) will fit in key cache.<br/>2M KV with measureDeep() will take 96M + 48M (48 *
2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key
size.</blockquote>Clarifying for my own benefit: Vijay is saying that before the original
fix, the key cache underestimated the real entry size by 1/3, so a configured size of 2M
would actually allow caching 3M worth of entries.  So to compare performance
apples-to-apples, we need to allow the fixed code to use an equivalent size. 


New Comment: 
Suggest standardizing on <tt>memorySize</tt> instead of <tt>size</tt> in
IMeasureableMemory, to avoid confusion with size = number of contained elements. 
Otherwise +1. 


New Comment: 
Wow that sounds lot better than my cryptic explanation <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>Committed to 1.2 and trunk, Thanks Ryan
and Jonathan! 


New Comment: 
I just rolled this patch and am hitting this exception:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">)
liveRatio is 7.8818923401518655 (just-counted was 7.8818923401518655).  calculation took
825ms <span class="code-keyword">for</span> 78373 columnsERROR [ReadStage:14] 2013-05-07
11:01:44,555 CassandraDaemon.java (line 175) Exception in thread <span
class="code-object">Thread</span>[ReadStage:14,5,main]java.lang.AssertionError: Serialized
size cannot be more than 2GB/<span class="code-object">Integer</span>.MAX_VALUE	at
org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:58)	at
org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:54)	at
com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher.weightOf(ConcurrentLinkedHashMap.java:1447)	at
com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:764)	at
com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:743)	at
org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:101)	at
org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:27)	at
org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)	at
org.apache.cassandra.io.sstable.SSTableReader.cacheKey(SSTableReader.java:696)	at
org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:834)	at
org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:717)	at
org.apache.cassandra.db.columniterator.SSTableSliceIterator.&lt;init&gt;(SSTableSliceIterator.java:43)	at
org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)	at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)	at
org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:274)	at
org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)	at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)	at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)	at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)	at
org.apache.cassandra.db.Table.getRow(Table.java:347)	at
org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)	at
org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)	at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)	at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)	at
java.lang.<span class="code-object">Thread</span>.run(<span
class="code-object">Thread</span>.java:662)</pre></div></div> 


New Comment: 
I don't suppose you can turn that into a failing unit test?  It's not obvious to me what
could be different about what you're doing, and what we're already doing in the <span
class="error">&#91;passing&#93;</span> tests. 


New Comment: 
<a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carlyeks"
class="user-hover" rel="carlyeks">Carl Yeksigian</a> is on the case... 


New Comment: 
Looking at the code this line looks suspect....<a
href="https://github.com/apache/cassandra/blob/da93a1cfe483a1522b2c149d287279a74e43a8a9/src/java/org/apache/cassandra/utils/ObjectSizes.java#L65"
class="external-link"
rel="nofollow">https://github.com/apache/cassandra/blob/da93a1cfe483a1522b2c149d287279a74e43a8a9/src/java/org/apache/cassandra/utils/ObjectSizes.java#L65</a>If
you pass in a mmapped bytebuffer I don't think capacity is the right thing to use... 


New Comment: 
Agreed, the way we use BB it should be .remaining 


New Comment: 
Also do we even care about direct byte buffers? it's not on heap so we should just skip
those? 


New Comment: 
Don't we make a copy for the row cache entry?  If we don't we probably should. 


New Comment: 
Providing a couple of tests which show the broken byte buffer. Don't think that we should
be comparing against the meter for these, but since it appears to be compared against that
in the other parts, I followed the same specification. 


New Comment: 
We should probably check BB isDirect and skip adding the BB overhead, honestly it was an
oversight. <br/>We should probably just switch to remaining() for the rest to be
safe.<blockquote>Don't we make a copy for the row cache entry? If we don't we probably
should.</blockquote>Do you want to open a separate ticket on this? Since this happens on
CLHM (in-heap cache) i am not sure if it will benefit from bytes copying. 


New Comment: 
I'm more worried about safety issues after a mapped buffer like Jake's gets unmapped.Edit:
but if he's using 1.2.4 safely then I guess the "clean out row cache after compaction"
code is working pretty well. 


New Comment: 
I'm not using 1.2.4. We went from 1.2.3 -&gt; 1.2.5Our workaround is to disable the
keycache 


New Comment: 
I did some code diving but just got more confused.  Here's the KeyKacheKey relevant
parts:<div class="code panel" style="border-width: 1px;"><div class="codeContent
panelContent"><pre class="code-java">    <span class="code-keyword">public</span> <span
class="code-keyword">final</span> <span class="code-object">byte</span>[] key;    <span
class="code-keyword">public</span> KeyCacheKey(Descriptor desc, ByteBuffer key)    {      
 <span class="code-keyword">this</span>.desc = desc;        <span
class="code-keyword">this</span>.key = ByteBufferUtil.getArray(key);        <span
class="code-keyword">assert</span> <span class="code-keyword">this</span>.key != <span
class="code-keyword">null</span>;    }</pre></div></div><ol>	<li>we are indeed making a
defensive copy</li>	<li>we don't even have a ByteBuffer involved here, so while
<tt>.capacity</tt> could sometimes not be what we want, it's not causing the problem here.
 (Other times, <tt>capacity</tt> <b>could</b> be what we want; I don't think there's a
one-size-fits-all answer here.  JAMM has an <tt>omitSharedBufferOverhead</tt> setting to
configure this behavior.)</li></ol>I think we need to look elsewhere for our smoking gun. 


New Comment: 
I think the issue is the RowIndexEntry size. If you look at IndexInfo.memorySize() those
bytebuffers would have the wrong capacity since we mmap them...<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">  
     <span class="code-keyword">public</span> <span class="code-object">long</span>
memorySize()        {            <span class="code-object">long</span> fields =
ObjectSizes.getSize(firstName) + ObjectSizes.getSize(lastName) + 8 + 8;             <span
class="code-keyword">return</span> ObjectSizes.getFieldSize(fields);       
}</pre></div></div> 


New Comment: 
<blockquote>Providing a couple of tests which show the broken byte buffer. </blockquote>No
it is not broken, MeasureDeep was omitting the shared buffer size... which is kind of
wrong... try the following.<div class="code panel" style="border-width: 1px;"><div
class="codeContent panelContent"><pre class="code-java">        ByteBuffer bb =
ByteBuffer.allocate(1000);        <span class="code-object">long</span> objectSize =
ObjectSizes.getSize(bb);        MemoryMeter meter2 = <span class="code-keyword">new</span>
MemoryMeter();        <span class="code-object">long</span> meterSize =
meter2.measureDeep(bb);        Assert.assertEquals(meterSize,
objectSize);</pre></div></div><blockquote>Here's the KeyKacheKey relevant
parts</blockquote>Agreed! Looks like error is from CLHC, i am still not sure where its
broken MappedFileDataInput.readBytes copies anyways.... <a
href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake"
class="user-hover" rel="tjake">T Jake Luciani</a>? 


New Comment: 
<blockquote>i am still not sure where its broken MappedFileDataInput.readBytes copies
anyways</blockquote>Also a good point.  For reference:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">  
     <span class="code-comment">// we have to copy the data in <span
class="code-keyword">case</span> we unreference the underlying sstable.  See
CASSANDRA-3179</span>        ByteBuffer clone = ByteBuffer.allocate(bytes.remaining());   
    clone.put(bytes);        clone.flip();        <span class="code-keyword">return</span>
clone;</pre></div></div> 


New Comment: 
The issue is in the following code from RowIndexEntry:208<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java"><span class="code-object">long</span> internal = 0;            <span
class="code-keyword">for</span> (IndexHelper.IndexInfo idx : columnsIndex)               
internal += idx.memorySize();            <span class="code-object">long</span> listSize =
ObjectSizes.getFieldSize(ObjectSizes.getArraySize(columnsIndex.size(), internal) + 4);    
       <span class="code-keyword">return</span>
ObjectSizes.getFieldSize(deletionTime.memorySize() + listSize);</pre></div></div>The list
size is <b>not</b> getArraySize(columnsIndex.size(), internal). That multiplies the number
of elements by the size that was just calculated.It should instead be:<div class="code
panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">ObjectSizes.getFieldSize(internal + 4)</pre></div></div>There are no
other suspicious usages of getArraySize, the only other ones are from the ObjectSizes file
for byte array sizes (those make sense). 


