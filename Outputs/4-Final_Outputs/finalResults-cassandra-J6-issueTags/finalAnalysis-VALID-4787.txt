Pattern changes caused by commit: 6d0d57e29b6dc44988c5f23b90f129181ec1c94e

From: Decorator-1
To:   Decorator-0

From: Flyweight-4
To:   Flyweight-5

From: Strategy-1
To:   Strategy-0


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-4787.txt 

commit 6d0d57e29b6dc44988c5f23b90f129181ec1c94e
Author: Sylvain Lebresne <slebresne@apache.org>

    Improve SSTableSimpleUnsortedWriter speed with large rows
    patch by slebresne; reviewed by jebllis for CASSANDRA-3122



==================================
 Issue CASSANDRA-3122 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-3122] SSTableSimpleUnsortedWriter take long time when inserting big rows
-----------------

-----------------
Summary: SSTableSimpleUnsortedWriter take long time when inserting big rows
-----------------

-----------------
Issue type: Improvement
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Fri, 2 Sep 2011 09:15:43 +0000
-----------------

-----------------
Resolved at: Fri, 2 Sep 2011 18:01:28 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

In SSTableSimpleUnsortedWriter, when dealing with rows having a lot of columns, if we
call newRow several times (to flush data as soon as possible), the time taken by the
newRow() call is increasing non linearly. This is because when newRow is called, we merge
the size increasing existing CF with the new one.
 

-----------------

-----------------
Comments: 

New Comment: 
Small patch that merge the smallest CF with the other one instead of always the existing
one with the new one. 


New Comment: 
Version 2 using getEstimatedColumnCount 


New Comment: 
As said on the mailing list, though that solution does improve performance, I think we can
do better, by simply having the insertions go into the previous column family  when we
"reopen a row" instead of creating a new column family each time and copying everything to
the previous one afterwards. Attaching patch (3122.patch) that does just this. Note that
this patch also fix a bug by which the last row wasn't written and add a unit test for the
UnsortedWriter. 


New Comment: 
That's even better that way. Thanks ! 


New Comment: 
I don't understand how the changes to writeRow work without doing anything to cF b/s
asking for its serializedSize.nit: unused imports in SimpleUnsorted. 


New Comment: 
<blockquote>I don't understand how the changes to writeRow work without doing anything to
cF b/s asking for its serializedSize</blockquote>In (the new method) getColumnFamily, when
we reuse a previous column family to add new columns to it, we start by removing it's size
from the estimate, so that when writeRow is called on the updated cf, by adding the whole
size we should still have a good estimate (actually a better one that before because we
don't count the row key multiple times anymore). 


New Comment: 
+1I also note that newSuperColumn in abstract writer is unused. 


New Comment: 
Committed, thanks 


New Comment: 
Integrated in Cassandra-0.8 #312 (See <a
href="https://builds.apache.org/job/Cassandra-0.8/312/" class="external-link"
rel="nofollow">https://builds.apache.org/job/Cassandra-0.8/312/</a>)<br/>    Improve
SSTableSimpleUnsortedWriter speed with large rows<br/>patch by slebresne; reviewed by
jebllis for <a href="https://issues.apache.org/jira/browse/CASSANDRA-3122"
title="SSTableSimpleUnsortedWriter take long time when inserting big rows"
class="issue-link" data-issue-key="CASSANDRA-3122"><del>CASSANDRA-3122</del></a>slebresne
: <a
href="http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1164657"
class="external-link"
rel="nofollow">http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1164657</a><br/>Files
:
<ul>	<li>/cassandra/branches/cassandra-0.8/CHANGES.txt</li>	<li>/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java</li>	<li>/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableSimpleUnsortedWriter.java</li>	<li>/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableSimpleWriter.java</li></ul> 


New Comment: 
Digging further in SSTableSimpleUnsortedWriter, I found out another point : every time
newRow is called, serializedSize iterate through all the columns to compute the size.In my
use case, I have line whith hourly values (data:h0|h1|h2|...|h23), and for every line I
will use the date of the day concatenated with the hour as key ("dateoftheday|hour"), and
the value composed (using composite) with the data as column name (<span
class="error">&#91;value,data&#93;</span>=null). More clearly, my data look like
:<br/>abc:1|2|1|2|1|2|1|2|1|2|1|2|1|2|1|2|1|2|1|2|1|2|1|2<br/>bcd:3|4|3|4|3|4|3|4|3|4|3|4|3|4|3|4|3|4|3|4|3|4|3|4and
the for every line I call writer.newRow("20110804|0"), writer.addColum(Composite(1,
"abc"), empty_array), <br/>writer.newRow("20110804|1"), writer.addColum(Composite(2,
"abc"), empty_array), <br/>writer.newRow("20110804|3"), writer.addColum(Composite(1,
"abc"), empty_array), <br/>writer.newRow("20110804|4"), writer.addColum(Composite(2,
"abc"), empty_array), <br/>...So writer.newRow() is called 24 times for every lines.So one
solution could be to have a local class "CachedSizeColumFamily" extending ColumFamily that
will increase the serialized size at every addColumn, and return it directly when
serializedSize() is called.In the same topic, even if ConcurrentSkipListMap claims to have
good performances (which is the case in multi threading environments), I had really better
results using a TreeMap in ColumnFamily (and then avoid the putIfAbscent call on the
ConcurrentSkipListMap). In bulk loading, SSTableSimpleUnsortedWriter is single threaded
anyway, there is no needs of having a complex but yes slower data structure like
ConcurrentSkipListMap. An improvement in bulk loading would be to use a "single threaded"
ColumFamily for bulk loading. This could be part of another Jira. 


New Comment: 
<blockquote>every time newRow is called, serializedSize iterate through all the columns to
compute the size</blockquote>Yes and I agree this ain't the more efficient thing ever,
though I would kind of be surprised this would be a bottleneck. Anyway, I don't oppose
improving this, but we should create a new ticket for that.<blockquote>An improvement in
bulk loading would be to use a "single threaded" ColumFamily for bulk
loading.</blockquote>Yes, but we'll do it in 1.0 only because we have <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2843" title="better performance on
long row read" class="issue-link"
data-issue-key="CASSANDRA-2843"><del>CASSANDRA-2843</del></a> there that basically make
this trivial, while this is uglier to do without it. 


