Pattern changes caused by commit: 33cdcaf73463c351ada6e64dcd158a4823bdb902

From: Flyweight-3
To:   Flyweight-1

From: Mediator-2
To:   Mediator-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6543.txt 

commit 33cdcaf73463c351ada6e64dcd158a4823bdb902
Author: Vijay Parthasarathy <vijay2win@gmail.com>

    remove intialCapacity from rowcache
    patch by Vijay; reviewed by xedin for CASSANDRA-4141



==================================
 Issue CASSANDRA-4141 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4141] Looks like Serializing cache broken in 1.1
-----------------

-----------------
Summary: Looks like Serializing cache broken in 1.1
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 12 Apr 2012 00:00:46 +0000
-----------------

-----------------
Resolved at: Thu, 12 Apr 2012 15:39:24 +0000
-----------------

-----------------
Assigned to: Vijay
-----------------

-----------------
Description: 

I get the following error while setting the row cache to be 1500 MB

INFO 23:27:25,416
Initializing row cache with capacity of 1500 MBs and provider
org.apache.cassandra.cache.SerializingCacheProvider<br/>java.lang.OutOfMemoryError: Java
heap space<br/>Dumping heap to java_pid26402.hprof ...

havent spend a lot of time looking
into the issue but looks like SC constructor has


.initialCapacity(capacity)<br/>.maximumWeightedCapacity(capacity)

 which 1500Mb
 

-----------------

-----------------
Comments: 

New Comment: 
attached fixes this issue... changes to ConcurrentLinkedHashCache is not needed but
thought the default was good instead of setting it to 0. 


New Comment: 
Wouldn't that instead crash with OOM later in some unpredictable moment (e.g. when cache
is read from disk) or when cache resizing is done? 


New Comment: 
initialCapacity here is the number of elements in the hashmap and not the size. Size
should be controlled by maximumWeightedCapacity (yes the names are confusing
through)<br/>We are running OOM because we where trying to allocate 1500 * 1024 *1024
elements.the intial capacity is used to create the ConcurrentHashMap<div class="code
panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java"><span class="code-keyword">new</span> ConcurrentHashMap&lt;K,
Node&gt;(builder.initialCapacity, 0.75f, concurrencyLevel);</pre></div></div> 


New Comment: 
+1 


