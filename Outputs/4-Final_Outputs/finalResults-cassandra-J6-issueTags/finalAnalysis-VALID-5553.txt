Pattern changes caused by commit: 554223b4a65c7839c36e8a17e36636b92f4f15a0

From: Facade-0
To:   Facade-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-5553.txt 

commit 554223b4a65c7839c36e8a17e36636b92f4f15a0
Author: Sylvain Lebresne <slebresne@apache.org>

    remove assumption that key and token are in bijection
    patch by slebresne; reviewed by jbellis for CASSANDRA-1034



==================================
 Issue CASSANDRA-1034 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-1034] Remove assumption that Key to Token is one-to-one
-----------------

-----------------
Summary: Remove assumption that Key to Token is one-to-one
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Thu, 29 Apr 2010 03:21:00 +0000
-----------------

-----------------
Resolved at: Thu, 1 Dec 2011 10:14:43 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

get_range_slices assumes that Tokens do not collide and converts a KeyRange to an
AbstractBounds. For RandomPartitioner, this assumption isn't safe, and would lead to a
very weird heisenberg.

Converting AbstractBounds to use a DecoratedKey would solve this,
because the byte[] key portion of the DecoratedKey can act as a tiebreaker. Alternatively,
we could make DecoratedKey extend Token, and then use DecoratedKeys in places where
collisions are unacceptable.
 

-----------------

-----------------
Comments: 

New Comment: 
The problem is that we are using DK both for routing and for local key sorting, partly
because it's very convenient to be able to use the "natural" compareTo to compare those
two kinds of DK.If the only place we have DK with null key is for the routing case, then
the right thing is to convert those usages to raw Tokens and make key non-optional in DK.i
have a nagging feeling that there are more complications though. 


New Comment: 
MD5 collisions are rare enough, so somebody would probably have to write their own
partitioner to trigger this. 


New Comment: 
<blockquote>it's very convenient to be able to use the "natural" compareTo to compare
those two kinds of DK</blockquote>In particular, we generate (Token, null) DKs for range
scans, at least in part because Hadoop thinks in terms of TokenRanges instead of
DecoratedKeyRanges.  (Presumably it is still ok to assume that a partitioner generates
many more tokens than there are nodes in the cluster; if not, this would need to
change.)We might be able to still do this, if we just say that DK(T, null) always sorts
before DK(T, non-null-key) for any given Token T.I still suspect we're using DK in some
places where Token is all we really need. 


New Comment: 
Discovered the same issue with a partitioner that shares tokens for many keys.  This patch
fixes the issue. all tests pass. 


New Comment: 
Jake's patch is only a partial fix for this problem, so I've moved it to #1720. The core
of this ticket is either: changes to the class hierarchy, or changes to ranges. 


New Comment: 
Can you elaborate as to what else needs to be fixed?As I said above, "Presumably it is
still ok to assume that a partitioner generates many more tokens than there are nodes in
the cluster" so I don't think we need to make Range a DK pair for granularity's sake. 


New Comment: 
&gt; Can you elaborate as to what else needs to be fixed?<br/>I guess the larger problem
here is that if a range query asks for 10 rows using a Token range, but there are 1000
rows sharing a particular token, which 10 rows do you return? 


New Comment: 
Okay, so that is a problem because we create DK(Token, null) internally for range queries
currently even when using the key-based API.Once that is fixed I'm fine with saying "the
first 10" (or if more convenient, "that's undefined"), since only Hadoop or similar
iterate-over-everything approaches should be using Token-based range queries at the API
level.  (Again, I'm assuming that there are enough tokens to achieve sufficient
granularity, iow, that the number of rows sharing a token is less than the InputSplit
size.) 


New Comment: 
Attaching a patch that I believe solves this. It makes Range accept both Token and
DecoratedKey and makes those two compare together correctly.It introduces a new marker
interface (RingPosition) instead of making DecoratedKey extends Token (for reason
explained in the comments of RingPosition but to sum up: I think it's cleaner).The second
patch attached is just a stupid partitioner that use for token the length of the key. It's
just for testing and not meant for inclusion. But this shows that with the first patch,
you can do correct range query that go from 'the middle of a token' to the 'middle of
another one'.An important note is that this breaks the serialization unit tests, because
now an AbstractBounds can use decoratedKeys, and thus serialized AbstractBounds are
incompatible with previous version. Not sure how to deal with that though, I though we had
a plan for dealing with that but I'll admit I don't remember the details. 


New Comment: 
Patch rebased 


New Comment: 
Rebased patch with code for binary backward compatibility. This still needs the first part
of <a href="https://issues.apache.org/jira/browse/CASSANDRA-2361" title="AES depends on
java serialization" class="issue-link"
data-issue-key="CASSANDRA-2361"><del>CASSANDRA-2361</del></a> to fully pass the
serialization unit tests. 


New Comment: 
What is LengthPartitioner for? 


New Comment: 
Oh, it's just a stupid partitioner with tons of collision (and predictable ones) that I
used for testing and attached so that other can test too. Not meant for inclusion. 


New Comment: 
Initial feedback:<ul class="alternate" type="square">	<li>I'm a fan of the RingPosition
approach</li>	<li>Less of a fan of pretending that Tokens and DK are equal if the token
component of DK is equal.  Shouldn't we force caller to ask Token.equals(DK.token) if
that's what they mean? As you pointed out in RP docstring, there is not an is-a
relationship there.</li>	<li>Should we add RP.isToken to encapsulate RP.asDecoratedKey.key
== null checks?</li>	<li>DK docstring is obsolete now</li></ul> 


New Comment: 
<blockquote>Less of a fan of pretending that Tokens and DK are equal if the token
component of DK is equal. Shouldn't we force caller to ask Token.equals(DK.token) if
that's what they mean? As you pointed out in RP docstring, there is not an is-a
relationship there.</blockquote>The thing is, we need them to be equal for compareTo()
(because we can't have token &gt; keys nor token &lt; keys, otherwise that would mess up
our ranges). Then for the equals, the motivation is summed up by the Comparable
documentation:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>It is strongly recommended (though not
required) that natural orderings be consistent with equals. This is so because sorted sets
(and sorted maps) without explicit comparators behave "strangely" when they are used with
elements (or keys) whose natural ordering is inconsistent with equals. In particular, such
a sorted set (or sorted map) violates the general contract for set (or map), which is
defined in terms of the equals method.</pre></div></div>And I do fear that we would get
something inconsistent at some point.<br/>But I'm not a super fan either, just felt the
less evil of the two choices.<br/>I'm happy with suggestion though and I'll work out the
other remarks. 


New Comment: 
I understand the Comparable docs, but <ul class="alternate" type="square">	<li>that's
primarily concerned w/ compareTo + equals b/t members of the same class</li>	<li>it's
valid to say "these are tied for sorting purposes, and yet they are not equal"</li></ul>In
other words I'm more worried about subtle bugs if we allow the equals, than if we don't.
<img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>The Map example is a good one &#8211; if I
setmap<span class="error">&#91;token(1)&#93;</span> = foo<br/>map<span
class="error">&#91;dk(1, 1)&#93;</span> = barI would expect two map entries, not one.  (If
you want one, you explicitly use asToken, then there is no ambiguity.)How about if we add
an assert to both equals to make sure we don't pass in the other kind of object? 


New Comment: 
You're right, I'm convinced, it's probably safer to have equals be a true equals.<br/>I'll
do the change. 


New Comment: 
I was reaaally hoping we could subclass here... adding RingPosition leads to explicit
conversions scattered all over that end up obscuring  implicit conversions.The hairiest
part of subclassing would be renaming all of our Token subclasses with DecoratedKey
subclasses, but it cleans up unnecessary references: for example, for a DecoratedKey for
ByteOrderPartitioner or LocalPartitioner you have:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">DecoratedKey   BytesToken token;      ByteBuffer token;   ByteBuffer
key;</pre></div></div>... while with subclassing you could save two object references:<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">DecoratedKey   ByteBuffer keyAndToken;</pre></div></div>Also, the
Comparable dilemma is relatively straightforward with subclassing: Token implements
Comparable&lt;Token&gt;, the subclasses override, call super.compare, and if their
superclass is equal, fall back to instanceof(myclass) to see whether they can compare the
key data. 


New Comment: 
I realize that this is a little more subtle than I first though.You just cannot compare a
Token and a DecoratedKey simply, because a Token is actually a range of keys. Hence
dealing with a Range that mixes Token and DecoratedKey correctly is doable, but a bit
complicated (typically, it involves declaring multiple different comparison functions). To
take quick example, consider that if you mix DK and Token, you must have the following
that stands:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>    (T(2), T(8)] should not contain DK(T(2),
"foo") =&gt; DK(T(2), "foo") &lt; T(2)    [T(2), T(8)] should contain     DK(T(2), "foo")
=&gt; DK(T(2), "foo") &gt;= T(2)</pre></div></div>So there is no way to write a
compareTo() function dealing with both DK and token.So I think that it will be simplest to
not mix DK and Token in the same ranges. We'll have ranges of Token (for everything
related to ring management) and ranges of DK (for rangeSlice and scan). This is what the
patch does (and the patch 'generify' AbstractBounds, Range and Bounds (a fair part of the
patch) to keep type information around and avoid unnecessary casts all over the place).We
still want to make a rangeSlice/scan over a range of token. To do that, we simply convert
a range of Token to a range of DK. This involves declaring for a given token a smallest
key and biggest key, and this in turn comes a slight complication related to the minimum
token, but the detail are in the docstrings of the patch. I am reasonably confident on
that new patch.(Note that this patch is much bigger than the previous one, but this is
mostly due to the generification of Range) 


New Comment: 
Can you break the generification out into a separate patch? 


New Comment: 
Generification broken into a separate patch + some tiny code style update 


New Comment: 
Attaching v2 for my second patch. There was some failure in the unit tests for
getRestrictedRanges. This fix and improves those test and fix a small bug related to the
handling of the minimum value for DecoratedKey. 


New Comment: 
I think we are almost done.  A couple comments:<ul class="alternate"
type="square">	<li>DK.isEmpty seems like a bad method name for a Key object &#8211;
intuitively, keys are a specific point and should not contain other points except for the
obvious identity case.  Would isMinimum be a better name?</li>	<li>I don't understand
RP.toSplitValue or why DK would throw away information, when calling it.  More generally,
I'm unclear why we would have null keys in DK &#8211; shouldn't you use a Token, if you
don't have key information?</li>	<li>using MINIMUM_TOKEN for both sort-before-everything
and sort-after-everything values has always been confusing.  Should we introduce a
MAXIMUM_TOKEN value to clear that up?</li></ul> 


New Comment: 
Reattaching v2, previous had a stupid mistake, sorry about that. 


New Comment: 
<blockquote>DK.isEmpty seems like a bad method name for a Key object – intuitively, keys
are a specific point and should not contain other points except for the obvious identity
case. Would isMinimum be a better name?</blockquote>Actually I don't even like isEmpty for
token, so in favor of isMinimum for both DK and token.<blockquote>don't understand
RP.toSplitValue or why DK would throw away information, when calling it. More generally,
I'm unclear why we would have null keys in DK – shouldn't you use a Token, if you don't
have key information?</blockquote>Current patch don't allow to mix token and DK in a
range/bounds (because that comes with its whole sets of complications). However
getRestrictedRange must be able to break a range of DK based on a node token. So
RP.toSplitValue() returns for a given token the value that splits the range: for a token
range it's the token itself, but for a DK range, it's the largest DK having this
token.<br/>The null keys is related: even though we don't mix DK and token in range, we
need to be able to have a range of DK that includes everything from x token to y token.
Hence, for a given token t, we need two DK: the smallest DK having t and the biggest DK
having t. In the patch, slightly but not totally randomly, I use DK(t, EMPTY_BB) for the
smallest key and DK(t, null) for the biggest one, hence the "need" for null keys.
<blockquote>using MINIMUM_TOKEN for both sort-before-everything and sort-after-everything
values has always been confusing. Should we introduce a MAXIMUM_TOKEN value to clear that
up?</blockquote>I think that would make wrapping stuffs more complicated. Because then
what would be the difference between the following ranges: (MIN, MIN], (MAX, MAX], (MIN,
MAX] and (MAX, MIN]. For DK, the code is already enforcing that the we only have one
minimum key (that is DK(MIN, EMPTY_BB)) and never ever use DK(MIN, null) because that
poses problems. I think a MAX token would make that worst. 


New Comment: 
<blockquote>RP.toSplitValue() returns for a given token the value that splits the range:
for a token range it's the token itself, but for a DK range, it's the largest DK having
this token. The null keys is related: even though we don't mix DK and token in range, we
need to be able to have a range of DK that includes everything from x token to y
token</blockquote>This feels messy and error-prone to me. I wonder if we haven't found the
right approach yet. 


New Comment: 
I agree that using null is a necessary solution here: you need a max value for keys, since
they are essentially the "child" of a one-token-range. The key range is bounded (since it
has parents), but the token range is not, so I agree with sylvain that a MAX_TOKEN is
probably not necessary.One way to remove toSplitValue would be to use DecoratedKey
everywhere; DecoratedKey is a compound of the Token and the key blob. The equivalent of
today's Token is a DecoratedKey for that token with a null key: it compares greater than
all valid child keys, so it contains them.I hope that it won't muddy the water, but the
&lt;empty&gt; as min and &lt;null&gt; as max approach is the same one I took forthe first
cut of the file-format, and it worked very well. You can use the min/max values to find
the beginning or end of a child range. See <a
href="https://github.com/stuhood/cassandra-old/blob/674/src/java/org/apache/cassandra/db/ColumnKey.java#L225"
class="external-link" rel="nofollow">ColumnKey.java</a>EDIT: Actually... I'm not so sure
about not having MAX_TOKEN... it might actually clean things up quite a bit. Any time a
range ends with what use to be the min token, you can make a direct translation to
MAX_TOKEN. 


New Comment: 
Patch rebased, this is against trunk. 


New Comment: 
<blockquote>One way to remove toSplitValue would be to use DecoratedKey
everywhere;</blockquote>I'm not saying it's not possible, but I think this is overkill (in
the changes it involves). Moreover, all the code that deals with topology really only care
about token. That's the right abstraction for those part of the code. So I really (really)
doubt using decorated key everywhere would be cleaner. Of course, anyone is free to
actually do the experiment and prove me wrong. I also don't think it would remove the need
for splitValue, it would just maybe call it differently.<blockquote>The equivalent of
today's Token is a DecoratedKey for that token with a null key</blockquote>This is only
true today because we assume key and token are one-to-one. The goal is to change that. If
multiple keys can have the same token (by definition the token is really the hash of a
key), then the statement above is false. If a token correspond to an infinite set of key
(with is the case with md5 btw, we just ignore it), then replacing a token by given key
<b>cannot</b> work.Overall, it could be that there is better way to do this, but having
spend some time on this, I have a reasonable confidence on that it fixes the issue at hand
without being too disruptive (which is not saying there isn't a few points here and there
that couldn't be improved). 


New Comment: 
What's the status on this? This issue and its relations back to <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2878" title="Allow map/reduce to use
server-side query filters" class="issue-link"
data-issue-key="CASSANDRA-2878"><del>CASSANDRA-2878</del></a> are the only reason we're
using OPP. I suspect other users setup with both cassandra and hadoop (or brisk) could be
in the same boat. Not only does OPP leave an unbalanced ring (i've had a case where all
data went to one node because the keys/tokens were longer than normal) it leaves poor
performance to hadoop jobs as tasks requirement on data locality has become stricter (w/
<a href="https://issues.apache.org/jira/browse/CASSANDRA-2388"
title="ColumnFamilyRecordReader fails for a given split because a host is down, even if
records could reasonably be read from other replica." class="issue-link"
data-issue-key="CASSANDRA-2388"><del>CASSANDRA-2388</del></a>). Apart from the plain
preference to be using secondary indexes over OPP. 


New Comment: 
Status is, I'm hoping that someone comes up with a fix that doesn't look error prone. 
Otherwise we'll probably end up with merging Sylvain's solution for 1.1. 


New Comment: 
I'm thinking of making Token an interface and implementing two classes RoutingToken(token)
and QueryToken(token, key) so all current token implementations LocalToken, StringToken,
BytesToken, BigIntegerToken are going to extend QueryToken. RoutingToken is going to be
used for operations where we don't need a key - bootstrap, midpoint calculation,
TokenMetadata class; QueryToken is going to be a replacement for DK, that will allow us to
remove DK completely and operate only on the token basis. Thoughts? 


New Comment: 
<blockquote>Thoughts?</blockquote>From your comment only I don't see right away what you
are proposing besides a renaming of Token -&gt; RoutingToken and DecoratedKey -&gt;
QueryToken. 


New Comment: 
Patch removes DK and IPartitioner.decorateKey(ByteBuffer key), which is replaced by
IPartitioner.getToken(ByteBuffer key), Token now takes second parameter - ByteBuffer key.
Most of the patch are replacements for DK -&gt; Token and decorateKey -&gt; getToken. All
tests (test, test-compression, long-test) pass.Rebased with the latest trunk (last commit
7624536ae7fea52bcf761c7dea212fe12d2f4586) 


New Comment: 
At first glancei  like this because it makes the Token first class and the key not
required. cleaning up the code below.<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">-        DecoratedKey
startWith = <span class="code-keyword">new</span> DecoratedKey(range.left, <span
class="code-keyword">null</span>);-        DecoratedKey stopAt = <span
class="code-keyword">new</span> DecoratedKey(range.right, <span
class="code-keyword">null</span>);+        Token startWith = range.left;+        Token
stopAt = range.right;</pre></div></div> 


New Comment: 
I have 2 major problems with that patch:The first one is I really dislike the idea of
merging DK into Token (I disliked the idea of merging Token into DK and that roughly the
same idea).  First, I fail to see how this is of any help in solving what this ticket is
trying to solve. Second, I think it's a very bad use of types. A Token is not a Key. By
merging those together we just weaken our type hierarchy, thus getting less insurance from
types. Typically, with this patch, a function that really want a DK, could get a Token,
i.e getKey() is not guaranteed to return a valid key. Now I know, we are already using
'false' DK by feeding null as a key sometimes. Well, that is ugly and error prone. I don't
think generalizing this everywhere while introducing a 300K patch is the right way to go,
quite the contrary. Besides, it's inefficient. All the places were we do use only a Token,
we'll now have a bigger structure with a useless pointer to the EMPTY_BYTE_BUFFER (granted
this has probably little impact, but it's another sign that it's doing it wrong).The
second problem is this doesn't work. This DK-&gt;Token really just muddy the water but it
doesn't solve anything. What we want is to fix the fact that the code identifies token and
keys as a one to one mapping. In particular, this is forced in DK.compareTo(), which only
compare the tokens, ignoring the keys.  Fixing that place is easy, and the patch does it,
but it's really just a few lines change.The real problem is that the code make the
assumption that key &lt;-&gt; token is one to one in other places. So making DK.compareTo
takes key into account breaks other parts. For instance, in RowIteratorFactory, we have
this:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>return startWith.compareTo(row.getKey())
&lt;= 0       &amp;&amp; (stopAt.isEmpty() || row.getKey().compareTo(stopAt) &lt;=
0);</pre></div></div>and say that startWith and stopAt are token only. The semantic is
that this is supposed to be inclusive on both bound. With the last patch, this would
include keys having the startWith token, but <b>not</b> the ones having stopAt as token,
because in the patch, a token compares strictly before all of the key having this token
(concretely, the attached patch skips keys during range queries).And this is not the only
places in the code where this problem manifest, because this is the symptom of a larger
problem. If more than one key can have the same token, then tokens are a range of
keys.<br/>If you ask for the range of tokens <span class="error">&#91;1, 4&#93;</span>,
then you expect that it will return all the keys having token 1, 2, 3 and 4. That excludes
having a token comparing strictly before all the keys having this token (or having it
compare strictly after all the keys having it as token for that matter). Merging Token and
DK just doesn't work.At the risk of sounding cocky, I really encourage people to have
another look at my patch. I do believe that once you've realized what solving this problem
entails, it's a solution that strike a reasonable balance in fixing the problem without a
entire rewrite of Cassandra. 


New Comment: 
@Sylvain This is all really confusing and I agree the core of the ticket is to make
key-&gt;token 1:1The core of the problem initially was explained in <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1733" title="get_range_slices
doesn&#39;t return key from start_key in KeyRange any more" class="issue-link"
data-issue-key="CASSANDRA-1733"><del>CASSANDRA-1733</del></a><blockquote>A Range object
(which Hadoop splits generate) is start-exclusive. A Bounds object (which normal user scan
queries generate) is start-inclusive.</blockquote>So by making Token the only way to deal
with keys it feels like a more consistent api.  Since Key can be null it needs to be Token
that becomes the primary internal class. In your impl we now have DK, Token, RingPosition
which too me is more confusing than having one Token class. 


New Comment: 
I'm not sure I'm am completely clear, so allow me to try to improve that. I think there is
two issues with the last patch that are largely orthogonal:<ol>	<li>the patch is broken
(again, this is largely not related to the shoving of DK into Token)</li>	<li>I believe
shoving DK into Token is a bad, error-prone idea that have no tangible
advantages</li></ol>But let's me first focus on the first issue, if only because it
involves no opinion whatsoever: I'm either right or wrong that it's broken (but i'm pretty
sure I'm right). So let's be concrete and take an example.The patch "merges" Token and
DecoratedKey together, so let me take the following notation:<ul>	<li>t(1, a) for what is
the DecoratedKey a of token 1 in current but is is just a Token instance in the
patch</li>	<li>t(1) for the 'pure' token 1. In other word it's a shortcut for t(1,
EMPTY_BYTE_BUFFER) in the attached patch and correspond to just a Token in the current
code.<br/>(as a side note, the fact that I have to speak of DecoratedKey and 'pure' token
to explain is imo yet another sign than melting everything into Token is a bad idea but
I'm diverging)</li></ul>Since when Token are compared, the token is compared then the key
is on token equality, we have t&#40;n) &lt; t(n, k) whatever the token n and key k are
(since t&#40;n) is t(n, EMPTY_BYTE_BUFFER) and EMPTY_BYTE_BUFFER &lt; k for any valid key
k) .Let's now take an example of multiple keys having the same token and say that I have
the following keys in my cluster:<div class="preformatted panel" style="border-width:
1px;"><div class="preformattedContent panelContent"><pre>tokens |   1   |     2     |  
3keys   | a | b | c | d | e | f | g</pre></div></div>In other words, a and b have the same
token 1; c, d and e have the same token 2; ...The goal for this ticket is to support that
situation correctly. Sor for instance, we should have that:<br/>   range_slice(start=t(1),
end=t(3)) returns c, d, e, f and g<br/>(because range_slice with tokens is start
exclusive).  However, with the attached patch:<br/>   range_slice(start=t(1), end=t(3))
will return a, b, c, d and eThe reason is fairly simple: we have that t(1) &lt; t(1, k)
for any k and t(3) &lt; t(3, k) for any k.Another way to put it is that it breaks our
token ranges: if you have a node that owns Range(t(1), t(3)), it's supposed to not
contains any key with token 1 and all keys with token 3, but it fails at both.So it's
broken. Now there is something we could be tempted to do. We could make it so that
t&#40;n) &gt; t(n, k) for any token n and any key k. But in turn that would break Bounds
(i.e, start inclusive) of 'pure' tokens. I.e, Bounds(t(1), t(2)) is supposed to include
all keys with token 1, but if t(1) &gt; t(1, k) for any key k, it won't include it.One
could argue however that this is still solution because I <b>think</b> that right now we
never really use a Bounds of 'pure' tokens (more precisely, the current code does it, but
only in place where we are actually doing a range slice between keys). And I <b>think</b>
that functions that take a startKey, when fed a 'pure' token only do start exclusive. So I
suppose we could assert that we never create Bounds of Token and put some assert here and
there (in SSTableReader.getPosition() for instance) and go with that.But imho this is a
bad idea. Because it's fragile and because this is ignoring a problem that may screw us up
later. Why not fix it the right way now? What if tomorrow we do want to be able to query
all the keys having a given token ? That is, what if we want to query <span
class="error">&#91;t(1), t(1)&#93;</span> ? We would not be able to, because if t(1) &gt;
t(1, k) for any k, then <span class="error">&#91;t(1), t(1)&#93;</span> don't include
anything.Again, all this is because a token actually correspond to a set of keys (once you
acknowledge multiple keys can have the same token), and so if you want to do things
correctly, you need for a given token n to have a representation for both:<ul>	<li>the
smallest key having token n</li>	<li>the greatest key having token n</li></ul>With that,
you can query all the keys having token n. Without, you can't. That is what my patch does
and I believe fairly strongly is the right way to do it.Alright, that the first thing that
a patch to this ticket must deal with. Then there is another thing: the current code only
allow for AbstractBounds of Token (by typing), but we want for this patch that once you do
a range_slice query with at startKey and endKey, you get a range of keys in
ColumnFamilyStore.getRangeSlice(), so that you can precisely answer those queries. That
means we must be able to construct AbstractBounds with keys in them. Note that it's
basically just a typing problem.The answer to that of this patch is show DK into Token. So
yeah, it fixes that problem, but what I'm arguing is that:<ul>	<li>It's error-prone and
make coding <b>more</b> complicated. We're merging object that are not the same thing.
Today if a methods takes a Token, you know it won't do anything at the granularity of keys
(well today Token and keys have the same granularity but this ticket is supposed to change
that). You lose that information if you merge DK and Token. And if a method takes a
DecoratedKey, you know that it doesn't expect a Token (more precisely, Typing ensures it).
Sure, we do already use a trick in a few places where we create 'fake' DK(null, key). But
at the very least, when we do that, we know we're doing something weird, and we are extra
careful that methods we call on that fake DK handle that case correctly. If everything is
Token, now the signature for a lot of method will suggest it is ok to give a 'pure' Token.
So what, all method should defensively assert this is not the case ? This is what types
are for.</li>	<li>It's a ~300K patch. Sure it's mostly simple changes, but it's still that
many changes that could introduce a typo somewhere that causes a bug.</li>	<li>It's a tad
less efficient because each time we really only care about 'pure' Token (and there is
quite a bunch of code that does that), we would allocate a slightly larger structure for
no reason. And I'm pretty sure god kills a kitten each time you do that.</li></ul>The
solution to that very same type problem I'm proposing (in my patch) is instead simply to
generalize AbstractBound slightly so you can have both AbstractBound of Token and of
DecoratedKey. That sound very reasonable to me.  After all we should be able to have
AbstractBounds of anything that implements Comparable right ? Well, as it turns out our
implementation of AbstractBound needs a little more than that (because our ranges wraps,
we need Comparable but with a minimum value for instance) and that is what RingPosition is
for.  But it's only a marker interface, and if you look at the code it's actually used in
a small number of places, so I admit I fail to see how this make thing much more
complicated. 


New Comment: 
Can you please define what do you mean by "pure token"? Aren't we supposed to generate
token from key in all situations except initial token in config and middle point between
tokens? So if you do a range slice using tokens instead of keys TokenFactory.fromString
will force you to use correctly serialized token data which will also include
key.<blockquote>It's error-prone and make coding more complicated. We're merging object
that are not the same thing etc...</blockquote>If token is generated from key than for me
it's natural to have a key as member. The thing is that you are enable to create a "pure"
token, Partitioner will always give you a Token with valid key except for midpoint method
so if partitioner is used to generate tokens you are guaranteed to have a valid key in the
resulting token instance.<blockquote>It's a ~300K patch. Sure it's mostly simple changes,
but it's still that many changes that could introduce a typo somewhere that causes a
bug.</blockquote>The same thing I can say about your set of patches - it's 198 KB. Aren't
we writing tests to catch such bugs? 


New Comment: 
<blockquote>Can you please define what do you mean by "pure token"?</blockquote>In you
patch, it's a Token whose key is EMPTY_BYTE_BUFFER (which is <b>not</b> a valid row key,
hence the 'pure' token name).<blockquote>Aren't we supposed to generate token from key in
all situations except initial token in config and middle point between
tokens?</blockquote>And? Is that not enough? There is tons of place in the code where we
manipulate those tokens 'not created from a key' (all the distribution code basically,
which is a big part of Cassandra). Moreover, there is also range_slice that accept a range
of token.<blockquote>So if you do a range slice using tokens instead of keys
TokenFactory.fromString will force you to use correctly serialized token data which will
also include key.</blockquote>To what is this supposed to be an answer ?<blockquote>If
token is generated from key than for me it's natural to have a key as member. The thing is
that you are enable to create a "pure" token, Partitioner will always give you a Token
with valid key except for midpoint method so if partitioner is used to generate tokens you
are guaranteed to have a valid key in the resulting token instance.</blockquote>But it's
not always generated from a key! There is nothing natural to a key member in all the Token
object manipulated by TokenMetadata and other, since there is not such key.<blockquote>The
same thing I can say about your set of patches - it's 198 KB. Aren't we writing tests to
catch such bugs?</blockquote>Well, in my patch, 148K of those are a type generification
only (that's why I've separated it). Because generics are erased at runtime, as long as it
compile, there is <b>NO</b> chance this can introduce a bug. As for trusting tests to
catch bugs, I think it's being overconfident in tests. But in the end, I'm happily taking
back that objection as this is by far the less important.Let me try to put things
graphically, everyone loves a graph: if I draw the set of all keys as this:<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>[-----------------------------------------------------------------------------[</pre></div></div>i.e,
the ring but as a line because I'm ignoring wrapping for this.Now, if I display row keys
(decorated or not, that doesn't matter, both are keys), I would have for instance:<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>[---------------------------|-------|---------------|---------------|---------[
                           k1      k2              k3              k4</pre></div></div>A
key is a point on the ring.Now if keys and tokens are a 1 to 1 mapping, then it could be
ok to say that a token is a point on the ring, but once it's not the case, then it looks
like that:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>                                t           
         t'             
t''[-------------------------[*|*******|*]----------[**|****]-------[**|******]--[        
                   k1      k2              k3              k4</pre></div></div>where t is
the token for both k1 and k2 (and an infinite number of other keys (actually finite
because we're working on a computer)), t' the token of k3 (and an 'infinite' number of
other keys), etc...A token is intrinsically a range, a segment on the ring. Shoving DK and
Token into the same class everywhere in the code is saying that we'll use the same class
for a point and an interval. How can that be a good idea? How can that not backfire on us
and be hard to work with, making it easy to introduce errors? 


New Comment: 
<blockquote>A token is intrinsically a range, a segment on the ring. </blockquote>But the
whole point of the ticket is to remove this concept. Are you saying that can't be
guaranteed?This should be possible by making a equals consider the token AND key.  The
problem with <a href="https://issues.apache.org/jira/browse/CASSANDRA-1733"
title="get_range_slices doesn&#39;t return key from start_key in KeyRange any more"
class="issue-link" data-issue-key="CASSANDRA-1733"><del>CASSANDRA-1733</del></a> is
sometimes we don't specify a key since we have have Min token and an intrinsic Max token. 


New Comment: 
<blockquote>But the whole point of the ticket is to remove this concept. Are you saying
that can't be guaranteed?</blockquote>There is a misunderstanding. The whole point of this
ticket is to <b>enforce</b> this concept. A token is a range, a segment on the ring, there
is nothing we can do about it. It's like saying the point of the ticket is to remove the
concept that a segment is different from a point.I'm happy to discuss that, and that is
clearly where we should start, but I'm pretty sure that the <b>problem</b> we want to fix
is that the current code is pretending than a segment is equal to a point. The current
code is pretending that a token t is the same thing than a key having this token. This
only work if there is only one key have a given token, otherwise it's buggy, you identify
all keys having the same token as equal, that is the problem.And saying that you'll change
the comparison of DK to include the key and pretending that a token is the same thing that
some fictive key that as far as key comparison is concerned would be before any key having
the token (which is <b>exactly</b> what Pavel's patch is doing) doesn't work either. As
I've said earlier with examples.I'm saying that the right way to fix is to make the code
treats Token as a segment (because you know, that's what it is) and a key as a point. Now
that, imho, is not of a debatable nature: it's either true or false (and imho clearly true
but maybe i'm completely stupid). But at the very least we should agree on that first,
even before thinking about how we will code it.Then, once we agree on the problem, there
is the question of how we do it. And then, my second argument is that shoving a token (a
segment) and a (decorated) key (a point) into the same class (that we would happen to call
Token) is, why probably "possible", likely an error-prone, confusing and frankly ugly
idea. You can create a class representing both a segment and point, having it work
correctly underneath and write code using that, but it will unlikely be beautiful nor easy
to use. But it's "possible". 


New Comment: 
<blockquote>The whole point of this ticket is to enforce this concept. A token is a range,
a segment on the ring, there is nothing we can do about it.</blockquote>Right.Is this
still a fair summary of why we want to fix this?<blockquote>the problem is that we are
using DK both for routing and for local key sorting</blockquote> 


New Comment: 
My view is a Key requires a Token in our system. I understand that you cant keep multiple
keys from mapping to the same token, still I would have liked to see the code deal with
Tokens with (optional) keys then a mix of keys and tokens.  I see now this idea is broken
in the sense that sorting a list of tokens means different things depending on the context
(partitioner bounds vs user defined range) 


New Comment: 
<blockquote>Is this still a fair summary of why we want to fix this?the problem is that we
are using DK both for routing and for local key sorting</blockquote>Hum, I would actually
rephrase it with token instead of DK, in the sense that we don't really use DK for
routing, DK is a key with it's token "cached" to speed up computing it, we're using only
the token to route. The problem is we're also using only the token for local key
sorting.But while we could/should be using token to route and the DK for local key
sorting, we still need to be able to handle local key <b>search</b> by token. And that is
imho the difficulty of this ticket (if we always had an actual valid key to do local key
search it would be much easier). And we need local search based on tokens
because:<ul>	<li>we allow range_slices on a range of tokens (so this translate ultimately
to local search by token)</li>	<li>even for range_slices by keys, we still end up
splitting the key range by a token in getRestrictedRanges, hence resulting ultimately to a
local search by token.<br/>Then the problem is that since a token is a segment and a key
(what we're searching for) a point, we can't really compare those, in the sense that a key
is not necessarily either stricly greater, equal, or stricly lesser than a token. So you
do have to consider both the "bounds" of the token, which are now point and that you can
compare to keys.</li></ul> 


New Comment: 
Attaching rebased patch (against trunk).I've slightly refactored the patches too. The
first contains only the generification of AbstractBounds. It's obviously a bit "dumb"
taken alone since it generify but doesn't allow anything else than tokens. The only other
noticeable thing is the removal of the Range.compare() method (in favor of the compareTo
method of Token). I have no idea what that method was about in the first place. The second
patch does the rest of the work and has got some minor cleanups. I've also tried to add
some new comments to make it more digestible.  I also include a third patch with a small
unit test.Having spend quite some time thinking about this issue, I do think that this is
a good way to fix it, the alternative of allowing to mixing Token and DecoratedKey
directly in a Range being (to have pursued it a bit before giving up) much more messy and
error prone imho. Now I can't force anyone to like this solution but I also won't rebase
this forever. 


New Comment: 
<div class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>+    public static final DecoratedKey minKey = new
DecoratedKey(partitioner.getMinimumToken(), false);</pre></div></div>I think I'd rather
have these in the partitioner.  (I know partitioner is cluster-global right now but it
still feels wrong to "hoist" something partitioner dependent out and make it static
final.)<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>+        assert token != null &amp;&amp; key
!= null;</pre></div></div>This feels odd when we go ahead and construct DKs with null key
anyway in the other constructor.<b>Important</b>: I think my biggest problem with this
patch is that a DK may or may not have a key that when given to the partitioner, results
in the Token in the DK.  And there's nothing to show that is the case, except that key ==
null or Empty.  So we're still pretending a Token "is" a key, we've just made it more
complicated.  Could we update the methods for whose benefits we're performing the Token
-&gt; DK conversion, to accept RingPosition instead?<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>+       
return token.hashCode() + (key == null ? 0 : key.hashCode());</pre></div></div>I don't see
a good reason to not use a "real" hashcode implementation (Objects.hashCode is useful
here).<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>+        // null is used as a 'end of range'
marker, so DK(t, k) is always before DK(t, null) unless k == null</pre></div></div>Still
not a huge fan of using null to mean end of range, but I guess I don't have a better
suggestion. There's clearly a lot of places in this patch where it's causing special case
ugliness though, independent of its status as "max."<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>+        //
minimunKey, see Token.upperBoundKey()</pre></div></div>typo.  (both occurrences.)<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>-        T min = (T)current.partitioner.getMinimumToken();+        T
min = (T)current.left.minimumValue(current.partitioner);</pre></div></div>I think the
positives of making this Generic are outweighed by the negative of implying that minimum
value for partitioner X depends on the RingPosition that is returning it.  I think I'd
rather accept the casting ugliness of having a Partitioner method that does instanceof
checks to return the appropriate type.  <b>Serializer code</b>: How does DK, AB, etc. code
deal w/ backwards compatibility issues?  Looks like some (AES) can get by with saying "we
don't support mixed-version streaming" but others (IndexScanCommand) cannot.<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>+        assert left.compareTo(right) &lt;= 0 ||
right.isMinimum(partitioner) : "[" + left + "," + right + "]";</pre></div></div>What if we
added a Partitioner reference so we could just ask isMinimum()? 


New Comment: 
<blockquote>I think my biggest problem with this patch is that a DK may or may not have a
key that when given to the partitioner, results in the Token in the DK.</blockquote>Put
another way: in my ideal world, DK.token would be purely an optimization to avoid calling
partitioner.getToken(key) over and over. 


New Comment: 
<blockquote>Put another way: in my ideal world, DK.token would be purely an optimization
to avoid calling partitioner.getToken(key) over and over.</blockquote>I understand that,
but I think there is two different things and I want to know exactly where the
disagreement/problem is.The first problem, which is imho the core of this ticket, is that
the code needs to be able somehow to deal with things like (where I use k for keys and t
for tokens, and the term range for either Range or Bounds):<ul>	<li>Is key k in the range
<span class="error">&#91;k&#39;, t&#93;</span> (or (t', k''])? Because when you do a
range_slice of <span class="error">&#91;k&#39;, k&#39;&#39;&#93;</span> and there is
multiple nodes and <span class="error">&#91;k&#39;, k&#39;&#39;&#93;</span> spans multiple
replica, we will end up requesting all keys in <span class="error">&#91;k&#39;,
t&#93;</span> (for some t) or (t', k''].</li>	<li>Is key k in range (t, t']? Because we're
allowed to range query keys by a token range, but also a few other reason, like the fact
that during validation compaction we hashes together keys within a token range.<br/>Note
that those are not trivial questions, because for instance <span
class="error">&#91;k&#39;, t&#93;</span>, while we intuitively understand what it
represents is a weird beast in that is a range a point and a segment?!</li></ul>Or in
other words, as much as I'd like the operations on Tokens and the ones on Keys to be two
completely orthogonal sets of operation with no interaction whatsoever, it is not the case
and we have to deal with it.Dealing with the case where we need tokens and we have keys is
trivial (we just call Key.getToken() and boom, we're back in the case with only
tokens).The problem is when we fundamentally work on keys, but have only token to start
with. Today (i.e. before this ticket), we take a simplification by doing essentially the
same thing that in the 'needs token but got keys' case by having a sort of Token.getKey()
(it's more ugly in practice, we inline calls to new DecoratedKey(t, null), but that's the
same thing). But doing that forces in itself the fact that key an token are in bijection
and we want to lift that.One solution could be to try to keep Token as long as we can,
even in places where we really need a key and have the code deal with that. I can
understand that on the surface that could look clean, but in practice the code to do that
correctly would a pure nightmare. Just trying to implement a Range that would mix token
and keys (like the <span class="error">&#91;k&#39;, t&#93;</span> range above) is a
complete mess.So what this patch does is realizing that you could characterize the set of
keys that a token t represents with only two keys: the smallest key having token t, and
the biggest key having token t.Now, supposing we agree on what is above, the rest is
implementation details and that's probably a much simpler discussion. Note that above I'm
not talking of DecoratedKey, only key. But the question is, how do you represent those two
new keys (for each token). The patch uses special values of the key field of DK to deal
with those. I can agree this is not the cleanest thing ever and I'm fine looking for a
different encoding, but I just don't have a much better idea, and frankly I don't find
that horrible either.<blockquote>I think I'd rather have these in the
partitioner</blockquote>Good idea.<blockquote>his feels odd when we go ahead and construct
DKs with null key anyway in the other constructor.</blockquote>The goal here is to avoid
constructing one of the two 'fake' keys by accident For that the second constructor is
dedicated to their construction and as the commnet says, you're not even supposed to use
this second constructor, but use Token.{upper|lower}Bound instead. Actually, the assert
should check for the EMPTY_BYTE_BUFFER.<blockquote>Could we update the methods for whose
benefits we're performing the Token -&gt; DK conversion, to accept RingPosition
instead?</blockquote>Frankly, and as argumented above, no, not without <b>huge</b> pain.
We only do that conversion in places where we will have to do it at some point, and trying
to push Tokens deeper would only serve in having operations that make no real sense for
Tokens be able to actually deal with Token. As one example, we would have to make Range
with a mix of Token and Keys, and frankly that will be a total mess to code.<blockquote>I
don't see a good reason to not use a "real" hashcode implementation (Objects.hashCode is
useful here)</blockquote>Not sure I follow but ByteBuffer.hashCode() does hash the content
of the buffer if that was what you meant.<blockquote>There's clearly a lot of places in
this patch where it's causing special case ugliness though, independent of its status as
"max."</blockquote>Again, I would be open to better encoding. But is there really that
much places? The patch tried to make it so that no code outside of DecoratedKey really
have to deal with it. If not perfect, I actually think it's better contained that before
the patch.<blockquote>I think the positives of making this Generic are outweighed by the
negative of implying that minimum value for partitioner X depends on the RingPosition that
is returning it. I think I'd rather accept the casting ugliness of having a Partitioner
method that does instanceof checks to return the appropriate type.</blockquote>I think
you're right.<blockquote>Serializer code: How does DK, AB, etc. code deal w/ backwards
compatibility issues?</blockquote>Basically, old version only understand AbstractBounds of
Token, while new version generates/accept AbstractBounds of either token, or keys. When
old sends to new and keys are expected, new convert the range/bounds of token as
range/bounds of keys. When new sends to old, it converts any range/bounds of keys to
range/bounds of token.<blockquote>What if we added a Partitioner reference so we could
just ask isMinimum()?</blockquote>Do you mean to have the DK to have a reference to the
partioner? If so, I agree that it's probably something we should, but it's nothing
specific to that patch so I'd rather leave it to another ticket. 


New Comment: 
<blockquote>in practice, we inline calls to new DecoratedKey(t, null)</blockquote>Right. 
I must be missing something crucial, because that's exactly what it looks like we're still
doing in this patch, only with a special constructor. 


New Comment: 
No, no, the patch does use the same think. I merely said that the patch does some attempt
at a better encapsulation, as it seems better to use the Token.{upper|lower}BoundKey to
creates those fake keys that inlining the call to the constructor all over the code (which
we do now). It makes the use of null more of an internal detail of DecoratedKey (not
completely, granted, but it's a little bit better). It also makes it simpler to check we
don't accidentally construct a DK with a null key by accident (the goal of the assertion
in the first DK constructor in the patch).<br/><br/>But let it be clear that I'm not
making any claim that this patch "cleans" some ugliness in the current code. It mainly try
to solve the problem at hand, which is basically to be able to do range_slices and getting
the right result even when multiple keys have the same token.<br/><br/>This is not saying
it wouldn't be good to fix any current ugliness at the same time if possible, but in
truth, I don't find that using special DK to represent special keys is such an ugly hack
(not either claiming it's super beautiful, I just don't have a particular hatred of this).
Besides, I don't have tons of ideas to fix the issue at end (the priority) and make the
code clearly cleaner. And I do think that whatever ugliness the current have, this patch
doesn't make it worst.<br/><br/>Anyway, I'll try to see if I can improve the encapsulation
of the Token.{upper|lower}BoundKey representation and see if I can come with something
slightly cleaner. 


New Comment: 
<blockquote>the patch does some attempt at a better encapsulation, as it seems better to
use the Token.{upper|lower}BoundKey to creates those fake keys that inlining the call to
the constructor all over the code (which we do now). </blockquote>Okay, I'll buy that. 
It's an awful lot of code churn for IMO a relatively minor win, but I see where you're
going with that.Help me understand this patchset a different way: which is the part
without which <a href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> is impossible? 


New Comment: 
<blockquote>Help me understand this patchset a different way: which is the part without
which <a href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> is
impossible?</blockquote><a href="https://issues.apache.org/jira/browse/CASSANDRA-1600"
title="Merge get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> requires that the row key
range requested be known by CFS.getRangeSlice/search, while today it only gest the
corresponding tokens.  We could possibly do what your first patch for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> did and add the keys
separately. You'll have to deal with wrapping and such, but that's probably doable.What
this patchset does is make getRangeSlice/search actually take keys, so this greatly
simplify <a href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a>. But <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> is probably doable without
this, it's just the logical first step before getting a clean implementation. Now for the
specific parts, as said we need to be able to have keys for getRangeSlice/search, which
basically require the bulk of this patchset (i.e. for <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1600" title="Merge
get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a>, we could still have
DecoratedKey.compareTo() to only compare the tokens and not the keys, but that's probably
it)But truth being told, <a href="https://issues.apache.org/jira/browse/CASSANDRA-1600"
title="Merge get_indexed_slices with get_range_slices" class="issue-link"
data-issue-key="CASSANDRA-1600"><del>CASSANDRA-1600</del></a> is by far not my main
motivation for this. My main motivation is <a
href="https://issues.apache.org/jira/browse/CASSANDRA-1684" title="Entity groups"
class="issue-link" data-issue-key="CASSANDRA-1684"><del>CASSANDRA-1684</del></a>. For the
latter, if we want to do it 'natively', we will have lots of key having the same token, so
this ticket is an absolute requirement before even getting started. And there is also the
problem of md5 collision <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
I've tried finding a better encapsulation for the 'fake' keys of the patch.  The idea
being to restrict DK to 'true' row key, i.e. the ones that can be written on disk and
create a new class (Token.KeyBound) to represent the two "fake" key for each token
representing the smallest/biggest key having the token. To make it work together, they
share the new RowPosition interface.Some of the methods accepts a RowPosition (instead of
DecoratedKey) to indicate that it can accept a 'fake' key for purpose of selecting true
keys.  So for instance SSTableReader.getPosition() accepts a RowPosition. However,
SSTableReader.getCachedPosition() only accepts a DK, because the key cache can only
contain a "true" row key.Anyway, I actually end up liking this. With that, we never ever
create a DK with a null key (nor even an empty one, which wouldn't be a true key either). 
This is more clear and avoids mistakes. Unfortunately the patch got bigger <img
class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.png"
height="16" width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
+1 on the KeyBound approach.  This is exactly what I was hoping for.Returning to a minor
point:<blockquote>bq. I don't see a good reason to not use a "real" hashcode
implementation (Objects.hashCode is useful here)</blockquote><blockquote>Not sure I follow
but ByteBuffer.hashCode() does hash the content of the buffer if that was what you
meant.</blockquote>I mean that straight addition is a weak hashcode combination since X +
Y is the same as Y + X.  "return Objects.hashCode(X, Y)" is an easy way to do it "right"
with no more code than the weak approach.  Doesn't matter much here but it's good practice
imo.Another nit: should we be using a enum for RowPosition.kind?Meta observation: I'm glad
we're not doing this a week before freeze. <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
<blockquote>I mean that straight addition is a weak hashcode combination since X + Y is
the same as Y + X. "return Objects.hashCode(X, Y)" is an easy way to do it "right" with no
more code than the weak approach. Doesn't matter much here but it's good practice
imo.</blockquote>Make sense. I made the DK hashcode be only based on the key hashcode
though (since the token is just a cached value for getToken() <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>). The hashCode method of Token.KeyBound
don't use Objects.hasCode(), but I really think that in that case it doesn't matter at all
and it avoids the boxing of the boolean. I can change it though if that's the only problem
remaining.<blockquote>Another nit: should we be using a enum for
RowPosition.kind?</blockquote>What do you mean exactly by that? Are you talking of the
kind use in RowPositionSerializer? To have an enum to distinguish between DK and
Token.KeyBound instance of doing the instanceof? If so why not, but I'm not sure it buys
us anything. 


New Comment: 
Okay, we can skip the hashcode change if you're worried about boxing.Yes, that's what I'm
referring to for "kind."  Seeing code like "if kind == 0" means I have to go back to the
kind method to see what a return value of 0 means. 


New Comment: 
Attaching v3, rebased and using an enum for the RowPosition kind. I could have changed a
few <tt>assert key instanceof DecoratedKey</tt> by <tt>assert key.kind() ==
RowPosition.Kind.ROW_KEY</tt> I suppose, but I prefered keeping the instanceof since each
time the next line do a cast to DK, so this feels more coherent like that. 


New Comment: 
+1 


New Comment: 
Integrated in Cassandra #1229 (See <a href="https://builds.apache.org/job/Cassandra/1229/"
class="external-link"
rel="nofollow">https://builds.apache.org/job/Cassandra/1229/</a>)<br/>    remove
assumption that key and token are in bijection<br/>patch by slebresne; reviewed by jbellis
for <a href="https://issues.apache.org/jira/browse/CASSANDRA-1034" title="Remove
assumption that Key to Token is one-to-one" class="issue-link"
data-issue-key="CASSANDRA-1034"><del>CASSANDRA-1034</del></a>slebresne : <a
href="http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1208993"
class="external-link"
rel="nofollow">http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;view=rev&amp;rev=1208993</a><br/>Files
:
<ul>	<li>/cassandra/trunk/CHANGES.txt</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/client/RingCache.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/DecoratedKey.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/IndexScanCommand.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/RangeSliceCommand.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/RowPosition.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexSearcher.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/KeysSearcher.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/db/marshal/LocalByPartionerType.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractBounds.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/Bounds.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/LocalPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/RingPosition.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummary.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableBoundedScanner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamIn.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamRequestMessage.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamingRepairTask.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/tools/BulkLoader.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java</li>	<li>/cassandra/trunk/src/java/org/apache/cassandra/utils/MerkleTree.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/Util.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/db/CleanupTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/db/KeyCollisionTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/db/SerializationsTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/dht/AbstractBoundsTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/dht/BootStrapperTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/dht/RangeTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/io/CompactSerializerTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/service/MoveTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/service/SerializationsTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/service/StorageProxyTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/streaming/SerializationsTest.java</li>	<li>/cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java</li></ul> 


