Pattern changes caused by commit: 7f9f79622aad20cb65349c9765fb43b0e0fa72b6

From: Mediator-2
To:   Mediator-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-4457.txt 

commit 7f9f79622aad20cb65349c9765fb43b0e0fa72b6
Author: Jonathan Ellis <jbellis@apache.org>

    fix re-using index CF sstable names after drop/recreate
    patch by jbellis; reviewed by slebresne for CASSANDRA-2872



==================================
 Issue CASSANDRA-2872 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-2872] While dropping and recreating an index, incremental snapshotting can hang
-----------------

-----------------
Summary: While dropping and recreating an index, incremental snapshotting can hang
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Fri, 8 Jul 2011 08:59:40 +0000
-----------------

-----------------
Resolved at: Tue, 19 Jul 2011 18:08:19 +0000
-----------------

-----------------
Assigned to: Jonathan Ellis
-----------------

-----------------
Description: 

When creating a hard link (at list with JNA), link() hang if the target of the<br/>link
already exists. In theory though, we should not hit that situation<br/>because we use a
new directory for each manual snapshot and the generation<br/>number of the sstables
should prevent this from hapenning with increment<br/>snapshot.

However, when you drop,
then recreate a secondary index, if the sstables are<br/>deleted after the drop and before
we recreate the index, the recreated index<br/>sstables will start with a generation to 0.
Thus, when we start backuping them<br/>incrementally, it will conflict with the sstables
of the previously dropped<br/>index.

First, we should check for the target existance
because calling link() to at<br/>least avoid hanging. But then we must make sure that when
we drop, then<br/>recreate an index, we will either not name the sstables the same way or
the<br/>incremental snapshot use a different directory.
 

-----------------

-----------------
Comments: 

New Comment: 
One option could be to have some kind of "index" generation that we would persist in the
system tables. What I mean here is that if you create a index on say column 'birthdate'
for some CF test, the index would start being called test.birthdate.1 (or just
test.birthdate for compatibility sake), then if you drop it and recreate it, it would be
called test.birthdate.2. That way, we would avoid the name clash during the incremental
backup. 


New Comment: 
I really don't like messing with the name.  That feels like the implementation is leaking
too much into something user-visible. 


New Comment: 
Remind me why simply making sstable generation global doesn't fix this?It's not like
there's so much contention on that AtomicInteger that we need to partition it. 


New Comment: 
<blockquote>Remind me why simply making sstable generation global doesn't fix
this?</blockquote>If you drop an index, then shutdown the node, then restart and recreate
the index. Upon restart and crawling of the existing data files, it could be that the
first available generation is the one of a sstable of the dropped index.I guess there is
two reasonably simple solutions:<ol>	<li>scan the (incremental) snapshot directories for
the generation number too. If we do that, I guess we don't even have to make the
generation global as long as we do this scanning each time a ColumnFamilyStore is
created.</li>	<li>make the generation number persistent in the system tables (again, no
need to make the number global for that).</li></ol>I think I prefer the second solution
because it's more general and feels more elegant. But we would still have to scan the data
dir and take the max(what we found during scan, what's in the system table) in case people
force-feed data files that weren't created on that node (or the system tables are
wiped).That being said, I totally agree that the generation number don't have to be
partitioned if we don't want to. But not sure it's a big deal that way either. 


New Comment: 
<blockquote>we would still have to scan the data dir and take the max(what we found during
scan, what's in the system table) in case people force-feed data files that weren't
created on that node (or the system tables are wiped).</blockquote>That's why I prefer the
scan approach &#8211; I'd rather have a single reliable source of truth (the contents of
the fs) than an unreliable one that we have to supplement with the reliable one
anyway.Patch attached. 


New Comment: 
<blockquote>That's why I prefer the scan approach</blockquote>Yeah, I kind of wrote my
comment before as things were coming to me, in particular I wrote that I was preferring
the second solution before realizing we would still need to scan. Agreeing that the scan
approach is cleaner/easier.On the patch, shouldn't we only include the sstables for the
column family we're creating. I know it's ok to take the max for all cfs, but it feels a
bit random unless we have only one global generation. And could be worst avoiding 2-3
mails on the mailing of people wondering why that has changed (I mean, I'm sure someone
will remark it) <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>. 


New Comment: 
v2 restricts to the CF in question. 


New Comment: 
lgtm +1And I can confirm that this fix the failing test of <a
href="https://issues.apache.org/jira/browse/CASSANDRA-2521" title="Move away from Phantom
References for Compaction/Memtable" class="issue-link"
data-issue-key="CASSANDRA-2521"><del>CASSANDRA-2521</del></a>. 


New Comment: 
committed 


