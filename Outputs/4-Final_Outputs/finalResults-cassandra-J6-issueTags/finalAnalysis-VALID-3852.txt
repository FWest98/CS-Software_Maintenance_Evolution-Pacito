Pattern changes caused by commit: 15a0f2b20771bfe21c7539c709cdb30032e652ab

From: Abstract Factory-3
To:   Abstract Factory-2

From: Factory Method-3
To:   Factory Method-2

From: Mediator-2
To:   Mediator-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-3852.txt 

commit 15a0f2b20771bfe21c7539c709cdb30032e652ab
Author: Jonathan Ellis <jbellis@apache.org>

    fixes for verifying destinationavailability under hinted conditions
    patch by Narendra Sharma and jbellis for CASSANDRA-2514



==================================
 Issue CASSANDRA-2514 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-2514] batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes
-----------------

-----------------
Summary: batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Wed, 20 Apr 2011 00:29:23 +0000
-----------------

-----------------
Resolved at: Wed, 20 Apr 2011 18:16:02 +0000
-----------------

-----------------
Assigned to: Narendra Sharma
-----------------

-----------------
Description: 

We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace
definition:<br/>&lt;snip&gt;<br/>keyspaces:
<ul class="alternate" type="square">	<li>name:
KeyspaceMetadata<br/>      replica_placement_strategy:
org.apache.cassandra.locator.NetworkTopologyStrategy<br/>      strategy_options:<br/>     
  DC1 : 2<br/>        DC2 : 2<br/>      replication_factor:
4<br/>&lt;/snip&gt;</li></ul>
I shutdown all except one node and waited for the live node
to recognize that other nodes are dead. Following is the nodetool ring output on the live
node:<br/>Address         Status State   Load            Owns    Token                    
                  <br/>                                                      
169579575332184635438912517119426957796     <br/>10.17.221.19    Down   Normal  ?         
     29.20%  49117425183422571410176530597442406739      <br/>10.17.221.17    Up    
Normal  81.64 KB        4.41%   56615248844645582918169246064691229930     
<br/>10.16.80.54     Down   Normal  ?               21.13% 
92563519227261352488017033924602789201      <br/>10.17.221.18    Down   Normal  ?         
     45.27%  169579575332184635438912517119426957796     

I expect UnavailableException
when I send batch_mutate request to node that is up. However, it returned
TimeOutException:<br/>TimedOutException()<br/>    at
org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493)<br/> 
  at
org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)<br/>   
at
org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)

Following
is the cassandra-topology.properties
<ol>	<li>Cassandra Node IP=Data
Center:Rack<br/>10.17.221.17=DC1:RAC1<br/>10.17.221.19=DC1:RAC2</li></ol>
10.17.221.18=DC2:RAC1<br/>10.16.80.54=DC2:RAC2
 

-----------------

-----------------
Comments: 

New Comment: 
I think the issue is because DatacenterWriteResponseHandler.assureSufficientLiveNodes is
not checking for live nodes.DatacenterWriteResponseHandler.assureSufficientLiveNodes works
on writeEndpoints. writeEndpoints contains list of the all the endpoints (may be more if
there are nodes bootstrapping).I think either writeEndpoints should ignore
dead/unreachable nodes or DatacenterWriteResponseHandler.assureSufficientLiveNodes should
use hintedEndpoints.keySet() as that contains the live endpoints.<br/>I compared the
implementation with WriteResponseHandler.assureSufficientLiveNodes and found that it uses
hintedEndpoints.I am attaching the patch that works for me. 


New Comment: 
Use hintedEndpoints instead of writeEndpoints to work on live endpoints only. 


New Comment: 
The code to reproduce this issue is a simple batch mutate operation. The operation I
performed involved adding 2 columns to a SuperColumn. Let me know if it is not
reproducible. I will provide the sample code. 


New Comment: 
Good catch, that is a bug.v2 adds a couple improvements:<ul class="alternate"
type="square">	<li>only count the hinted endpoint towards live count if it's a normal
write destination (hints can be sent elsewhere if all the write destinations are
dead)</li>	<li>similar fix for DSWRH (EACH_QUORUM)</li>	<li>unrelated fix in WRH for
CL.ANY not to continue through to the CL.Q/ALL code</li></ul> 


New Comment: 
how does that look to you? 


New Comment: 
Looks good to me. Just one comment/question:<br/>hintedEndpoints is subset of
writeEndpoints. So is the additional check writeEndpoints.contains(destination), while we
are iterating over hintedEndpoints, needed? I think assert would be better here. 


New Comment: 
That's the point, hintedEndpoints is <b>usually</b> but not always a subset of
writeEndpoints. Here is the code from getHintedEndpoints:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre class="code-java">  
     <span class="code-comment">// assign dead endpoints to be hinted to the closest live
one, or to the local node</span>        <span class="code-comment">// (since it is
trivially the closest) <span class="code-keyword">if</span> none are alive.  This way, the
cost of doing</span>        <span class="code-comment">// a hint is only adding the hint
header, rather than doing a full extra write, <span class="code-keyword">if</span>
any</span>        <span class="code-comment">// destination nodes are alive.</span>       
<span class="code-comment">//</span>        <span class="code-comment">// we <span
class="code-keyword">do</span> a 2nd pass on targets instead of using temporary
storage,</span>        <span class="code-comment">// to optimize <span
class="code-keyword">for</span> the common <span class="code-keyword">case</span>
(everything was alive).</span>        InetAddress localAddress =
FBUtilities.getLocalAddress();        <span class="code-keyword">for</span> (InetAddress
ep : targets)        {            <span class="code-keyword">if</span>
(map.containsKey(ep))                <span class="code-keyword">continue</span>;          
 <span class="code-keyword">if</span> (!StorageProxy.shouldHint(ep))            {         
      <span class="code-keyword">if</span> (logger.isDebugEnabled())                   
logger.debug(<span class="code-quote">"not hinting "</span> + ep + <span
class="code-quote">" which has been down "</span> +
Gossiper.instance.getEndpointDowntime(ep) + <span class="code-quote">"ms"</span>);        
       <span class="code-keyword">continue</span>;            }            InetAddress
destination = map.isEmpty()                                    ? localAddress             
                      : snitch.getSortedListByProximity(localAddress,
map.keySet()).get(0);            map.put(destination, ep);        }</pre></div></div> 


New Comment: 
that is: our last-resort local hint storage may not be part of writeEndpoints (probably
won't be, on a large cluster). 


New Comment: 
Got it. In my setup I had HH disabled. So I overlooked the rest of the
getHintedEndpoints.The change looks good to me now. Thanks! 


New Comment: 
committed, thanks! 


