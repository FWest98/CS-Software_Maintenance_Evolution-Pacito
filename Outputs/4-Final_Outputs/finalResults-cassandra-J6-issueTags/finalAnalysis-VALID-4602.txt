Pattern changes caused by commit: ba1821f87c0e757a4a1882d071c99952b7bd4695

From: Decorator-0
To:   Decorator-1

From: Mediator-2
To:   Mediator-3


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-4602.txt 

commit ba1821f87c0e757a4a1882d071c99952b7bd4695
Author: Pavel Yaskevich <xedin@apache.org>

    Add block level checksum for compressed data
    patch by Pavel Yaskevich; reviewed by Sylvain Lebresne for CASSANDRA-1717



==================================
 Issue CASSANDRA-1717 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-1717] Cassandra cannot detect corrupt-but-readable column data
-----------------

-----------------
Summary: Cassandra cannot detect corrupt-but-readable column data
-----------------

-----------------
Issue type: New Feature
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Sat, 6 Nov 2010 21:09:14 +0000
-----------------

-----------------
Resolved at: Tue, 9 Aug 2011 16:08:29 +0000
-----------------

-----------------
Assigned to: Pavel Yaskevich
-----------------

-----------------
Description: 

Most corruptions of on-disk data due to bitrot render the column (or row) unreadable, so
the data can be replaced by read repair or anti-entropy.  But if the corruption keeps
column data readable we do not detect it, and if it corrupts to a higher timestamp value
can even resist being overwritten by newer values.
 

-----------------

-----------------
Comments: 

New Comment: 
Naive proof of concept to checksum at the column level.  If this is too much overhead we
can checksum at the column index block instead, and check that on digest mismatch (so we
don't have to deserialize the entire block for each read).Otherwise, this needs to be
extended to (a) cover supercolumns and (b) maintain backwards compatibility w/ old data
files. 


New Comment: 
I think we should consider delaying this until #16 is fixed (hopefully in 0.8): adding
compression will require a block based format, which is a natural level to checksum at.
Additionally, if a user wanted to force corruption detection per lookup (as opposed to
only when the entire block is read) GZIPs built checksumming kills two birds with one
stone. 


New Comment: 
Would we really save that much by waiting until #16 is done? Perhaps we should take a shot
at this in 0.7. Right now it's possible to have data corrupted, then replicated leading to
loss of data.Edit: Apparently 16 is already done. 


New Comment: 
Sorry, that was supposed to say #674. We've been dealing with compaction and garbage
collection issues, and I haven't had as much time to work on it recently. I'm hoping to be
able to resume #674 in the next few weeks, but I don't think it will be 0.8 material. 


New Comment: 
Should probably do this (either column-level or block-level) at the same time as <a
href="https://issues.apache.org/jira/browse/CASSANDRA-47" title="SSTable compression"
class="issue-link" data-issue-key="CASSANDRA-47"><del>CASSANDRA-47</del></a>. 


New Comment: 
I know I'm starting to sound like a broken record, but <a
href="https://issues.apache.org/jira/browse/CASSANDRA-674" title="New SSTable Format"
class="issue-link" data-issue-key="CASSANDRA-674"><del>CASSANDRA-674</del></a> is going to
include checksums. And its almost ready for reviewing. 


New Comment: 
After thinking about this for a while I think we should do checksum at the column level
only which will give us better control over individual columns and does not seem to be a
big overhead (instead of doing it at the column index level and relaying on digest).
Checksum on the compressed block level is unnecessary because bitrot, for example, will be
detected right on decompression or column deserialization time. Thoughts? 


New Comment: 
<blockquote>checksum at the column level only which will give us better control over
individual columns and does not seem to be a big overhead</blockquote>I agree that it is
by far the simplest approach for non compressed data, but I, for one, am a bit concerned
by the overhead: 4 bytes per column is not negligible. On some load, that could easily
mean a 10-20% data size increase. Basically I am concerned about people upgrading to 1.0
and want to make sure that upgrading brings no surprise for them (and this even if they
don't "trust" compression yet, which would be perfectly reasonable). For that to be true,
I think that if we go with checksum at the column level we would need to make that
optional and off by default.<blockquote>Checksum on the compressed block level is
unnecessary because bitrot, for example, will be detected right on
decompression</blockquote>Not sure that's bulletproof. I don't think all compression
algorithm ships with a checksum (I don't know about snappy typically). When they don't,
it's totally possible for bitrot to corrupt compressed data without being a problem at
decompression nor at deserialization if you're unlucky (granted it is more unlikely to go
undetected that without compression but it is not good enough). So either we check that
snappy use checksumming and we only add support for algorithm that does, or it is still
useful. 


New Comment: 
Making a checksum optional and off by default sounds good to me.<blockquote>Not sure
that's bulletproof...</blockquote>That is why I mentioned that if we have checksum per
column it will work as a protection from wrong decompression on the block level and spares
us additional read and check, isn't it? 


New Comment: 
My bad, I read that as "let's not use checksum for compression at all". Nevermind. 


New Comment: 
I think checksums per column would be way too much overhead. We already add a lot of
overhead to all data stored in Cassandra, we should be careful about adding more. 


New Comment: 
That is why we want to let users to decide if they need that protection at all. 


New Comment: 
I'd rather not add more configuration complexity for this.What is the downside to doing it
at column index level?  Feels like a good compromise between overhead and granularity to
me. 


New Comment: 
If we do that on the column index level won't that imply that we will checksum (and check)
a row as a whole instead of a single column?Can you please describe your idea about doing
it at the column index in here just to make sure that we all are on the same page. 


New Comment: 
You should also consider that checksumming at the column index or column level will
require separate checksums for the column index and row header. Checksumming at the block
level gets you that in one go. 


New Comment: 
what about this?<ul class="alternate" type="square">	<li>add checksum-on-flush to
SequentialWriter</li>	<li>compressed reads will always check on
uncompress</li>	<li>uncompressed reads will check on repair</li>	<li>and also on read
repair digest mismatch retry</li></ul> 


New Comment: 
This is a good idea but it has few complications:<ul class="alternate"
type="square">	<li>buffer length should be stored in order to be used by
reader</li>	<li>reads should be aligned by that buffer length so we always read a whole
checksummed chunk of the data which implies that we will potentially always need to read
more data on each request</li></ul>This seems to be a clear tradeoff between using
additional space to store checksum for index + columns for each row v.s. doing more I/O... 


New Comment: 
xedin on IRC asked me to comment on this issue. For reference of what other systems do:
HDFS checksums every file in 512-byte chunks with a CRC32. It's verified on write (by only
the first DN in the pipeline) and on read (by the client). If the client gets a checksum
error while reading, it will report this to the NN, and the NN will mark that block as
corrupt, schedule another replication, etc.This is all transparent to the HBase layer
since it's done at the FS layer. So, HBase itself doesn't do any extra checksumming. If
you compress your tables, then you might get an extra layer of checksumming for free from
gzip as someone mentioned above.For some interesting JIRAs on checksum performance, check
out <a href="https://issues.apache.org/jira/browse/HADOOP-6148" title="Implement a pure
Java CRC32 calculator" class="issue-link"
data-issue-key="HADOOP-6148"><del>HADOOP-6148</del></a> and various followups, as well as
current work in progress <a href="https://issues.apache.org/jira/browse/HDFS-2080"
title="Speed up DFS read path by lessening checksum overhead" class="issue-link"
data-issue-key="HDFS-2080"><del>HDFS-2080</del></a> 


New Comment: 
BTW, we also scan all blocks verifying checksums in a background process, continuously, to
catch bit-rot even for data that isn't getting read. 


New Comment: 
My 2 cents:I see 3 options that seems to make sense somehow:<ol>	<li>checksums at the
column level:	<ul>		<li>pros: easy to do, easy to recover from a bitrot and efficiently so
(efficiently in that in general we would be able to only drop one column for a given
bitrot; it's more complicated if something in the row header (row key, row size, ...) is
bitrotten though).</li>		<li>cons: high overhead (mainly in disk space usage but also on
cpu usage because we have much more checksums to check)</li>	</ul>	</li>	<li>checksums at
the row level (or column index level, but I think this is essentially the same, isn't
it?):	<ul>		<li>pros: easy to recover from bitrot (we drop the row), though potentially
more wasteful than "column level". Incurs a small space overhead for big
rows.</li>		<li>cons: can't realistically check on every reads, so we need to do it only
on compaction/repair and on read digest mismatch (that last one is non optional if we want
checksums to be sure in that bitrot never propagate to other node); this adds complexity
and some I/O to check checksums on read digest mismatch that is not necessary (read digest
mismatch won't in general be due to bitrot). Also incurs a important space overhead for
tiny rows.</li>	</ul>	</li>	<li>checksums at the block level:	<ul>		<li>pros: super easy
in the compressed case (can be done "on every read", or more precisely each time we read a
block). Incurs a minimum overhead.</li>		<li>cons: super <b>not</b> easy in the
non-compressed case. We don't have blocks in the uncompressed case. While writing, we
could use the buffer size as a block size and add a checksum on flush. The problems are on
reads however.  First, we would need to align buffers on reads (which we don't do in the
non-compressed case) as Pavel said, which likely involves more reBuffer in general (aka
more I/O). But perhaps more importantly, I have no clue how you could make that work with
mmap efficiently (we would potentially have a checksum in the middle of a column value as
far as mmap is concerned).  Also slightly harder to recover from bitrot without dropping
the whole sstable (but doable as long as we have the index
around).</li>	</ul>	</li></ol>There may be other solutions I don't see, and there may be
some pros/cons for the ones above that I have missed (please feel free to complete).But
based on those, my personal opinion is that "column level" has too big an overhead and
"block level" is really problematic in the mmap non-compressed case (but sound like the
best option to me if we ignore mmap).So my personal preference leans towards using "block
level" but only having checksums in the compressed case and maybe in an uncompressed mode
for which mmap would be deactivated.If we really don't want to consider that, "row level"
checksums would maybe be the lesser evil. But I'm not fond of the overhead in case of tiny
rows and the 'check checksums on read digest mismatch', while I believe necessary in that
case, doesn't sound like the best idea ever. 


New Comment: 
Seems like in terms of overhead (which based on <a
href="https://issues.apache.org/jira/browse/HADOOP-6148" title="Implement a pure Java
CRC32 calculator" class="issue-link"
data-issue-key="HADOOP-6148"><del>HADOOP-6148</del></a> is potentially very significant in
both storage and CPU) - block level checksums is much better.I understand you believe
block level checksums are easy in the compressed case but not easy in the non-compressed
case.So can't you just implement a no-op compression option that will utilize what you're
doing / planning to do for compression in terms of block structure and block level
checksums?<br/>That would be easy if you already designed the compression algorithm to be
plugable. And if the compression algorithm is not plugable yet - adding that would have an
obvious side benefit besides having easier implementation of block level checksums. 


New Comment: 
The column index level seems like a nice fit to me. it would at least allow partial column
scans (in the case of bitrot) 


New Comment: 
<blockquote>can't you just implement a no-op compression option that will utilize what
you're doing / planning to do for compression in terms of block structure and block level
checksums?</blockquote>Good question.  Pavel? 


New Comment: 
<blockquote>So can't you just implement a no-op compression option</blockquote>That is
exactly what I had in mind for "maybe in an uncompressed mode for which mmap would be
deactivated". It will be trivial once we have made it easy to switch compression algorithm
(which is trivial too btw, I'll do that probably ... well why not now).That would make
things fairly clear imho. We would say "no mmap" with compression and no checksum without
compression, but you have the "compression with no-op algorithm". This has my preference
as said previously, but we must be aware that people will ask why we have a non-mmap non
compressed mode and a compressed no-op mode (we cannot really get rid of the first one
because otherwise we say "you shall use mmap forever now"). 


New Comment: 
<blockquote>The column index level seems like a nice fit to me.</blockquote>Right, I kind
of forgot this one previously, it is different from row level and actually better (you
will check on each read). However, it has a fair overhead in terms of cpu and storage
(less than column level, but much that we would want imho). I still believe block level is
the right level, if not for the mmap problem. 


New Comment: 
Block level and column index level are actually the same right? 64kbThe reason block isn't
ideal to me is it makes it much harder to recover/support partial reads since the block
has no context in the file format.  Though if there is corruption with block level
compression then it's inherently a block level problem <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>So what kind of recovery can we support?
Can we ever recover from bad blocks or just throw errors "bad blocks found, manual repair
required "? 


New Comment: 
<blockquote>Block level and column index level are actually the same right?
64kb</blockquote>True, but that only covers the big indexed rows. If you have lots of tiny
rows, you have a much bigger overhead. Block level is more consistent and predictable. And
with column index level, you also need to checksum the row header, so it's a slightly
greater overhead anyway even for big rows. It's also a bit more complicated conceptually,
you need to checksum row header and body separately and distinguish between indexed and
non-indexed rows.<blockquote>The reason block isn't ideal to me is it makes it much harder
to recover/support partial reads since the block has no context in the file
format</blockquote>I agree as I mentioned earlier that it is harder. I don't know about
the much (at least for recovery) however. With the row index, I'm sure it's not too hard
to only drop the block and maybe a little bit around to get something consistent. Yes, it
means we will always drop more than with column index level, but imho it is not like
bitrot happens so often that it matters much (but I understand one could
disagree).<br/>Also, with column index, you can still have bitrot of the row header, in
which case the whole row is still screwed.Anyway, don't get me wrong, I'm not saying that
column index is a stupid idea. I think however that for some (non exceptional( use cases
(small rows, aka, probably most of the 'static CF'), the overhead will be much more
important than with block level. I also think block level is cleaner in that you don't
have to care about different cases. On the other side, the advantages of the column index
level are only useful in the exceptional case of bitrot (not the case we should optimize
for imho), and it is more efficient then but not so much. 


New Comment: 
Right, I think given that we are using block compression it really only makes sense todo
checksums at the block level, I just didn't know what recovery tools we can build.Sounds
like using the row index we could go repair the range containing the bad block(s) from
replicas. 


New Comment: 
I like compression options {snappy, checksum-only, none}. 


New Comment: 
If you're afraid of people getting confused with compression options that have nothing
with compression, why not give it a more generic name like encoding options. e.g. encoding
options = (snappy-with-checksum, checksum-only, none) 


New Comment: 
<blockquote>can't you just implement a no-op compression option that will utilize what
you're doing / planning to do for compression in terms of block structure and block level
checksums? Good question. Pavel?</blockquote>That sounds like a special-casing and it has
complications mentioned before - more I/O, need to hold up buffer size, won't play nice
with mmap. Placing it to the block level will harden creation of the tools to process
corruption (as Jake mentioned) because we think in the "data model" way not in the file
block way.First all we should define a goal we pursue by this - which is essential.If this
is only about repair and replication I think that the good way will be to checksum at row
boundary level which will be: relatively simple to check and play nice with mmap.I still
think that the best way to check for corruption will be to use checksum at row header (key
and row index) and column level even if that introduces disk space and CPU overhead (the
necessary sacrifice), this could be most elegant solution because of few things where two
of them could be: introduces no system wide complexity (aka special-casing) related to how
we work with SSTables and repair and allow as think in our data model terms.But it somehow
fills like we are missing better solution in here... 


New Comment: 
<blockquote>That sounds like a special-casing</blockquote>I don't follow.  It feels
exactly the opposite of a special case to me: using the per-block code that we already
have.<blockquote>more I/O, need to hold up buffer size, won't play nice with
mmap</blockquote>That's why we give people the choice.  But I'm pretty sure that after 1.0
we'll make compression the default.  So I don't want to add a lot of complexity for
uncompressed sstables.<blockquote>we should define a goal we pursue by
this</blockquote>Here's our requirement:<ul class="alternate" type="square">	<li>prevent
corruption from being replicated</li>	<li>detect and remove corruption on
repair</li></ul>Nice to have:<ul class="alternate" type="square">	<li>low complexity of
implementation</li>	<li>low space overhead</li>	<li>detect corruption as soon as it is
read</li></ul><blockquote>I still think that the best way to check for corruption will be
to use checksum at row header (key and row index) and column level</blockquote>That's not
crazy, and it achieves all goals except low space overhead.  But for the reasons above I
still think block-level is a better fit. 


New Comment: 
By special-casing i mean the following:<blockquote>We would say "no mmap" with compression
and no checksum without compression, but you have the "compression with no-op algorithm".
This has my preference as said previously, but we must be aware that people will ask why
we have a non-mmap non compressed mode and a compressed no-op mode (we cannot really get
rid of the first one because otherwise we say "you shall use mmap forever
now").</blockquote><blockquote>But I'm pretty sure that after 1.0 we'll make compression
the default. So I don't want to add a lot of complexity for uncompressed
sstables.</blockquote>In that case can we consider making that compression only feature
instead of banging our heads against the wall trying to come up with solution for
non-compressed data? 


New Comment: 
<blockquote>can we consider making that compression only feature?</blockquote>I'm fine
with that.  We can always add the checksum-only uncompressed encoding later if someone
wants it badly enough. <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
Let me prepare a patch for that and meanwhile wait if someone disagrees <img
class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.png"
height="16" width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
patch introduces checksum on the compressed block level. rebased with the latest trunk
(last commit 64d3409556de0b305cd478877c6afc108d8f02b0) 


New Comment: 
<blockquote>In that case can we consider making that compression only
feature</blockquote>For what it's worth, that was basically the idea of having a no-op
compression. Basically we only support compression and if people really really want
something uncompressed with checksum, we give them a compression algorithm that doesn't
compress squat.<br/>Anyway, I'm for "compression only feature" and the no-op compression
was just some idea "in case we need it". We don't even have to do it now. But if someone
asks, it will be trivial to do. 


New Comment: 
Comments:<ul>	<li>CSW.flushData() forgot to reset the checksum (this is caught by the unit
tests btw).</li>	<li>We should convert the CRC32 to an int (and only write that) as it is
an int internally (getValue() returns a long only because CRC32 implements the interface
Checksum that require that).</li>	<li>Here we checksum the compressed data. The other
approach would be to checksum the uncompressed data. The advantage of checksumming
compressed data is the speed (less data to checksum), but checksumming the uncompressed
data would be a little bit safer. In particular, it would prevent us from messing up in
the decompression (and we don't have to trust the compression algorithm, not that I don't
trust Snappy, but...). This is a clearly a trade-off that we have to make, but I admit
that my personal preference would lean towards safety (in particular, I know that
checksumming the uncompressed data give a bit more safety, I don't know what is our exact
gain quantitatively with checksumming compressed data). On the other side, checksumming
the uncompressed data would likely mean that a good part of the bitrot would result in a
decompression error rather than a checksum error, which is maybe less convenient from the
implementation point of view. So I don't know, I guess I'm thinking aloud to have other's
opinions more than anything else.</li>	<li>Let's add some unit tests. At least it's
relatively easy to write a few blocks, switch one bit in the resulting file, and checking
this is caught at read time (or better, do that multiple time changing a different bit
each time).</li>	<li>As Todd noted, <a
href="https://issues.apache.org/jira/browse/HADOOP-6148" title="Implement a pure Java
CRC32 calculator" class="issue-link"
data-issue-key="HADOOP-6148"><del>HADOOP-6148</del></a> contains a bunch of discussions on
the efficiency of java CRC32. In particular, it seems they have been able to close to
double the speed of the CRC32, with a solution that seems fairly simple to me. It would be
ok to use java native CRC32 and leave the improvement to another ticket, but quite frankly
if it is that simple and since the hadoop guys have done all the hard work for us, I say
we start with the efficient version directly.</li></ul> 


New Comment: 
I don't mind doing the CRC optimization in a separate ticket.  There are other places (CL,
others?) that use CRC as well. 


New Comment: 
Oh, true. Make sense to move to a separate ticket then. 


New Comment: 
<blockquote>CSW.flushData() forgot to reset the checksum (this is caught by the unit tests
btw).</blockquote>  Not a problem since it was due to Sylvain's bad merge.<blockquote>We
should convert the CRC32 to an int (and only write that) as it is an int internally
(getValue() returns a long only because CRC32 implements the interface Checksum that
require that).</blockquote>  Lets leave that to the ticket for CRC optimization which will
allow us to modify that system-wide.<blockquote>Here we checksum the compressed data. The
other approach would be to checksum the uncompressed data. The advantage of checksumming
compressed data is the speed (less data to checksum), but checksumming the uncompressed
data would be a little bit safer. In particular, it would prevent us from messing up in
the decompression (and we don't have to trust the compression algorithm, not that I don't
trust Snappy, but...). This is a clearly a trade-off that we have to make, but I admit
that my personal preference would lean towards safety (in particular, I know that
checksumming the uncompressed data give a bit more safety, I don't know what is our exact
gain quantitatively with checksumming compressed data). On the other side, checksumming
the uncompressed data would likely mean that a good part of the bitrot would result in a
decompression error rather than a checksum error, which is maybe less convenient from the
implementation point of view. So I don't know, I guess I'm thinking aloud to have other's
opinions more than anything else.</blockquote>  It checksums original (non-compressed)
data and stores checksum at the end of the compressed chunk, reader makes a checksum check
after decompression.<blockquote>Let's add some unit tests. At least it's relatively easy
to write a few blocks, switch one bit in the resulting file, and checking this is caught
at read time (or better, do that multiple time changing a different bit each
time).</blockquote>  Test was added to CompressedRandomAccessReaderTest.<blockquote>As
Todd noted, <a href="https://issues.apache.org/jira/browse/HADOOP-6148" title="Implement a
pure Java CRC32 calculator" class="issue-link"
data-issue-key="HADOOP-6148"><del>HADOOP-6148</del></a> contains a bunch of discussions on
the efficiency of java CRC32. In particular, it seems they have been able to close to
double the speed of the CRC32, with a solution that seems fairly simple to me. It would be
ok to use java native CRC32 and leave the improvement to another ticket, but quite frankly
if it is that simple and since the hadoop guys have done all the hard work for us, I say
we start with the efficient version directly.</blockquote>  As decided previously this
will be a matter of the separate ticket.Rebased with latest trunk (last commit
1e36fb1e44bff96005dd75a25648ff25eea6a95f) 


New Comment: 
<blockquote><blockquote>We should convert the CRC32 to an int (and only write that) as it
is an int internally (getValue() returns a long only because CRC32 implements the
interface Checksum that require that).</blockquote>Lets leave that to the ticket for CRC
optimization which will allow us to modify that system-wide</blockquote>Let's
not:<ul>	<li>this is completely orthogonal to switching to a drop-in, faster, CRC
implementation.</li>	<li>it is unclear we want to make that system-wide. Imho, it is not
worth breaking commit log compatibility for that, but it it stupid to commit new code that
perpetuate the mistake, especially to change it later.</li></ul> 


New Comment: 
Saving 4 bytes out of 64K doesn't seem like enough benefit to make life harder for
ourselves if we want to use a long checksum later. 


New Comment: 
+1 with Jonathan, also it is better if we satisfy interface instead of relying on internal
implementation details that also could be helpful if we will decide to change checksum
algorithm. 


New Comment: 
What are the chance we'll switch from CRC32 any time soon ? And even if we do, why would
that help us to save 4 bytes of 0's right now ? We will still have to bump the file format
versioning and to keep the code to be compatible with the old CRC32 format if we do so.
It's not like the only difference between checksum algorithms is the size of the
checksum.So yes, 4 bytes out of 64K is not a lot of data, but to knowingly write 4 bytes
of 0's every 64k every time for the vague remote chance that it may save us 1 or 2 lines
of code someday (again, that even remains to be proven) feels ridiculous to me. But if I'm
the only one to feel that way, fine, it's not a big deal. 


New Comment: 
I still think that such change is a matter of the separate ticket as we will want to
change CRC stuff globally, we can make own Checksum class with will return int value,
apply performance improvements mentioned by <a
href="https://issues.apache.org/jira/browse/HADOOP-6148" title="Implement a pure Java
CRC32 calculator" class="issue-link"
data-issue-key="HADOOP-6148"><del>HADOOP-6148</del></a> to it and use system-wide.Is there
anything else that keeps this from being committed? 


New Comment: 
As previously said, I both disagree on using 8 bytes when we need 4 and that using 4 is a
matter for another ticket, but since this is probably me being too anal as usual, +1 on
the rest of the patch, modulo a small optional nitpick: the toLong() function is a bit
hard to read imho. It's hard to see where the parenthesis are, and if it does the right
thing. It seems ok though, I just think a simple for loop on the bytes would be more
readable. We also historically keep ByteBufferUtil for ByteBuffer manipulations and use
FBUtilities for byte[] manipulation. 


New Comment: 
Ok, I will move toLong(byte[] bytes) to FBUtilities and commit, thanks! 


New Comment: 
You're right, if we change checksum implementation we need to bump sstable revision
anyway.  +1 casting to int here.  (But as you said above, -1 changing this in CommitLog.) 


New Comment: 
v3 which removes BBU.toLong and adds FBU.byteArrayToInt + uses int instead of long for
checksum 


New Comment: 
lgtm, +1 


New Comment: 
committed. 


