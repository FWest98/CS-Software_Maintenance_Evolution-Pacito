Pattern changes caused by commit: 02672936f635c93e84ed6625bb994e1628da5a9b

From: Decorator-2
To:   Decorator-1

From: Flyweight-2
To:   Flyweight-1

From: Mediator-3
To:   Mediator-1

From: Strategy-0
To:   Strategy-1

From: Template Method-3
To:   Template Method-2


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6837.txt 

commit 02672936f635c93e84ed6625bb994e1628da5a9b
Author: Vijay Parthasarathy <vijay2win@gmail.com>

    update clhm to 1.3
    patch by Vijay; reviewed by jbellis for CASSANDRA-4150



==================================
 Issue CASSANDRA-4150 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-4150] Allow larger cache capacities than 2GB
-----------------

-----------------
Summary: Allow larger cache capacities than 2GB
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Fri, 13 Apr 2012 20:08:54 +0000
-----------------

-----------------
Resolved at: Thu, 7 Jun 2012 04:26:46 +0000
-----------------

-----------------
Assigned to: Vijay
-----------------

-----------------
Description: 

The problem is that capacity is a Integer which can maximum hold 2 GB,<br/>I will post a
fix to CLHM in the mean time we might want to remove the maximumWeightedCapacity code path
(atleast for Serializing cache) and implement it in our code.
 

-----------------

-----------------
Comments: 

New Comment: 
issue: <a href="http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=33"
class="external-link"
rel="nofollow">http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=33</a>
created. 


New Comment: 
For the records the comments from issue in CLHM library:<blockquote>I think a long
capacity is fine, but I'm not actively working on a next release to roll this into soon.
If this is critical than it could be a patch release. You are of course welcome to fork if
neither of those options are okay.I helped my former colleagues at Google with Guava's
CacheBuilder (formerly MapMaker), which could be considered the successor to this project.
There the maximum weight is a long.</blockquote>IRC: Guava doesnt support
descendingKeySetWithLimit<br/>Possibly fork the CLHM code into Cassandra code base or drop
the hotkey's method and use guava (Thats the only limitation which i see for now). 


New Comment: 
CLHM (Ben) fixed the issue:<blockquote>Fixed in v1.3. I plan on releasing this
tonight.Also introduced EntryWeigher&lt;K, V&gt; to allow key/value weighing. We fixed
this oversight in Guava's CacheBuilder from the get-go. I believe Cassandra wanted entry
weighers too, but it wasn't high priority (no bug filed). Please consider adopting it when
you upgrade the library.</blockquote> 


New Comment: 
Using MemoryMeter on a hot path like cache updates scares me a bit &#8211; it's pretty
slow.  Might want to switch to using the dataSize * liveRatio measurement that memtables
use.Aside #1: So basically current 1.1 CLHM is totally broken since it has a capacity of X
MB but weighs each item as one byte?Aside #2: I don't see any use of IRCP with
useMemoryWeigher=false, looks like we can remove that parameter 


New Comment: 
Aside #3: would prefer to assert value.size() &lt; Integer.MAX_VALUE in SC.Weigher,
instead of having a Math.min call in there.  Strongly doubt the rest of the code would
support serialized values larger than that anyway. 


New Comment: 
<blockquote>So basically current 1.1 CLHM is totally broken since it has a capacity of X
MB but weighs each item as one byte?</blockquote>(If that's the case, we should target
this for 1.1.1.) 


New Comment: 
&gt;&gt;&gt; Using MemoryMeter on a hot path like cache updates scares me a bit â€“ it's
pretty slow. Might want to switch to using the dataSize * liveRatio measurement that
memtables use.<br/>Will do.&gt;&gt;&gt; So basically current 1.1 CLHM is totally broken
since it has a capacity of X MB but weighs each item as one byte?<br/>No we assume that
the KeyCache to be 48 byte long average size (CacheService.AVERAGE_KEY_CACHE_ROW_SIZE)...
<br/>basically, cache capacity = keyCacheInMemoryCapacity /
AVERAGE_KEY_CACHE_ROW_SIZE<br/>hence each entry is considered to be 1 byte/one entry.
(Kind of hackie may be we didn't have EntryWeigher i guess, now that we have it we can get
rid of that logic).&gt;&gt;&gt; I don't see any use of IRCP with useMemoryWeigher=false,
looks like we can remove that parameter<br/>We still need it if we dont have Jamm
Enabled.&gt;&gt;&gt; would prefer to assert value.size() &lt; Integer.MAX_VALUE in
SC.Weigher<br/>Will do, Plz let me know if everything else is fine...&gt;&gt;&gt;If that's
the case, we should target this for 1.1.1<br/>Either ways we have to target for 1.1.1
because the serialized cache is broken <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/sad.png" height="16" width="16"
align="absmiddle" alt="" border="0"/> 


New Comment: 
The problem with the liveRatio is that it is variable and we might have a leak if we use
the ratio... <ul class="alternate" type="square">	<li>Lets say when we add the entry we
have a ratio of 1.2 and when the entry is removed we will have the ratio of 1.1 then we
will leak some space for cache utilization. So it will be better if this number doesn't
change.</li></ul>0001 removes memory meter and does changes to fix the issue.<br/>0002
adds entry weight and small refactor to change the Provider interface as suggested.Thanks! 


New Comment: 
<div class="code panel" style="border-width: 1px;"><div class="codeContent
panelContent"><pre class="code-java">.       EntryWeigher&lt;KeyCacheKey,
RowIndexEntry&gt; weigher = <span class="code-keyword">new</span>
EntryWeigher&lt;KeyCacheKey, RowIndexEntry&gt;()        {            <span
class="code-keyword">public</span> <span class="code-object">int</span>
weightOf(KeyCacheKey key, RowIndexEntry value)            {                <span
class="code-keyword">return</span> key.serializedSize() + value.serializedSize();         
  }        };        ICache&lt;KeyCacheKey, RowIndexEntry&gt; kc =
ConcurrentLinkedHashCache.create(keyCacheInMemoryCapacity,
weigher);</pre></div></div>Using the serialized size for a non-serialized cache looks
fishy to me. 


New Comment: 
Partially committed 0001 d to 1.1.1 and trunk, which will remove the limitation of 2 GB
which this ticket was originally ment to do.<br/>for 0002 i am working on calculating the
object overhead + size without using reflection. Thanks! 


New Comment: 
Before i go ahead with the complicated implementation (touching most of the cache values),
i did a smoke test and thought of sharing the results.lgmac-vparthsarathy:cass
vparthasarathy$ java
-javaagent:/Users/vparthasarathy/Documents/workspace/cassandraT11/lib/jamm-0.2.5.jar -jar
~/Desktop/TestJamm.jar 100000000<br/>Using reflection took: 25954<br/>Using
NativeCalculation took: 178<br/>Using MemoryMeter took: 992<br/>lgmac-vparthsarathy:cass
vparthasarathy$ I used  <a
href="https://github.com/twitter/commons/blob/master/src/java/com/twitter/common/objectsize/ObjectSizeCalculator.java"
class="external-link"
rel="nofollow">https://github.com/twitter/commons/blob/master/src/java/com/twitter/common/objectsize/ObjectSizeCalculator.java</a>
for reflection test. 


New Comment: 
<blockquote>Lets say when we add the entry we have a ratio of 1.2 and when the entry is
removed we will have the ratio of 1.1 then we will leak some space for cache utilization.
So it will be better if this number doesn't change</blockquote>I did some code diving into
CLHM &#8211; it looks like it annotates the cache entry with the weight-at-put-time, and
uses that in remove or replace.  If so, I think MemoryMeter might be fine after all.OTOH
if the cache isn't churning maybe MemoryMeter is just fine the way it is.I'm inclined to
close this and just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys
in our weights.  Thoughts? 


New Comment: 
<blockquote>just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys in
our weights. Thoughts?</blockquote>Done! opened <a
href="https://issues.apache.org/jira/browse/CASSANDRA-4315" title="Use EntryWeigher
instead of Weigher to Measuring the cache." class="issue-link"
data-issue-key="CASSANDRA-4315"><del>CASSANDRA-4315</del></a>... Thanks! 


New Comment: 
I'm having trouble following what happened from this ticket and 4315 ("just open a new
ticket for updating to CLHM 1.3 in 1.2... ").  Isn't
02672936f635c93e84ed6625bb994e1628da5a9b in the 1.1 branch and we already upgraded to CLHM
1.3? <a
href="https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=02672936f635c93e84ed6625bb994e1628da5a9b"
class="external-link"
rel="nofollow">https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=02672936f635c93e84ed6625bb994e1628da5a9b</a> 


New Comment: 
Yes we did upgrade it to CLHM 1.3 in both Cassandra 1.1 and Cassandra 1.2 to fix the
original issue mentioned in the ticket.Ben (author of CLHM) also added EntryWeight&lt;K,
V&gt; to CLHM 1.3 on our request. Which is 0002 part of the patch originally submitted.
<br/>To use EntryWeights we needed to calculate the memory size's (We where trying out
multiple options using MemoryMeter or using other mechanisms) of the Key and Value, we
where trying that in this ticket.Hence <a
href="https://issues.apache.org/jira/browse/CASSANDRA-4315" title="Use EntryWeigher
instead of Weigher to Measuring the cache." class="issue-link"
data-issue-key="CASSANDRA-4315"><del>CASSANDRA-4315</del></a> was opened to continue the
work of updating CLHM Weigher&lt;V&gt; to CLHM EntryWeight&lt;K, V&gt;. 


New Comment: 
It looks like the Maven dependencies in build.xml weren't updated to reference CLHM
1.3:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>...&lt;dependency
groupId="com.googlecode.concurrentlinkedhashmap" artifactId="concurrentlinkedhashmap-lru"
version="1.2"/&gt;...</pre></div></div>As a result, the <a
href="http://mojo.codehaus.org/cassandra-maven-plugin/" class="external-link"
rel="nofollow">Cassandra Maven Plugin</a> blows up if you try to make it use Cassandra
1.1.1.<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>...[ERROR] 10:59:07,698 Exception
encountered during startup[INFO] java.lang.NoSuchMethodError:
com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(J)Lcom/googlecode/concurrentlinkedhashmap/ConcurrentLinkedHashMap$Builder;[INFO]
    at
org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)[INFO]
    at
org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)...</pre></div></div> 


New Comment: 
Thanks! committed to 1.1 and trunk. 


New Comment: 
The Maven plugin issue still exists with Cassandra 1.1.5 


