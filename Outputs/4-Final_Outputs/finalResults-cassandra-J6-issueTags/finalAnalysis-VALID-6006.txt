Pattern changes caused by commit: ced22f76b3a0495257c26d861ab3b85a94727d47

From: Abstract Factory-2
To:   Abstract Factory-3

From: Factory Method-2
To:   Factory Method-3

From: Facade-0
To:   Facade-1

From: Flyweight-4
To:   Flyweight-3


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-6006.txt 

commit ced22f76b3a0495257c26d861ab3b85a94727d47
Author: Vijay Parthasarathy <vijay2win@gmail.com>

    add dclocal read repair to the DC-level
    patch by Vijay and Sylvain Lebresne; reviewed by Sylvain Lebresne for
    CASSANDRA-2506



==================================
 Issue CASSANDRA-2506 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-2506] Push read repair setting down to the DC-level
-----------------

-----------------
Summary: Push read repair setting down to the DC-level
-----------------

-----------------
Issue type: New Feature
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Tue, 19 Apr 2011 16:31:05 +0000
-----------------

-----------------
Resolved at: Fri, 10 Feb 2012 18:52:04 +0000
-----------------

-----------------
Assigned to: Vijay
-----------------

-----------------
Description: 

Currently, read repair is a global setting.  However, when you have two DCs and use one
for analytics, it would be nice to turn it off only for that DC so the live DC serving the
application can still benefit from it.
 

-----------------

-----------------
Comments: 

New Comment: 
It would also be nice if you could specify a different repair rate for intra-DC and
inter-DC repairs. 


New Comment: 
The idea is to make read_repair_chance=x to be a global setting which will allow us to
specify the inter DC repair chance. Users can add use read_repair_chance_options={DC:x,
DC2:x} to tune the amount of RR needed within  a DC.If we dont specify RRCO then global
will take into effect. For a single DC installation users can just rely on RRC 


New Comment: 
On the principle itself, I'm a bit sad that we would add a second configuration options
just for read_repair, especially given that read_repair is becoming much less useful in
1.0. I'm also afraid that the configuration options as implemented by this patch will be a
bit confusing for people. The fact that read_repair_chance concern repair on the whole
cluster while read_repair_chance_options concerns repair local to the cluster in
particular and the fact that read_repair_chance has "priority" over the dc options.That
being said, I agree that distinguishing between read repair that cross dc boundaries and
those that don't have merits, and that the proposed solution does allow for reasonable
scenarios. And I have don't really see a much better solution right off the bat.On the
patches themselves:<ul>	<li>In ReadCallback, in the LOCAL case, we're including all the
'local' nodes, but that's not necessarilly enough to get blockFor endpoints. As is, it
breaks CL.QUORUM and CL.ALL on multi-dc setup.</li>	<li>The patch breaks
DataCenterReadCallback. In the case where we don't read repair, we want to include
blockFor endpoints <b>from the local DC</b>. That last part is crucial, otherwise we're
breaking CL.LOCAL_QUORUM. The patch replace this by a sublist of the initial endpoints,
but even though the endpoints are sorted by the snitch, there is no guarantee that the
blockFor first one will be all of the local DC. It should be the case if the snitch is not
ill configured, but there is no guarantee. That's why DCReadCallback if overriding
preferedEndpoints in the first place.</li>	<li>I would maybe rename
read_repair_chance_options to something like dclocal_read_repair_chance or something like
that.</li>	<li>I think we can probably get rid of READ_REPAIR_TYPE and avoid too much
duplication with DCReadCallback with a small refactor. For instance, we would have
something like  <div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>  private List&lt;InetAddress&gt;
filterEndpoints(List&lt;InetAddress&gt; endpoints)  {      if (resolver instanceof
RowRepairResolver)          return endpoints;      if (!(resolver instanceof
RowDigestResolver))          return preferedEndpoints(endpoints);      double chance =
random.get().nextDouble();      if (cfmd.getReadRepairChance() &gt; chance)         
return endpoints;      Double dcConfigChance =
cfmd.getReadRepairChanceOptions().get(localdc);      if (dcConfigChance != null &amp;&amp;
dcConfigChance &gt; chance)          return localEndpoints(endpoints, localdc);      else 
        return preferedEndpoints(endpoints);  }  </pre></div></div>  Then localEndpoints
would basically picks all the endpoints of the local dc plus enough of other dc to match
blockFor and preferedEndpoints shouldn't have to change at all from what it is now.  That
filterEndpoints would directly be used in the ReadCallback constructor.</li>	<li>For
compatibility sake with thrift, I don't think we should renumber the fields, the new
option should be at the end.</li>	<li>The patch seems to randomly change some field
visibility for no reason I can see (the command and received field for RC are made public
as well as the static random).</li>	<li>In CFMetadata, a default is defined for the new
options, but we still have the line  <div class="preformatted panel" style="border-width:
1px;"><div class="preformattedContent panelContent"><pre>  private Map&lt;String,
Double&gt; readRepairChanceOptions = new HashMap&lt;String, Double&gt;(); // defaults to
null  </pre></div></div>  which will allocate a map that will be overwrited by the defaul
right away (and the default is empty, not null)</li>	<li>For CQL, the keyword is added but
is not used. It would also be nice to update the docs for the cli and CQL while we're at
it.</li></ul> 


New Comment: 
attached patch incorporates all the feedbacks.Sylvain, does read_repair_option sounds
good? i just was trying to find a name close to strategy_options etc... 


New Comment: 
<blockquote>I'm also afraid that the configuration options as implemented by this patch
will be a bit confusing for people. The fact that read_repair_chance concern repair on the
whole cluster while read_repair_chance_options concerns repair local to the cluster in
particular and the fact that read_repair_chance has "priority" over the dc
options.</blockquote>What if we had <tt>read_repair_chance</tt> and
<tt>read_repair_datacenters</tt>?  Not quite as flexible but it covers the known use cases
without being confusing. 


New Comment: 
Update with read_repair_datacenters as an option to have local dc repair settings. 


New Comment: 
<blockquote>What if we had read_repair_chance and read_repair_datacenters? Not quite as
flexible but it covers the known use cases without being confusing.</blockquote>Can you be
more specific on what you mean by that? I don't think you're just proposing a naming
change right? 


New Comment: 
I'm proposing the global r_r_c, and a set of datacenters whose cordinators will apply that
chance.  Coordinators in other DCs would not do RR at all. 


New Comment: 
Also, coordinators doing RR should only include nodes in the read_repair_datacenters set. 


New Comment: 
Hi Jonathan. The problem with this approach is that we wont have control over the
settings... there are apps which needs higher local consistency. How about when the user
says RR on x to be 0 then we can skip RR on that DC even the global repair?<br/>but i am
worried that this will cause more confusion. 


New Comment: 
On one side, I do like the simplicity of Jonathan proposition (from the point of view of
explaining it).<br/>On another side, one thing it doesn't allow is to have some kind of
lower chance to pick a node for RR cross-DC versus intra-DC, which is a concept that makes
sense (I'm less convinced by the actual usefulness of being able to assign a different RR
chance to each DC, but if you have example where it's useful, feel free to share).But the
truth is, the question I want to ask is: is this useful at all in the first
place?<br/>Don't get me wrong, in a pre-1.0 world, it would definitively be useful. But in
1.0 we now have reliable hinted-handoffs. RR is still activated but with a low chance and
that's only cautionary in case we had bumps on the road with the new HH code. But if HH
does work as intended, RR becomes a costly and pretty much useless operation. In that
context, do we really want to add more complication to it? 


New Comment: 
I agree that this is not interesting in 1.1, but there are occasional chances of loosing
of a CL or bad blocks remains. Manual Repair seem to be better in 1.1 too...Currently
enabling RR in Multi dc/region is almost unusable (throughput is very low on the cluster).
so if we support it and if the users use it, it will be better to support RR for local
DC.I like the idea of just 2 settings global and local RR, let me know if you think
otherwise. Thanks! 


New Comment: 
<blockquote>I like the idea of just 2 settings global and local RR</blockquote>I like this
too.  I'm perhaps not quite as optimistic as Sylvain that the new HH code will be
something everyone will want to leave enabled; at the very least there's some teething
problems (<a href="https://issues.apache.org/jira/browse/CASSANDRA-3440" title="local
writes timing out cause attempt to hint to self" class="issue-link"
data-issue-key="CASSANDRA-3440"><del>CASSANDRA-3440</del></a>). 


New Comment: 
incorporated discussed changes with 2 settings read_repair_chance and
dclocal_read_repair_chance. 


New Comment: 
Minor change for Schema backward compatibility <br/>change is to add the default value:
<br/>+    33: optional double dclocal_read_repair_chance=0.0, 


New Comment: 
I want to note that this v3 doesn't handle the case in the description of this ticket.
Basically there has been 2 proposition of enhancements for RR on this
ticket:<ol>	<li>Being able to completely exlude a given DC of any RR</li>	<li>Have a
different chance of repair for intra-DC and inter-DC RR</li></ol>The initial patch from
Vijay was basically handling both. I only regretted that this was resulting in something
imo complicated for the user (but without having a much better initiave to
suggest).Jonathan then proposed to only add a list of DC that would be excluded from RR.
This obviously only takle the 1st improvement above. The v3 chooses to only add a local RR
chance, which only tackle the 2nd improvement above.I'm fine saying RR is still useful and
needs DC-related improvements, but we should first agree on what those are maybe.On the
patch itself:<ul>	<li>Still no handling of KW_DCLOCALREADREPAIRCHANCE in
CreateColumnFamilyStatement.java</li>	<li>If there is not enough node in the local DC for
blockfor and if the first endpoints of the endpoint list are not (all) from the local DC
(possible), then we could end up no read repairing all the node from the local DC even if
the chance &lt; DcLocalReadRepair.</li></ul> 


New Comment: 
Plz find the attachement, Thanks! 


New Comment: 
<blockquote>Basically there has been 2 proposition of enhancements for RR on this
ticket:<ol>	<li>Being able to completely exlude a given DC of any RR</li>	<li>Have a
different chance of repair for intra-DC and inter-DC RR</li></ol></blockquote>You're
right.  I think what we really want is (2) ... on a per-DC basis.  That is, DC A might
want intra-DC RR chance at X and cross-DC chance at Y, but DC B might want them at V and
W.Looking back, I think the main problem with the initial patch was the poorly defined
interation between global read_repair_chance and per-DC settings.  In which case, dropping
the global r_r_c seems like a reasonable simplification, especially as part of 1.1 (which
is where we should be doing new features anyway).If we think per-DC settings are too
complicated in any form, then I'd be okay with the different-cross-DC chance approach as
an 80% solution. 


New Comment: 
Sylvain, I am ok with both, do you think the attached patch is good enough? i can rebase
if needed. 


New Comment: 
Sorry, I kind of forget about this one. I'm good with the last version of the patch of
just adding a dclocal read repair chance. Do you mind rebasing Vijay? 


New Comment: 
Hi Sylvain, plz find the attachement. 


New Comment: 
In the following code:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>if (cfmd.getDcLocalReadRepair() &gt;
chance){    List&lt;InetAddress&gt; address = localEndpoints(ep);    // check if blockfor
more than we have localep's    return address.size() &gt;= blockfor ? address :
ep;}</pre></div></div>we shouldn't default to all the endpoints if we don't have enough in
the current DC to satisfy blockfor. What we should do is include all local endpoint and
adds enough other endpoints to satisfy blockfor, but not more. Attaching 2506_v2 patch
that does this (note that it would almost work to return <tt>ep.subList(0,
Math.min(ep.size(), blockfor))</tt> since ep are sorted by proximity but but the patch
don't use that since it could be possible in theory that some node from other DC sort
before local endpoint if the snitch happens to say so)The rebase was also missing the CQL
code for the new option, v2 includes it (for CQL 2 &amp; 3).Otherwise, this lgtm. 


