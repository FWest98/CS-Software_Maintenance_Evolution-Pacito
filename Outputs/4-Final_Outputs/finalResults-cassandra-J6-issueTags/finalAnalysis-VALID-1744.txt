Pattern changes caused by commit: 5531a7f5db05adad04d393b4ccffb6d87d806ec9

From: Abstract Factory-2
To:   Abstract Factory-3

From: Factory Method-2
To:   Factory Method-3


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-1744.txt 

commit 5531a7f5db05adad04d393b4ccffb6d87d806ec9
Author: Jonathan Ellis <jbellis@apache.org>

    fix race condition in SSTable*Iterator.
    patch by Sylvain Lebresne; reviewed by jbellis for CASSANDRA-1130



==================================
 Issue CASSANDRA-1130 Description 
=======================================

Project: Cassandra
-----------------

-----------------
Title: [CASSANDRA-1130] Row iteration can stomp start-of-row mark
-----------------

-----------------
Summary: Row iteration can stomp start-of-row mark
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Resolved
-----------------

-----------------
Created at: Tue, 25 May 2010 18:13:16 +0000
-----------------

-----------------
Resolved at: Mon, 14 Jun 2010 18:06:50 +0000
-----------------

-----------------
Assigned to: Sylvain Lebresne
-----------------

-----------------
Description: 

Hello,

I am trying to use TTL (timeToLive) feature in SuperColumns.<br/>My usecase
is:
<ul class="alternate" type="square">	<li>I have a SuperColumn and 3
subcolumns.</li>	<li>I try to expire data after 60 seconds.</li></ul>
While Cassandra is
up and running, I am successfully able to push and read data without any problems. Data
compaction and all occurs fine. After inserting say about 100000 records, I stop Cassandra
while data is still coming.

On startup Cassandra throws an exception and won't start up.
(This happens 1 in every 3 times). Exception varies like:
<ul class="alternate"
type="square">	<li>EOFException while reading data</li>	<li>negative value encountered
exception</li>	<li>Heap Space Exception</li></ul>
Cassandra simply won't start up.

Again
I get this problem only when I use TTL with SuperColumns. There are no issues with using
TTL with regular Columns.

I tried to diagnose the problem and it seems to happen on
startup when it sees a Column that is marked Deleted and its trying to read data. Its off
by some bytes and hence all these exceptions.

Caused by: java.io.IOException: Corrupt
(negative) value length encountered<br/>        at
org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:317)<br/>        at
org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)<br/>       
at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:336)<br/>   
    at
org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:285)<br/>      
 at
org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)<br/>
       at
org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)<br/>
       ... 18 more

Let me know if you need more information.

Thanks,<br/>Jignesh
 

-----------------

-----------------
Comments: 

New Comment: 
If you have some script that allows to reproduce, that would be awesome. Alternatively, if
you're able to locate the culprit data file and if the infos are not sensible, providing
the file could help.Are you using latest trunk ? 


New Comment: 
Yes I am using the latest source code from trunk.I have a small java application that
deals with creating schema and populating data.This is what I was able to debug till
now:<br/>The error occurs during deserialization in ColumnSerializer.There is an extra int
byte that needs to be read before ColumnSerializer.java:84. Value of this extra int byte
is "4". Not sure what it stands for.I am not sure from where that byte is set. After
reading that byte, I get the localDeletionTime value.Also this happens when the
DELETION_MASK is set on a record. It works fine for records with EXPIRATION_MASK. I am
thinking that converting a record from EXPIRY to DELETED is causing this error at startup
or something like that.Let me know if you need more information.But you should be able to
reproduce this if you have a SuperColumn and their subColumns with TTL. Stop and start
cassandra after loading some data. It consistently fails to startup 1 out of every 3
times.Jignesh 


New Comment: 
OK. I think I narrowed it down further..The bug may be in
"org/apache/cassandra/db/filter/SSTableSliceIterator.java:getNextBlock() method.See the
while loop on line 233.<br/>Out here, its reading 1 column at a time.As I said before, the
problem is when an Column of Type ExpiringColumn becomes DeletedColumn when time has
expired.In that case, once the Supercolumn whose subcolumns are of type "DELETED" are read
in this while loop, there are some extra bytes that needs to be skipped but instead it
goes in the second iteration in the while loop and tries to read the next column and thats
where all the problem starts.Shouldn't the while loop just read one Column at a time and
then exit. That is what it does when it reads all the bytes. If I put a "break statement"
in the end of while loop after reading a column all works fine as the extra bytes are
skipped during the next read of a Column.I am not sure what is the purpose of this while
loop? but if we break after reading  1 column at a time, all works fine and cassandra
starts up smoothly.This looks similar to issue<br/><a
href="https://issues.apache.org/jira/browse/CASSANDRA-1073" class="external-link"
rel="nofollow">https://issues.apache.org/jira/browse/CASSANDRA-1073</a>Jignesh 


New Comment: 
I did some more testing with the patch that I suggested i.e. skipping extra bytes after
reading the column and exiting the while loop worked fine. I am now able to load and start
cassandra with SuperColumn data with TTL without any problems. 


New Comment: 
this sounds like something you could build a unit test for? 


New Comment: 
But they shouldn't be extra bytes to skip. An expired column becomes a deletedColumn only
<br/>after everything is deserialized. It is expected that we read all and everything that
is written.The while loop you're referring to is here to read all the column (or super
columns) that falls <br/>into one given index range (the index is sparse). If you exit the
while loop, you will just potentially <br/>skip some columns (super columns in your
example). That it makes cassandra start may just be <br/>that it skips the problematic
parts. I'll try to reproduce this tomorrow (but if you want and can share your test code
to make that <br/>easier, feel free to <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>). 


New Comment: 
Sorry but I don't seem able to reproduce this.<br/>I've tried a simple test, inserting
supercolumns with 100 colums in them, <br/>each having a TTL (that I varied from 10
seconds to like 3 minutes).<br/>I typically let it insert over 10000 super columns  (so
around 1 millions ttled <br/>columns) and kill it. I run cassandra again, let it compact,
kill it again, run again, <br/>start insertion again, etc... I tried like 20 times, no
crashes whatsoever.I've tried with a trunk of a week or so ago and then with trunk from 1
hour ago. <br/>Sounds like you have no problem reproducing on your side so .. I don't
know.If you could somehow come up with a unit test that make it crashes or a small
<br/>script test that triggers it, that would be amazing. 


New Comment: 
I checked out the latest source code this morning and I am still able to reproduce it.My
usecase is:<ul class="alternate" type="square">	<li>Start Cassandra</li>	<li>Keep adding
SuperColumns with 3 subcolumns within each SuperColumn. Each subcolumn expires in 35
seconds.</li>	<li>Let cassandra run until you see statements  like "Deleted 
files"</li>	<li>Stop cassandra and try to start and it will give you all the exceptions
that I am talking about.</li></ul>Also I believe ExpiringColumn contains some more data
compared to DeletedColumn. Correct? In my testing I found that length of each
DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was
read there were some more extra bytes at the end of the record which is causing all this
issue?When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or
the old record is marked for deletion by just changing the EXPIRING_MASK to DELETED_MASK.I
will try to produce a junit test case. But one needs to still stop cassandra when one sees
some files being deleted. At that point you will see the error that I am talking
about.Jignesh 


New Comment: 
Sorry but I'm still unable to reproduce.I'm attaching a small insert script (in java)
that, as far as I can see,<br/>seems to do what you say triggers the bug. Could you look
if this fails <br/>on your side. If I haven't understand the steps correctly, would you
mind <br/>updating this test so that it reproduce the bug you see ?<br/>(the script uses
raw thrift and requires a very recent trunk)<blockquote>Also I believe ExpiringColumn
contains some more data compared to DeletedColumn. Correct? In my testing I found that
length of each<br/>DeletedColumn was similar to ExpiringColumn and once a complete
DeletedColumn record was read there were some more extra bytes at the <br/>end of the
record which is causing all this issue?<br/>When you convert a ExpiringColumn to
DeletedColumn, is it in place replacement or the old record is marked for deletion by just
<br/>changing the EXPIRING_MASK to DELETED_MASK.</blockquote>File on disk are not updated
in place. So we never change on disk an<br/>ExpiringColumn to a DeletedColumn. There only
is a small optimisation in the<br/>deserialization code that, after having fully
deserialize an ExpiringColumn,<br/>will return an equivalent DeletedColumn if the column
is expired. But we always<br/>read exactly what we have written (or it's a bug). 


New Comment: 
Hello,I am still able to reproduce the problem consistently. I have attached my JUNIT Test
case which reproduces it 1 out of 3 times.Streps to Reproduce:<ul class="alternate"
type="square">	<li>Run it until it inserts atleaset 200000 records or until you see a
"Deleted files" message on your console.</li>	<li>Stop cassandra while the data is still
coming in.</li>	<li>Start Cassandra and you should get exceptions.</li></ul>If you do not
get exceptions, without deleting the index repeat the above steps.Let me know if you are
able to reproduce the problem.Thanks,<br/>Jignesh 


New Comment: 
Ok, I'm able to reproduce with your test.<br/>I'll have a look at it, thanks 


New Comment: 
Attached fiie should fix the problem. The problem is unrelated to TTL per se but a problem
in row iterations so the <br/>ticket title can be a bit misleading.<br/>Citing irc: <br/> 
"a columnGroupReader mark the file when it is created. Then it reset() when getting a next
block.<br/>   but with the way the row iteration works, a new columnGroupReader is created
(and mark the file) before <br/>   the previous one has retrieved it's block<br/>  (it's
because computeNext() create the next SSTableSliceIterator before getReduced() had
retrieved the actual <br/>   column of the previous one)"<br/>The patch allow for each
columnGroupReader to have it's own mark on the fileBtw, I was unable to reproduce
previously because it's the cache preloading that <br/>trigged the error and I did not use
one in my first tests. Thanks Jignesh for helping find this one. 


New Comment: 
Attached version of the patch rebased against 0.6 


New Comment: 
Attaching a unit test for trunk that fails without the patch and passes with it.It doesn't
fail in 0.6 though. I suspect this is because getRangeSlice doesn't <br/>work in the same
way in 0.6. So I'm not sure how to reproduce in 0.6. But I'll <br/>have a better at how it
works in 0.6 and see if I can have a unit test for it too. 


New Comment: 
Excellent. I guess the changes are in trunk. I will check it out.Thanks,<br/>Jignesh 


New Comment: 
This bug is only present in 0.7. 


New Comment: 
Sorry for the delay applying.  Can you rebase to trunk please? 


New Comment: 
No problem, attaching rebased patches 


New Comment: 
committed, thanks! 


New Comment: 
weird, I totally remember committing this (and editing SSTNI to make it apply) but it
didn't make it to svn.  Probably I forgot git-svn dcommit somehow.really committed this
time. 


