Pattern changes caused by commit: ff85e557fc8f235f480234076f47231cdd8c5bfc

From: Mediator-0
To:   Mediator-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-198.txt 

commit ff85e557fc8f235f480234076f47231cdd8c5bfc
Author: Owen O'Malley <omalley@apache.org>

    HDFS-641. Move all of the components that depend on map/reduce to
    map/reduce. (omalley)



==================================
 Issue HDFS-641 Description 
=======================================

Project: Hadoop HDFS
-----------------

-----------------
Title: [HDFS-641] Move all of the benchmarks and tests that depend on mapreduce to mapreduce
-----------------

-----------------
Summary: Move all of the benchmarks and tests that depend on mapreduce to mapreduce
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Closed
-----------------

-----------------
Created at: Tue, 22 Sep 2009 17:59:13 +0000
-----------------

-----------------
Resolved at: Sat, 14 Nov 2009 00:19:23 +0000
-----------------

-----------------
Assigned to: Owen O'Malley
-----------------

-----------------
Description: 

Currently, we have a bad cycle where to build hdfs you need to test mapreduce and iterate
once. This is broken.
 

-----------------

-----------------
Comments: 

New Comment: 
I propose moving all of the the tests and benchmarks that depend on map/reduce into
map/reduce. That will enable us to build in a linear order.common -&gt; hdfs -&gt;
mapreduce 


New Comment: 
+1 


New Comment: 
We will also need to move block_forensics stuff from hdfs/contrib into map-reduce if we
want to straighten up project dependencies. 


New Comment: 
The current planned move is moving file-system specific tests which seem out of place in
mapreduce to me. If something changes in FS/HDFS, it looks very odd to go and change the
corresponding tests in mapred.Why shouldn't HDFS tests and benchmarks just run with a
specified version of mapreduce instead of the latest version. This will remove the
dependency of building mapreduce every time a HDFS build is needed. If the
tests/benchmarks need something cutting edge in mapreduce, then the corresponding
mapreduce jar can be committed to HDFS at that point of time. Thoughts? 


New Comment: 
That had been the plan, but in practice, it didn't work well. To build, you needed to
compile common, and update the common jars in hdfs and mapred. Then you compile hdfs and
push the jar to mapreduce. Then you compile mapreduce and push to hdfs. Then you compile
hdfs-test and push to mapreduce. Then you compile mapreduce test and push it to hdfs. Then
you run the hdfs tests. Then you run the mapreduce tests. By comparison, if we break the
cycle, we can compile common, test common, compile hdfs, test hdfs, compile mapreduce, and
test mapreduce. Yes, we need to do more work to test hdfs without mapreduce. But this is a
good change. 


New Comment: 
With the approach that I've outlined, we will not have cycles. Building mapreduce doesn't
need common/hdfs build again. This will be similar to what we will do once we have <a
href="https://issues.apache.org/jira/browse/HADOOP-5107" title="split the core, hdfs, and
mapred jars from each other and publish them independently to the Maven repository"
class="issue-link" data-issue-key="HADOOP-5107"><del>HADOOP-5107</del></a> and make
mapreduce work with stable/intended versions of common/hdfs instead of always working on
the latest versions. 


New Comment: 
The problem is that we will have breakage that won't be detected until HDFS is updated.
That would substantially increase the chance of missed problems. 


New Comment: 
<blockquote>The problem is that we will have breakage that won't be detected until HDFS is
updated.</blockquote>A similar problem happens with your proposal too no? Let's say
someone changes some functionality in common/hdfs and do not update the corresponding the
test-case in mapreduce(for e.g.,
org.apache.hadoop.security.authorize.TestServiceLevelAuthorization.java). This will NOT be
detected till mapreduce is built. If we talk of Hudson, then this won't happen till a
mapreduce build is triggered by Hudson, which will be only when some other mapreduce patch
is committed.What do others in the community think of this problem in general? Is it OK to
move these hdfs/common related tests/benchmarks into mapreduce? If not, what are the
alternative suggestions? 


New Comment: 
&gt; If something changes in FS/HDFS, it looks very odd to go and change the corresponding
tests in mapred.I agree with Vinod. Moreover, the change cannot happen atomically because
this will require two patches, one for HDFS and then one for the unit tests in mapred. 


New Comment: 
I think the test cases in question are using map/reduce as a tool to submit jobs and are
really not related to map/reduce. In that sense, it seems like HDFS is still the right
place to keep these tests, no ? 


New Comment: 
&gt; If not, what are the alternative suggestions?Perhaps we just need a higher-level
build file, so that the standard working pattern is something like:<div
class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent
panelContent"><pre>svn co https://svn.apache.org/repos/asf/hadoop/build/trunk .svn co
https://svn.apache.org/repos/asf/hadoop/common/trunk commonsvn co
https://svn.apache.org/repos/asf/hadoop/hdfs/trunk hdfssvn co
https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk mapreduceant test-hdfsant
test-mapreduce</pre></div></div>If you want to test against a different version, then
checkout its branch or tag, or unpack its tarball.  The test-mapreduce and test-hdfs
targets would first build both hdfs and mapreduce, then build and run tests. 


New Comment: 
<blockquote>A similar problem happens with your proposal too no? Let's say someone changes
some functionality in common/hdfs and do not update the corresponding the test-case in
mapreduce(for e.g.,
org.apache.hadoop.security.authorize.TestServiceLevelAuthorization.java). This will NOT be
detected till mapreduce is built. If we talk of Hudson, then this won't happen till a
mapreduce build is triggered by Hudson, which will be only when some other mapreduce patch
is committed.</blockquote>That's correct but this is no cycle if we remove the dependence
from hdfs to mapreduce.BTW, tests like TestServiceLevelAuthorization should be spitted
into individual hdfs and mapreduce tests. 


New Comment: 
Since the tests in question use mapreduce as a <b>tool</b>, it can use a <b>specific</b>
version of mapreduce not the SNAPSHOT version.<br/>common (SNAPSHOT) -&gt; hdfs (SNAPSHOT)
-&gt; mapreduce<br/>mapreduce (specific version) -&gt; hdfs (only for hdfs_with_mr
tests)<br/>Above dependencies don't lead to cycles. The version of mapreduce used by hdfs
tests doesn't need to be latest. It can upgrade to newer version whenever required in the
same fashion as it would upgrade any third party lib.On a side note I think the issue of
cycles only arise if we make the assumption that whenever common or hdfs is built, their
jars should be updated in mapreduce, which is not correct IMO. That defeats the purpose of
project split, no? Since these are different projects, they could have different release
cycles in future. So building and validating the dependencies whenever the base version
changes is not very correct. It should be looked at just like any other third party
dependency. For example the change in third party library doesn't and can't validate that
all projects using it are not broken. It is up to the project using it in case it wants to
upgrade to newer version.<br/>Since at this stage each sub projects can't wait for the
dependent project's changes to be released, they can work with SNAPSHOT versions. (<a
href="https://issues.apache.org/jira/browse/HADOOP-5107" title="split the core, hdfs, and
mapred jars from each other and publish them independently to the Maven repository"
class="issue-link" data-issue-key="HADOOP-5107"><del>HADOOP-5107</del></a>). 


New Comment: 
Yes, using a checked-in specific version of mapreduce should work well. Maybe this is true
for all hdfs-tools that use Stable interfaces of mapreduce. 


New Comment: 
Integrated in Hadoop-Hdfs-trunk #98 (See <a
href="http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk/98/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk/98/</a>) 


New Comment: 
There is an addition to DFSIO benchmark related to append, see <a
href="https://issues.apache.org/jira/browse/HDFS-663" title="DFSIO for append"
class="issue-link" data-issue-key="HDFS-663"><del>HDFS-663</del></a>. I plan to commit it
to branch 0.21. Do you want me also commit it to branch <a
href="https://issues.apache.org/jira/browse/HDFS-641" title="Move all of the benchmarks
and tests that depend on mapreduce to mapreduce" class="issue-link"
data-issue-key="HDFS-641"><del>HDFS-641</del></a>? I just don't want this changes to get
lost in the transition process, wherever it goes. 


New Comment: 
The same question will be with <a
href="https://issues.apache.org/jira/browse/MAPREDUCE-2593" title="Random read benchmark
for DFS" class="issue-link" data-issue-key="MAPREDUCE-2593">HDFS-236</a> if it needs to be
committed before this is resolved. 


New Comment: 
I am going to commit <a href="https://issues.apache.org/jira/browse/HDFS-641" title="Move
all of the benchmarks and tests that depend on mapreduce to mapreduce" class="issue-link"
data-issue-key="HDFS-641"><del>HDFS-641</del></a> to branch 0.21 and branch <a
href="https://issues.apache.org/jira/browse/HDFS-641" title="Move all of the benchmarks
and tests that depend on mapreduce to mapreduce" class="issue-link"
data-issue-key="HDFS-641"><del>HDFS-641</del></a> if there are no objections... 


New Comment: 
Sorry, I meant to say:<br/>I am going to commit <a
href="https://issues.apache.org/jira/browse/HDFS-663" title="DFSIO for append"
class="issue-link" data-issue-key="HDFS-663"><del>HDFS-663</del></a> (FDSIO for append) to
branch 0.21 and branch <a href="https://issues.apache.org/jira/browse/HDFS-641"
title="Move all of the benchmarks and tests that depend on mapreduce to mapreduce"
class="issue-link" data-issue-key="HDFS-641"><del>HDFS-641</del></a> if there are no
objections. 


New Comment: 
Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #20 (See <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/20/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/20/</a>) 


New Comment: 
Should the patch for <a href="https://issues.apache.org/jira/browse/HDFS-587" title="Test
programs support only default queue." class="issue-link"
data-issue-key="HDFS-587"><del>HDFS-587</del></a> (0.21 version) also be part of this bug
(committed to <a href="https://issues.apache.org/jira/browse/HDFS-641" title="Move all of
the benchmarks and tests that depend on mapreduce to mapreduce" class="issue-link"
data-issue-key="HDFS-641"><del>HDFS-641</del></a> branch)? It deals with the tests that
were moved from Hdfs to Mapreduce. 


New Comment: 
Integrated in Hadoop-Mapreduce-trunk-Commit #118 (See <a
href="http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/118/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/118/</a>)<br/>
   . Move all of the components that depend on map/reduce to <br/>map/reduce. (omalley) 


New Comment: 
I just committed this. 


New Comment: 
Integrated in Hadoop-Mapreduce-trunk-Commit #119 (See <a
href="http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/119/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/119/</a>)<br/>
   . Missed a testcase in the move. 


New Comment: 
Integrated in Hadoop-Mapreduce-trunk #143 (See <a
href="http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/143/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/143/</a>)<br/>
   . Missed a testcase in the move.<br/>. Move all of the components that depend on
map/reduce to <br/>map/reduce. (omalley) 


New Comment: 
It is unfortunate that we couldn't make any progress on the complaints we have with this
fix.It is more unfortunate that the fix got committed clearly when no consensus was ever
reached. Sharad/Hemant/Dhruba/Me all were either -1 or not +1 for the fix that got
committed. 


New Comment: 
<b>Sigh</b> I apologize for not closing the loop on the complaints before committing this.
It had been so long since I started the process that I lost track that you still weren't
happy with the solution.Cycles in the dependency graph make everything harder. Until we
get the interfaces locked down more and Common becomes more stable, there will be
cross-project commits. While that is true, we need all 3 trunks (common, hdfs, and
mapreduce) to depend on the current SNAPSHOT of each other. Otherwise, a change a common
will break all of them. Using a fixed version of mapreduce for HDFS testing wouldn't work
because that fixed version would become broken. In short, we would need to depend on a
specific SNAPSHOT version of mapreduce. When would that be updated? Who would update
it?The majority of the move is about the tools and benchmarks using HDFS and MapReduce
that are better served being in MapReduce. Some of the the tests should be recoded without
MapReduce and pushed back into HDFS. However, most of the tests are the distributed
equivalent of the tests still in HDFS and aren't that badly placed in MapReduce.Having a
strict build order without cycles is a huge win, especially for automated build systems
like Hudson running CI builds for us. With <a
href="https://issues.apache.org/jira/browse/HADOOP-5107" title="split the core, hdfs, and
mapred jars from each other and publish them independently to the Maven repository"
class="issue-link" data-issue-key="HADOOP-5107"><del>HADOOP-5107</del></a>, we no longer
need to check in jar files. That makes life far easier for developers. Automated CI builds
are big part of it. 


New Comment: 
&gt; Sharad/Hemant/Dhruba/Me all were either -1 or not +1 for the fix that got
committed.No clear -1 votes were cast, but It's still not too late.  If a committer feels
this is wrong, it can be reverted and discussed further.  We require one +1 review before
commits, which I don't see above, and, even then, committers should still be able to veto
and revert a change for the first few days after it's been committed in case they didn't
have a chance to review it before it was committed. 


New Comment: 
Circular dependencies are inherently evil, and fragile, and checked in jar files doubly
so.  This cycle really needs to be broken for good build/test hygiene.    While it is a
step backwards for HDFS users to have to build and maintain an MR layer to test their
fixes, it's really not much different than before the project split, except that now the
dependency is clearly exposed.Perhaps there's a solution that doesn't involve using MR to
test HDFS?   That would resolve both the circular dependency, and the delayed testing of
HDFS (because you're waiting for MR to build). 


