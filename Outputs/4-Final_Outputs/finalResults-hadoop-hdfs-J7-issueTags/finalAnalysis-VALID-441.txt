Pattern changes caused by commit: 56e0c33a1ddc84d30888b73784b9716d5e3ae135

From: Mediator-0
To:   Mediator-1


=========================
       NEW GIT LOG
=========================

This commit refers to file: VALID-441.txt 

commit 56e0c33a1ddc84d30888b73784b9716d5e3ae135
Author: Konstantin Shvachko <shv@apache.org>

    HDFS-909. Wait until edits syncing is finishes before purging edits. Contributed by Todd Lipcon.



==================================
 Issue HDFS-909 Description 
=======================================

Project: Hadoop HDFS
-----------------

-----------------
Title: [HDFS-909] Race condition between rollEditLog or rollFSImage ant FSEditsLog.write operations  corrupts edits log
-----------------

-----------------
Summary: Race condition between rollEditLog or rollFSImage ant FSEditsLog.write operations  corrupts edits log
-----------------

-----------------
Issue type: Bug
-----------------

-----------------
Current status: Closed
-----------------

-----------------
Created at: Wed, 20 Jan 2010 18:04:32 +0000
-----------------

-----------------
Resolved at: Wed, 21 Apr 2010 03:09:10 +0000
-----------------

-----------------
Assigned to: Todd Lipcon
-----------------

-----------------
Description: 

closing the edits log file can race with write to edits log file operation resulting in
OP_INVALID end-of-file marker being initially overwritten by the concurrent (in
setReadyToFlush) threads and then removed twice from the buffer, losing a good byte from
edits log.

Example:
<div class="code panel" style="border-width: 1px;"><div
class="codeContent panelContent"><pre class="code-java">FSNameSystem.rollEditLog() -&gt;
FSEditLog.divertFileStreams() -&gt; FSEditLog.closeStream() -&gt;
EditLogOutputStream.setReadyToFlush()FSNameSystem.rollEditLog() -&gt;
FSEditLog.divertFileStreams() -&gt; FSEditLog.closeStream() -&gt;
EditLogOutputStream.flush() -&gt;
EditLogFileOutputStream.flushAndSync()ORFSNameSystem.rollFSImage() -&gt;
FSIMage.rollFSImage() -&gt; FSEditLog.purgeEditLog() -&gt; FSEditLog.revertFileStreams()
-&gt; FSEditLog.closeStream() -&gt;EditLogOutputStream.setReadyToFlush()
FSNameSystem.rollFSImage() -&gt; FSIMage.rollFSImage() -&gt; FSEditLog.purgeEditLog()
-&gt; FSEditLog.revertFileStreams() -&gt; FSEditLog.closeStream()
-&gt;EditLogOutputStream.flush() -&gt;
EditLogFileOutputStream.flushAndSync()VERSUSFSNameSystem.completeFile -&gt;
FSEditLog.logSync() -&gt; EditLogOutputStream.setReadyToFlush()FSNameSystem.completeFile
-&gt; FSEditLog.logSync() -&gt; EditLogOutputStream.flush() -&gt;
EditLogFileOutputStream.flushAndSync()OR Any FSEditLog.write</pre></div></div>
Access on
the edits flush operations is synchronized only in the FSEdits.logSync() method level.
However at a lower level access to EditsLogOutputStream setReadyToFlush(), flush() or
flushAndSync() is NOT synchronized. These can be called from concurrent threads like in
the example above

So if a rollEditLog or rollFSIMage is happening at the same time with a
write operation it can race for EditLogFileOutputStream.setReadyToFlush that will
overwrite the the last byte (normally the FSEditsLog.OP_INVALID which is the "end-of-file
marker") and then remove it twice (from each thread) in flushAndSync()! Hence there will
be a valid byte missing from the edits log that leads to a SecondaryNameNode silent
failure and a full HDFS failure upon cluster restart. 

We got to this point after
investigating a corrupted edits file that made HDFS unable to start with 
<div class="code
panel" style="border-width: 1px;"><div class="codeHeader panelHeader"
style="border-bottom-width: 1px;"><b>namenode.log</b></div><div class="codeContent
panelContent"><pre class="code-java">java.io.IOException: Incorrect data format.
logVersion is -20 but writables.length is 768.         at
org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:450</pre></div></div>
EDIT:
moved the logs to a comment to make this readable
 

-----------------

-----------------
Comments: 

New Comment: 
Hi Cosmin,Does this exist in branch-20 as well? or is it a regression in 20 when the edit
log code was changed to support backupnode etc? 


New Comment: 
Hi Todd, I haven't check yet, so it's possible to be on 0.20 as well. I forgot to add that
the issue particularly nasty because it will first fail silently. In our case, the log was
corrupted on 17th of December but we only discovered it yesterday when we restarted HDFS.
It can be detected early by monitoring the secondary-namenode.out log file. 


New Comment: 
I looked briefly over 0.20 branch and it seems it's susceptible to the race condition (not
exactly the same as here). To simplify things a bit from the initial description: in 0.21
FSEditLog methods logSync() and closeStream()  call the setReadyToFlush() freely<br/>in
0.20 FSEditLog.setReadyToFlush() is called by logSync() and close(). Apparently close() is
synchronized and also checking if isSyncRunning is set. However it doesn't stop it to race
for setReadyToFlush() against logSync(), but rather just attempting to wait a second if a
sync is already in progress.I can see how these could be avoided with a shared lock in
both 0.20 and 0.21+, however it might be better to get the code reviewed by someone that
knows the adjacent parts as well too as it currently seems really fragile and prone to
deadlock creation. 


New Comment: 
I don't think I see this in 0.20. logSync is not itself a synchronized method, but there
is a synchronized (this) block around the invocations of setReadyToFlush. 


New Comment: 
Here's a bit of an ugly unit test that I think shows this issue on trunk. I haven't
reproduced exactly Cosmin's behavior, but I do manage to get it to throw an exception
pretty quickly. The test starts up a bunch of threads which log actions, then alternates
(a) roll edit log, (b) verify edits, (c) purge edits.On my system it fails quickly
with:<div class="preformatted panel" style="border-width: 1px;"><div
class="preformattedContent panelContent"><pre>    [junit] java.io.IOException: Bad file
descriptor    [junit]     at sun.nio.ch.FileChannelImpl.position0(Native Method)   
[junit]     at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:263)    [junit]   
 at
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream.flushAndSync(EditLogFileOutputStream.java:145)
   [junit]     at
org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:83)
   [junit]     at
org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:867)    [junit]   
 at
org.apache.hadoop.hdfs.server.namenode.TestEditLogRace$Transactions.run(TestEditLogRace.java:76)
   [junit]     at java.lang.Thread.run(Thread.java:619)</pre></div></div>This isn't the
same behavior Cosmin reported, so I may actually have an error in my test logic (eg not
synchronizing on something that a real FSNamesystem would) 


New Comment: 
Here is a patch that fixes the issue.The included unit test isn't optimal, though it does
tend to fail on the unpatched code, and always passes on the patched code. Since this is a
race condition, it's difficult to force.After this is reviewed, I will post a similar
patch against 0.21. I'll also port back the unit test to 0.20 to make absolutely sure the
issue doesn't occur there. 


New Comment: 
Backported the test to 0.20, it passes just fine since roll and purge both call close(),
which waits for any running syncs to complete first. 


New Comment: 
-1 overall.  Here are the results of testing the latest attachment <br/>  <a
href="http://issues.apache.org/jira/secure/attachment/12431737/hdfs-909.txt"
class="external-link"
rel="nofollow">http://issues.apache.org/jira/secure/attachment/12431737/hdfs-909.txt</a><br/>
 against trunk revision 903906.    +1 @author.  The patch does not contain any @author
tags.    +1 tests included.  The patch appears to include 2 new or modified tests.    +1
javadoc.  The javadoc tool did not generate any warning messages.    +1 javac.  The
applied patch does not increase the total number of javac compiler warnings.    +1
findbugs.  The patch does not introduce any new Findbugs warnings.    +1 release audit. 
The applied patch does not increase the total number of release audit warnings.    +1 core
tests.  The patch passed core unit tests.    -1 contrib tests.  The patch failed contrib
unit tests.Test results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/testReport/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/testReport/</a><br/>Findbugs
warnings: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html</a><br/>Checkstyle
results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/artifact/trunk/build/test/checkstyle-errors.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/artifact/trunk/build/test/checkstyle-errors.html</a><br/>Console
output: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/console"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/211/console</a>This
message is automatically generated. 


New Comment: 
Failing test is unrelated (been failing other patch builds, seems security related) 


New Comment: 
Cosmin, thanks for reporting and investigating this. I want to clarify the
problem.<br/>Looks like that <tt>rollFSImage()</tt> occurred exactly between two
synchronized sections in <tt>FSEditLog.logSync()</tt>, when <tt>logSync()</tt> was
flushing one of the streams.<br/><tt>setReadyToFlush()</tt> is called by
<tt>logSync()</tt> in the first synchronized section, so it cannot race with calls
<tt>setReadyToFlush()</tt> originated in <tt>FSEditLog.divertFileStreams()</tt>, because
the latter is also synchronized.<br/>So the race condition in your case is between<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">FSNameSystem.rollFSImage() -&gt; FSIMage.rollFSImage() -&gt;
FSEditLog.purgeEditLog() -&gt; FSEditLog.revertFileStreams() -&gt; FSEditLog.closeStream()
-&gt;EditLogOutputStream.setReadyToFlush() VERSUSFSNameSystem.completeFile -&gt;
FSEditLog.logSync() -&gt; EditLogOutputStream.flush() -&gt;
EditLogFileOutputStream.flushAndSync()</pre></div></div><tt>setReadyToFlush()</tt> swaps
the buffers <tt>bufCurrent</tt> and <tt>bufReady</tt>, and <tt>flushAndSync()</tt> writes
<tt>bufReady</tt> into the edits file, and it could write inconsistent data if the buffer
has been replaced while writing.Todd, it seems the exception you posted
<tt>java.io.IOException: Bad file descriptor</tt> means that file has been closed before
<tt>flush()</tt> could complete. Is it the same race or a different one.<br/>I haven't
looked at the patch yet.Also, usually the description of the issue should be short.
Detailed explanations can be posted in the first comment after that. Is it possible to
edit the description now and remove those unwrapped long lines? 


New Comment: 
Konstantin: I think it's pretty much the same race. The included unit test should shake
out either one, and it passes now.The patch is pretty simple - revertFileStreams and
divertFileStreams simply need to wait for any thread that's in the unsynchronized middle
part of logSync. 


New Comment: 
Todd: I agree the patch fixes this particular race condition, when rollFSImage() is
involved.<br/>I fear there is a similar race here, which it does not solve<div class="code
panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">FSNameSystem.saveNamespace() -&gt; FSImage.saveFSImage( -&gt;
FSEditLog.createEditLogFile() -&gt; EditLogFileOutputStream.create() -&gt;
EditLogFileOutputStream.setReadyToFlush() VERSUSFSNameSystem.completeFile -&gt;
FSEditLog.logSync() -&gt; EditLogOutputStream.flush() -&gt;
EditLogFileOutputStream.flushAndSync()</pre></div></div>You can probably verify this with
your test.<br/>I was thinking is there a universal way to avoid these type of
errors.<br/>I mean somebody will write a new contravertFileStreams() method and will
forget to waitForSyncToFinish(), then we will have to fix it again.<br/>May be we should
synchronize on the <tt>EditLogFileOutputStream</tt> level, then <tt>flushAndSync()</tt>
and <tt>setReadyToFlush()</tt> will not intervene with each other?<br/>In
<tt>TestEditLogRace</tt> you call <tt>FSEditLog</tt> methods directly avoiding the
<tt>FSNamesystem</tt> layer.<br/>Would it be better to involve the FSNamesystem lock into
the picture? 


New Comment: 
<blockquote>May be we should synchronize on the EditLogFileOutputStream level, then
flushAndSync() and setReadyToFlush() will not intervene with each other?</blockquote>If we
do that, isn't there a worry that each stream could flush at a slightly different
point?<br/>If we do <a href="https://issues.apache.org/jira/browse/HDFS-903" title="NN
should verify images and edit logs on startup" class="issue-link"
data-issue-key="HDFS-903"><del>HDFS-903</del></a> and add some kind of
checksumming/verification to the edit logs (and we should)<br/>then we rely on them
rolling together. (Maybe I'm misunderstanding the locking you're
suggesting?)<blockquote>In TestEditLogRace you call FSEditLog methods directly avoiding
the FSNamesystem layer. Would it be better to involve the FSNamesystem lock into the
picture?</blockquote>I don't think the namesystem lock really matters, since all the write
operations are separated into an internalFoo()<br/>which does the work while synchronized,
followed by the logSync unsynchronized. I was trying to keep the test<br/>as close to a
"unit test" as possible though I did have to use a minicluster anyway.<blockquote>I fear
there is a similar race here, which it does not solve...</blockquote>Let me add a unit
test for this one as well and see what I come up with. I had forgotten about
saveNamespace. Thanks 


New Comment: 
This patch adds a unit test for saveNamespace concurrent with edit ops.There is no race
here because createEditLogFile is creating a new output stream, and thus won't race with
any of the existing streams. The edit log isn't mutated until the image has been dumped
and then the logs are rolled, so it's the same rollEditLogs fixed originally.As for the
danger of future changes missing the subtlety, I added a giant comment at the top of
FSEditLog in the hopes that it will warn people. A different design with multiple locks
(one protecting the buffers, one protecting sync and stream ops) might make it harder to
make this mistake, but I think what we have isn't so bad as long as it's documented. 


New Comment: 
-1 overall.  Here are the results of testing the latest attachment <br/>  <a
href="http://issues.apache.org/jira/secure/attachment/12434498/hdfs-909.txt"
class="external-link"
rel="nofollow">http://issues.apache.org/jira/secure/attachment/12434498/hdfs-909.txt</a><br/>
 against trunk revision 903906.    +1 @author.  The patch does not contain any @author
tags.    +1 tests included.  The patch appears to include 2 new or modified tests.    +1
javadoc.  The javadoc tool did not generate any warning messages.    +1 javac.  The
applied patch does not increase the total number of javac compiler warnings.    +1
findbugs.  The patch does not introduce any new Findbugs warnings.    +1 release audit. 
The applied patch does not increase the total number of release audit warnings.    -1 core
tests.  The patch failed core unit tests.    -1 contrib tests.  The patch failed contrib
unit tests.Test results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/testReport/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/testReport/</a><br/>Findbugs
warnings: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html</a><br/>Checkstyle
results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/artifact/trunk/build/test/checkstyle-errors.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/artifact/trunk/build/test/checkstyle-errors.html</a><br/>Console
output: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/console"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/110/console</a>This
message is automatically generated. 


New Comment: 
Failing tests in test-patch are hudson BS per usual 


New Comment: 
<div class="code panel" style="border-width: 1px;"><div class="codeHeader panelHeader"
style="border-bottom-width: 1px;"><b>completeFile</b></div><div class="codeContent
panelContent"><pre
class="code-java">FFFFFFEC0900000005003F2F68626173652F64656D6F5F5F75736572732F636F6D70616374696F6E2E6469722F3336343035313634362F38333238313438373139303730333137323739000133000D31323631303832363331383335000D313236313038323632383039340008363731303838363400000003F6CBB87EF376E3E600000000040000000000000000039665F9549DE069A5735E00000000040000000000000000039665ADCC71A050B16ABF00000000015A179A0000000000039665066861646F6F700A737570657267726F757001??</pre></div></div>followed
by a rename operation<div class="code panel" style="border-width: 1px;"><div
class="codeContent panelContent"><pre
class="code-java">0100000003003F2F68626173652F64656D6F5F5F75736572732F636F6D70616374696F6E2E6469722F3336343035313634362F3833323831343837313930373033313732373900352F68626173652F64656D6F5F5F75736572732F3336343035313634362F746573742F36393137333831323838333034343734333836000D31323631303832363331383639</pre></div></div>The
first byte of the rename was instead read as part of the completeFile() operation. This
resulted in reading the next operation as 0x00 (OP_ADD) followed by an int (length) which
would have been 0x0000030 which is 768 that was read and failed in the following code<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java"> <span class="code-keyword">case</span> OP_ADD:        <span
class="code-keyword">case</span> OP_CLOSE: {          <span class="code-comment">//
versions &gt; 0 support per file replication</span>          <span class="code-comment">//
get name and replication</span>          <span class="code-object">int</span> length =
in.readInt();          <span class="code-keyword">if</span> (-7 == logVersion &amp;&amp;
length != 3||              -17 &lt; logVersion &amp;&amp; logVersion &lt; -7 &amp;&amp;
length != 4 ||              logVersion &lt;= -17 &amp;&amp; length != 5) {             
<span class="code-keyword">throw</span> <span class="code-keyword">new</span>
IOException(<span class="code-quote">"Incorrect data format."</span>  +                   
                <span class="code-quote">" logVersion is "</span> + logVersion +          
                         <span class="code-quote">" but writables.length is "</span> +    
                               length + <span class="code-quote">".
"</span>);</pre></div></div>Filling the missing byte with a value resulted in correct
interpretation of the 0x01 (OP_RENAME) and correct parsing for the rest of the edits file
(&gt;1MB)This theory is also supported by the NameNode log file from the date the
corruption took place:<div class="code panel" style="border-width: 1px;"><div
class="codeHeader panelHeader" style="border-bottom-width:
1px;"><b>namenode.log</b></div><div class="codeContent panelContent"><pre
class="code-java">2009-12-17 12:43:51,276 INFO
org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from
10.72.17.1622009-12-17 12:43:51,338 INFO org.apache.hadoop.hdfs.StateChange: BLOCK*
NameSystem.addStoredBlock: blockMap updated: 10.72.17.165:50010 is added to
blk_-480585673051114658_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.165:50010|RBW],
ReplicaUnderConstruction[10.72.17.167:50010|RBW]]} size 671088642009-12-17 12:43:51,339
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap
updated: 10.72.17.166:50010 is added to
blk_-480585673051114658_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.165:50010|RBW],
ReplicaUnderConstruction[10.72.17.167:50010|RBW]]} size 671088642009-12-17 12:43:51,342
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock:
/hbase/demo__users/compaction.dir/364051646/8328148719070317279.
blk_-5923234476536534337_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.164:50010|RBW],
ReplicaUnderConstruction[10.72.17.163:50010|RBW]]}2009-12-17 12:43:51,352 INFO
org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated:
10.72.17.167:50010 is added to blk_-480585673051114658_235109{blockUCState=COMMITTED,
primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.165:50010|RBW],
ReplicaUnderConstruction[10.72.17.167:50010|RBW]]} size 671088642009-12-17 12:43:51,833
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap
updated: 10.72.17.163:50010 is added to
blk_-5923234476536534337_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.164:50010|RBW],
ReplicaUnderConstruction[10.72.17.163:50010|RBW]]} size 226814982009-12-17 12:43:51,834
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap
updated: 10.72.17.164:50010 is added to
blk_-5923234476536534337_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.164:50010|RBW],
ReplicaUnderConstruction[10.72.17.163:50010|RBW]]} size 226814982009-12-17 12:43:51,834
INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap
updated: 10.72.17.166:50010 is added to
blk_-5923234476536534337_235109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,
replicas=[ReplicaUnderConstruction[10.72.17.166:50010|RBW],
ReplicaUnderConstruction[10.72.17.164:50010|RBW],
ReplicaUnderConstruction[10.72.17.163:50010|RBW]]} size 226814982009-12-17 12:43:51,835
INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file
/hbase/demo__users/compaction.dir/364051646/8328148719070317279 is closed by
DFSClient_-19897796672009-12-17 12:43:51,835 INFO
org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll FSImage from
10.72.17.1622009-12-17 12:43:51,870 INFO
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=hadoop,hadoop      
ip=/10.72.17.166        cmd=rename     
src=/hbase/demo__users/compaction.dir/364051646/8328148719070317279    
dst=/hbase/demo__users/364051646/test/6917381288304474386      
perm=hadoop:supergroup:rw-r--r--</pre></div></div>The last 3 entries show a completeFile
operation followed by a rollFSIMage operation followed by a rename operation. This is
where most probably the race condition took place.<br/>Synchronizing access to
EditLogOutputStream could fix the problem, however other race or deadlocks may still
occur. 


New Comment: 
@Konstantin I moved the details to a comment and broke it on more lines but I missed the
log entry that really messes up the layout. I can't edit the comment, unfortunately so if
you can please break the log entry lines in my comment to have decent layout on this page.
Sorry and thanks. PS I'll look at the code again to see the race issue you described. 


New Comment: 
<blockquote>This patch adds a unit test for saveNamespace concurrent with edit ops.
</blockquote>Would it be possible to make this test according to JUnit4 conventions? 


New Comment: 
New patch in junit 4 style 


New Comment: 
Todd I checked the race with -saveNamespace and logSync is there.<ul class="alternate"
type="square">	<li>I start the name-node in debugger.</li>	<li>I do -mkdir and stop the
debugger in logSync() just before it does flush.</li>	<li>Then I enter safe mode with
another client</li>	<li>I start saveNamepsace and stop the debugger in
FSImage.saveFSImage() -&gt; FSEditLog.createEditLogFile() -&gt;
EditLogFileOutputStream.create() -&gt; after truncating the file but before writing
LAYOUT_VERSION into it.</li>	<li>Then I let logSync() run.</li>	<li>Then I terminate the
name-node.</li>	<li>After that the name-node wont start, since the edits file is
broken.</li></ul>In short the problem is that we get two file descriptors to the same file
and modifying it simultaneously through two different threads. The results are expected to
be unpredictable.Cosmin, thanks for editing. Comments are not editable, so we will have to
live with what we've got. It's ok now. 


New Comment: 
Canceling patch while I look into this some more. 


New Comment: 
&gt; I added a giant comment at the top of FSEditLog <br/>Minor thing. Could you pls move
the comment down to logSync(). I think it belongs to the implementation of this method
rather than to the class itself. The comment is good. 


New Comment: 
-1 overall.  Here are the results of testing the latest attachment <br/>  <a
href="http://issues.apache.org/jira/secure/attachment/12434606/hdfs-909.txt"
class="external-link"
rel="nofollow">http://issues.apache.org/jira/secure/attachment/12434606/hdfs-909.txt</a><br/>
 against trunk revision 905760.    +1 @author.  The patch does not contain any @author
tags.    +1 tests included.  The patch appears to include 2 new or modified tests.    +1
javadoc.  The javadoc tool did not generate any warning messages.    +1 javac.  The
applied patch does not increase the total number of javac compiler warnings.    +1
findbugs.  The patch does not introduce any new Findbugs warnings.    +1 release audit. 
The applied patch does not increase the total number of release audit warnings.    -1 core
tests.  The patch failed core unit tests.    -1 contrib tests.  The patch failed contrib
unit tests.Test results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/testReport/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/testReport/</a><br/>Findbugs
warnings: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html</a><br/>Checkstyle
results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/artifact/trunk/build/test/checkstyle-errors.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/artifact/trunk/build/test/checkstyle-errors.html</a><br/>Console
output: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/console"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/217/console</a>This
message is automatically generated. 


New Comment: 
Hi Konstantin. Thanks for the in depth exploration of that race.Let me ask a question -
sorry if I'm being dense here <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/>In FSImage.saveFSImage(boolean)
(FSImage.java:1243) we create a new "current" edit log file before rolling the
FSImage.<br/>This seems to be related to the race, and I don't quite understand the design
here. It seems to me that the correct operation<br/>of saveFSImage should be:1. Dump the
image in IMAGE_NEW<br/>2. Create a new empty edit log in EDITS_NEW (while leaving EDITS
alone)<br/>3. Call purgeEditLogs which rolls the new image/edits pair in.Analyzing the
failure cases:<ul class="alternate" type="square">	<li>During step 1, if the NN restarts,
it can recover from the preexisting IMAGE/EDITS as if the saveFSImage never
started</li>	<li>Between step 1 and 2, it can recover from either the preexisting
IMAGE/EDITS or the IMAGE_NEW.<br/>IMAGE_NEW is consistent and up to date because
saveFSImage is synchronized, and thus it represents the exact state<br/>of the
namesystem.</li>	<li>The same goes for step 2 - the empty edit log is correct against the
new log. If we decide to recover from current instead<br/>of NEW at this point it would
also be correct.</li>	<li>We must assume that step 3 performs rolling in a way that allows
recovery - this is what allows rolling to work in general<br/>safely.</li></ul>In the
implementation as I understand it, step 2 is implemented differently such that it calls
createEditLogFile on EDITS, not<br/>just EDITS_NEW. This truncates the edits, which means
we can only recover from the NEW image at this point. This makes<br/>for tricky recovery
logic, since we have an old image with truncated edit log, plus a new image (which we
don't have any great way<br/>of knowing is complete). Additionally, I don't think there's
a guarantee that steps 1 and 2 happen in that order - the order of<br/>saveFSImage is
determined by the order of the storage directories in the directory iterator. It seems
like edits <em>usually</em><br/>come last, but in the case of failed/recovered storage
directories, I don't think it's always true. If the order of step 1 and 2
are<br/>inverted, it could truncate the current edits log before saving the new image, and
thus end up losing all those edits if the<br/>NN failed in between.Am I misunderstanding
something? The simple fix to the issue you suggested above is to add a waitForSyncToFinish
in<br/>createEditLogFile, but I'm worried there may be more subtle issues as described
above.Thanks! 


New Comment: 
Todd, EDITS_NEW is created when the checkpoint starts. <br/>After that all new
transactions will go into EDITS_NEW, while EDITS will remains untouched. <br/>So if you
empty EDITS_NEW in your step (2) you basically wipe out latest transactions in
HDFS.<br/>Does that make sense? 


New Comment: 
I think we're talking about different things. I'm discussing the saveFSImage that is
called by<br/>FSNamesystem.saveNamespace, where you found the race
above.FSImage.saveFSImage has this code:<div class="preformatted panel"
style="border-width: 1px;"><div class="preformattedContent panelContent"><pre>1240      
if (dirType.isOfType(NameNodeDirType.IMAGE))1241         saveFSImage(getImageFile(sd,
NameNodeFile.IMAGE_NEW));1242       if (dirType.isOfType(NameNodeDirType.EDITS)) {1243    
    editLog.createEditLogFile(getImageFile(sd, NameNodeFile.EDITS));1244         File
editsNew = getImageFile(sd, NameNodeFile.EDITS_NEW);1245         if
(editsNew.exists())1246           editLog.createEditLogFile(editsNew);1247      
}</pre></div></div>On line 1243 we truncate EDITS. Then if EDITS_NEW exists, we truncate
it on 1246. All of this happens when<br/>the NN is in safe mode, so there shouldn't be any
new edits coming in in the first place.I'm contending that line 1243 and 1245 should both
be deleted. We should always create the image as<br/>IMAGE_NEW (line 1241). Touching EDITS
seems incorrect - what if the order of storage dirs is EDITS<br/>then IMAGE, so we run
line 1243, kill our current edit log, and then crash before saving the current image?(this
is possibly orthogonal to the issue you raised - even though there are no edits, there can
be an ongoing sync.<br/>I think the NN should call editLog.waitForSyncToFinish before
entering safemode to avoid this issue) 


New Comment: 
&gt; NN is in safe mode, so there shouldn't be any new edits coming in in the first
place.True no new edits, but those started before entering safemode can still run.&gt;
what if the order of storage dirs is EDITS then IMAGE, so we kill our current edit log,
and then crash before saving the current image?This is a reasonable concern.
<br/>Historically fsimage and edits used to be always in the same directory, so we first
saved the image <br/>in the directory and then reset the edits. Now the order may change,
and this was not taken care of.<br/>We should file a new jira for fixing this. 


New Comment: 
I think next step is to write a unit test which tries to trigger the race you found above,
and fix it with an additional<br/>call to waitForSyncToFinish in an appropriate place.
Should be able to get to that this week.I'll open another JIRA for the saveFSImage
behavior described above.Sound good? 


New Comment: 
Sounds good. I think we will need to call <tt>waitForSyncToFinish()</tt> both before
entering safe mode and in <tt>createEditLogFile()</tt>. 


New Comment: 
Unfortunately, I wasn't able to write a unit test that reproduces your race. This is
because the race only<br/>occurs if the NN exits before rollEditLogs() is called at the
end of FSImage.saveFSImage(boolean) &#8211;<br/>the race induces corruiption in EDITS, but
EDITS_NEW is a correct empty file. rollEditLogs thus fixes<br/>up the state of the file.I
think we'll deal with this issue in the other JIRA regarding saveFSImage
operation.<blockquote>think we will need to call waitForSyncToFinish() both before
entering safe mode</blockquote>I think it's actually impossible to be correct here. The
issue with waitForSyncToFinish in enterSafeMode is that many<br/>of the FSNamesystem calls
have a structure that looks like:<div class="code panel" style="border-width: 1px;"><div
class="codeContent panelContent"><pre class="code-java">1 void someOperation() {2   <span
class="code-keyword">synchronized</span> (<span class="code-keyword">this</span>) {3    
<span class="code-keyword">if</span> (!isInSafeMode()) { explode; }4    
internalSomeOperation();5   }6   getEditLog().logSync();7 }</pre></div></div>If we call
enterSafeMode between line 5 and 6, a waitForSyncToFinish would return
immediately,<br/>since the sync isn't running yet. Really this is the case for any of
lines 3-6 since enterSafeMode<br/>is synchronized as well.I think we need an additional
method with stronger guarantees than waitForSyncToFinish - something
like<br/>syncAllOutstandingOperations that waits until synctxid == txid.What do you think? 


New Comment: 
&gt; Unfortunately, I wasn't able to write a unit test that reproduces your race. Yes unit
test is hard here. <br/>But you can reproduce the problem manually, then fix it and verify
the same sequence of steps does not corrupt the edits.<br/>This is a perfectly acceptable
way of doing things imo, and in line with practices in this community.&gt; I think we need
an additional method with stronger guarantees than waitForSyncToFinishWhat if we just call
<tt>getEditLog().logSync();</tt> before entering safe mode. <br/>If it is a synchronized
call it should guarantee all syncs are complete when we enter safe mode. 


New Comment: 
<blockquote>But you can reproduce the problem manually, then fix it and verify the same
sequence of steps does not corrupt the edits.<br/>This is a perfectly acceptable way of
doing things imo, and in line with practices in this community.</blockquote>For less
critical code paths, I agree, but for potential data loss I really would like to figure
out an automatic test here.<br/>I'll take another swing at a unit test - I often find
after a few days away a second try is a lot easier than the first.<blockquote>What if we
just call getEditLog().logSync(); before entering safe mode. </blockquote>That won't work
unless we also write something in the log so that the ThreadLocal transaction ID is
updated, right? 


New Comment: 
<blockquote>But you can reproduce the problem manually, then fix it and verify the same
sequence of steps does not corrupt the edits.</blockquote><blockquote>This is a perfectly
acceptable way of doing things imo, and in line with practices in this
community.</blockquote>Except that there won't be a regression test which might be very
handy when a consequent change will affect the fix in question. And today's fix might
deteriorate silently. 


New Comment: 
Here's a patch which fixes the problem and also includes some more unit tests. It depends
on <a href="https://issues.apache.org/jira/browse/HADOOP-6554"
title="DelegationTokenSecretManager lifecycle is inconsistent" class="issue-link"
data-issue-key="HADOOP-6554"><del>HADOOP-6554</del></a> to pass.One thing I'd like review
comments on:Most of the FSNamesystem calls have the general form:<div class="code panel"
style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">void xxx() {  <span class="code-keyword">synchronized</span> (<span
class="code-keyword">this</span>) {    doXXX(); <span class="code-comment">// mutate
state</span>    getEditLog().logXXX(); <span class="code-comment">// log edits</span>  } 
getEditLog().logSync();}</pre></div></div>FSEditLog.waitForAllOngoingEdits() exists to
wait for any threads that have written edits but not yet synced them. This ensures that
once we enter safe mode, all edits are safely in the log and we can be sure that the log
is no longer changing.However, iff there were an FSNamesystem call that looked like
this:<div class="code panel" style="border-width: 1px;"><div class="codeContent
panelContent"><pre class="code-java">void xxx() {  <span
class="code-keyword">synchronized</span> (<span class="code-keyword">this</span>) {   
getEditLog().logXXX();  }  ...  <span class="code-keyword">synchronized</span> (<span
class="code-keyword">this</span>) {    getEditLog().logXXX();  } 
logSync();}</pre></div></div>then we could deadlock if enterSafeMode() were called in
between the two synchronized sections, since it would hold the FSN lock while waiting for
the logSync(), meanwhile xxx() is waiting to get the lock.Is this likely to be a problem?
Any ideas how to avoid it? 


New Comment: 
I agree with Todd's latest observation. i think we should break up the FSNameSystem lock
into a read lock and a write lock. The FSNamesystem.getWriteLock() will presumably be
exactly similar to the synchronized sections of the current code.<br/>The above two pieces
of code can then we modified as follows:<div class="code panel" style="border-width:
1px;"><div class="codeContent panelContent"><pre class="code-java">void xxx() { 
getWriteLock();  <span class="code-keyword">try</span> {    doXXX(); <span
class="code-comment">// mutate state</span>    getEditLog().logXXX(); <span
class="code-comment">// log edits</span>  } <span class="code-keyword">finally</span> {   
writeUnlock();  }  getEditLog().logSync();}</pre></div></div>and the second piece via:<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">void xxx() {  getReadLock();    ....     getWriteLock()     ....    
<span class="code-keyword">try</span> {      getEditLog().l)ogXXX();    } <span
class="code-keyword">finally</span> {      writeUnlock()    }  ...     getWriteLock();    
 <span class="code-keyword">try</span>  {      getEditLog().logXXX();    } <span
class="code-keyword">finally</span> {      writeUnlock();    }    ...    logSync();   
readUnlock();}</pre></div></div>In short, if a thread is inside any method in
FSNamesystem, it should keep the readlock. The readlock essentially counts the number of
threads executing inside FSNamesystem.The enterSafeMode() can be written as<div
class="code panel" style="border-width: 1px;"><div class="codeContent panelContent"><pre
class="code-java">enterSafeMode() {  getWriteLock();  <span
class="code-keyword">try</span> {   ....... set safemode flags   } <span
class="code-keyword">finally</span> {     writeUnlock()   }}</pre></div></div>Breaking up 
the FSNamesystem lock also has the advantage that it makes the NN code more mult-threaded.
For example, if there are multiple requests to getFileStatus() or getRelicationFactor they
can all execute in parallel. 


New Comment: 
Dhruba: nice idea about switching to an RWLock here. However, I think that's likely to be
a larger project<br/>(or at least out of scope for this issue, which I think we ought to
fix for branch-0.20). Do you think there<br/>are any potential deadlocks in the current
code, given the observation above? i.e is it unsafe to commit<br/>this patch, and then
later do the RWLock conversion to make it less error prone and get the other<br/>benefits
you noted? 


New Comment: 
I agree that the RWlock idea is only for trunk.in 0.20, is there any code paths that does
the following:<div class="code panel" style="border-width: 1px;"><div class="codeContent
panelContent"><pre class="code-java">void xxx() {  <span
class="code-keyword">synchronized</span> (<span class="code-keyword">this</span>) {   
getEditLog().logXXX();  }  ...  <span class="code-keyword">synchronized</span> (<span
class="code-keyword">this</span>) {    getEditLog().logXXX();  } 
logSync();}</pre></div></div>if there re no such code paths, then there is no chance of
deadlock. Otherwise, we migth want a fix for 0.20 


New Comment: 
I thought about this a bit more deeply over dinner <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> I think the potential issue is not in
fact a deadlock,<br/>since waitForAllOngoingEdits is a bit misnamed. In fact, it just
calls through to logSync, so in the<br/>"bad code" example above, what will happen is that
we sync the first synchronized block's edit for it,<br/>and then enter safe mode before
entering the second synchronized block.So, to be correct, code just needs to make sure
that it always checks isInSafeMode() each time it enters<br/>synchronized (this) {...},
before making any edit. Dhruba, do you concur?Switching to the ReadWriteLock would be nice
since we could enforce the safemode check as part of<br/>the lock acquisition. But that's
definitely out of scope.So, I think this is a certain improvement with no deadlock
potential. The error case described above is<br/>no worse than what currently happens. So,
I think the patch is good, and we'll use <a
href="https://issues.apache.org/jira/browse/HDFS-956" title="Improper synchronization in
some FSNamesystem methods" class="issue-link"
data-issue-key="HDFS-956"><del>HDFS-956</del></a> to fix the<br/>"always check safe mode
after entering synchronized blocks" problems. 


New Comment: 
@Todd what's the state of this patch? This happens more often than I initially thought.
Just hit it again. 


New Comment: 
Hi Cosmin,I think the most recent patch is good, though it will need some munging to apply
to branch-20.<br/>If you can apply it and test it out for us, that would be great! 


New Comment: 
@Todd,Thanks! We're using 0.21 <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> 


New Comment: 
I think this problem does not exist in 0.20. Both FSEditLog.close and FSEditLog.logSync
invoke setReadyToFlush() within a synchronized section. 


New Comment: 
One problem I can see in 0.20 is that isSyncRunning is not a volatile variable. Can this
cause a problem? 


New Comment: 
<blockquote>I think this problem does not exist in 0.20. Both FSEditLog.close and
FSEditLog.logSync invoke setReadyToFlush() within a synchronized section. </blockquote>I
also think we're safe from the issue Cosmin is seeing, since close() waits for the sync to
complete before moving on. But we may be susceptible to Konstantin's race,
since<br/>enterSafeMode doesn't have the right wait behavior.I hope to get a chance to
backport my current set of tests to 20 and see what the minimal set of changes are to fix,
if they expose issues.<blockquote>One problem I can see in 0.20 is that isSyncRunning is
not a volatile variable. Can this cause a problem? </blockquote>I don't think so, because
in this case isSyncRunning is only accessed inside synchronized blocks. It may not need to
be volatile even in trunk -<br/>I'll check that out too. 


New Comment: 
I recently experienced the race condition between saveNamespace and logSync methods. The
problem is that the last transaction is still in progress even though the namenode has
entered safemode.<br/>The last transaction in the edits log gets corrupted. 


New Comment: 
OK. I will target a patch against branch-20 as well, then. Would you mind reviewing the
top patch against trunk? 


New Comment: 
I also posted a patch just fr the savenamespace-safemode race via <a
href="https://issues.apache.org/jira/browse/HDFS-988" title="saveNamespace race can
corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a>. appreciate ur feedback on that one. 


New Comment: 
Patch updated against trunk. Verified that without the patch, the unit test fails, and
passes after patch. 


New Comment: 
-1 overall.  Here are the results of testing the latest attachment <br/>  <a
href="http://issues.apache.org/jira/secure/attachment/12440954/hdfs-909.txt"
class="external-link"
rel="nofollow">http://issues.apache.org/jira/secure/attachment/12440954/hdfs-909.txt</a><br/>
 against trunk revision 931338.    +1 @author.  The patch does not contain any @author
tags.    +1 tests included.  The patch appears to include 7 new or modified tests.    +1
javadoc.  The javadoc tool did not generate any warning messages.    +1 javac.  The
applied patch does not increase the total number of javac compiler warnings.    +1
findbugs.  The patch does not introduce any new Findbugs warnings.    +1 release audit. 
The applied patch does not increase the total number of release audit warnings.    -1 core
tests.  The patch failed core unit tests.    -1 contrib tests.  The patch failed contrib
unit tests.Test results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/testReport/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/testReport/</a><br/>Findbugs
warnings: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html</a><br/>Checkstyle
results: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/artifact/trunk/build/test/checkstyle-errors.html"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/artifact/trunk/build/test/checkstyle-errors.html</a><br/>Console
output: <a
href="http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/console"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/304/console</a>This
message is automatically generated. 


New Comment: 
Looks like hudson's just being crazy &#8211; failed tests are unrelated. Do we need to
resubmit? 


New Comment: 
Can someone please take a look  at this blocker? I've verified it still applies against
trunk.<br/>The new tests fail when run against unpatched trunk and pass when I apply the
rest<br/>of the patch. 


New Comment: 
This patch has intersections with <a href="https://issues.apache.org/jira/browse/HDFS-988"
title="saveNamespace race can corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a>. Could you please clarify the
dependency? <br/>As it looks now we need both of them to get it right. <br/>Does Dhruba
plan to continue with <a href="https://issues.apache.org/jira/browse/HDFS-988"
title="saveNamespace race can corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a> or does it make sense to incorporate
them? 


New Comment: 
The patch looks good. A few nits.<ol>	<li>The giant comment should be a javadoc for
logSync(). Right now it is attached to a private variable
<tt>isSyncRunning</tt>.</li>	<li><tt>waitForAllOngoingEdits()</tt> should not be
public.</li>	<li>If I were to choose between names and <tt>logSyncAll()</tt> for the new
method I'd prefer Dhurba's <tt>logSyncAll()</tt>.</li>	<li>Instead of increasing
visibility of <tt>FSEditLog.editStreams</tt> I'd rather introduce getter or setter
methods. You need only for tests, right?</li></ol> 


New Comment: 
Sorry, (3) should read:<br/>3. If I were to choose between names
<tt>waitForAllOngoingEdits()</tt> and <tt>logSyncAll()</tt> for the new method I'd prefer
Dhurba's <tt>logSyncAll()</tt>. 


New Comment: 
Thanks for the review. New patch addresses your comments. 


New Comment: 
+1 for the patch.<br/>The interconnection with <a
href="https://issues.apache.org/jira/browse/HDFS-988" title="saveNamespace race can
corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a> is still not clear, but this should not
block committing this, imo. 


New Comment: 
yep, I will update the patch at 988 once this is committed. 


New Comment: 
&gt; Does Dhruba plan to continue with <a
href="https://issues.apache.org/jira/browse/HDFS-988" title="saveNamespace race can
corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a> or does it make sense to incorporate
them?Todd, if you have the time and energy, please feel free to reassign <a
href="https://issues.apache.org/jira/browse/HDFS-988" title="saveNamespace race can
corrupt the edits log" class="issue-link"
data-issue-key="HDFS-988"><del>HDFS-988</del></a> to yourself. Thanks. 


New Comment: 
I see it passed Hudson<br/><a
href="http://hudson.zones.apache.org/hudson/view/Hdfs/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/153/"
class="external-link"
rel="nofollow">http://hudson.zones.apache.org/hudson/view/Hdfs/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/153/</a><br/>with
one failed test TestDiskError.testLocalDirs.<br/>When I run it on my box it does not fail,
and it failed on Hudson in some other builds. <br/>I don't think this is related to the
patch.Todd, this marked for committing to 0.21 and 0.20. <br/>Could you please compile the
respective patches, and make sure they pass tests on their branches.<br/>I will commit it
then. 


New Comment: 
Here is a patch against branch-0.20. All four tests pass. If I revert the fix but keep the
test, 3/4 of them fail (one times out after an exception causes the edit log to get stuck
in sync and the others fail outright)I am not planning on doing one for branch-0.21, since
as I understand it we are abandoning that branch in favor of a release based on what's now
trunk.One question: the sync() in trunk has a nice fix that adds a finally clause to reset
the isSyncRunning flag in case an exception got thrown. Should we backport this fix as
well? 


New Comment: 
Not sure how much 0.21 is abandoned. I hear people use it with HBase. Here is the patch. 


New Comment: 
Todd, I tried to run TestEditLogRace with your 0.20 patch. It runs forever and finaly
times out.<br/>Feels like it does a lot of transactions. Could you please verify.<br/>In
the trunk the same test runs 42 secs. <br/>Also you have some debug printouts in the
patch, like "===== CLOSE DONE", and you use FSnamesystem.LOG for logging in the
test.<br/>The latter is confising, as then you'd expect a message from FSNamesystem while
it comes from the test. 


New Comment: 
Some more:<br/>What <tt>org.apache.tools.ant.taskdefs.WaitFor</tt> is used for?<br/>And
there is an blank line change at the end of FSEditLog. 


New Comment: 
Hi Konstantin,Thanks for the review. It does seem like the test for branch-20 occasionally
fails - I had it passing here, but it's flaky and<br/>doesn't pass every time. Let me dig
into this and upload a new fixed patch.<blockquote>What
org.apache.tools.ant.taskdefs.WaitFor is used for?</blockquote>No idea where this came
from. I've been trying out Eclipse recently instead of my usual vim, and haven't gotten
used to<br/>clean up after its "smarts" <img class="emoticon"
src="https://issues.apache.org/jira/images/icons/emoticons/smile.png" height="16"
width="16" align="absmiddle" alt="" border="0"/> Will doublecheck the next patch for such
cruft as well. 


New Comment: 
It turns out the test on trunk was flaky as well. The issue was that we were calling
saveNamespace directly on the FSImage while also performing edits from the Transactions
threads. This is exactly the behavior we're trying to avoid by forcing the NN into
safemode first. Also, we were calling verifyEdits() on an edit log that was being
simultaneously written to, which is likely to fail if it reads a partial edit.This patch
against trunk does the following:<ul class="alternate" type="square">	<li>Bumps up the
number of rolls and saves to 30 instead of 10, since obviously 10 wasn't enough to have it
fail reliably.</li>	<li>Replaces use of the FSN log with the test's own
log</li>	<li>Changes the transaction threads to operate via FSN rather than logging
directly to the edit log.</li>	<li>Any exceptions thrown by the edits will cause the test
to properly fail</li></ul>To verify this fix, I temporarily bumped the constants for
number of rolls up to 200 and checked that it passed.This failed sometimes for me without
<a href="https://issues.apache.org/jira/browse/HADOOP-6717" title="Log levels in
o.a.h.security.Groups too high" class="issue-link"
data-issue-key="HADOOP-6717"><del>HADOOP-6717</del></a>, a trivial patch which reduces the
amount of log output from new security code.I'll separately amend the branch-20 patch with
the same changes. 


New Comment: 
FSEditLog,java imports org.apache.tools.ant.taskdefs.WaitFor in your patch for
0.20.<br/>As you see I've already committed the other two branches. So it would be good to
finish this sooner than later. Thanks. 


New Comment: 
Updated branch-20 patch with same changes (plus cleanup of the changes I accidentally left
in before) 


New Comment: 
<blockquote>Not sure how much 0.21 is abandoned. I hear people use it with HBase. Here is
the patch.</blockquote>The plan for HBase 0.20.5 is to work against Tom's new 21 release
or a 20 with <a href="https://issues.apache.org/jira/browse/HDFS-200" title="In HDFS,
sync() not yet guarantees data available to the new readers" class="issue-link"
data-issue-key="HDFS-200"><del>HDFS-200</del></a> applied,<br/>not the current 21 branch.
I checked with Cosmin and he is OK moving to what's now trunk. 


New Comment: 
What is hdfs-909-ammendation.txt for? 


New Comment: 
hdfs-909-ammendation.txt goes with this comment above:<a
href="https://issues.apache.org/jira/browse/HDFS-909?focusedCommentId=12859069&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12859069"
class="external-link"
rel="nofollow">https://issues.apache.org/jira/browse/HDFS-909?focusedCommentId=12859069&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12859069</a>(the
test as committed in trunk is flaky as well, this is a patch against trunk that fixes it.
The bug is just in the test, though, not the code itself) 


New Comment: 
<ul class="alternate" type="square">	<li>The issue is not closed, so it would be better to
have a unified patch, rather than doing 2 commits. I don't mind to recommit.</li>	<li>Test
for 0.20 passes fine now. Found 2 (eclipse) warnings in TestEditLogRace:	<ul
class="alternate" type="square">		<li>Method <tt>getFormattedFSImage()</tt> is not used
anywhere.</li>		<li>Static method <tt>setBufferCapacity()</tt> should be called in static
manner, like <tt>FSEditLog.setBufferCapacity()</tt></li>	</ul>	</li>	<li>I understand
Tom's plan for 0.21. It does not hurt to commit though.</li></ul> 


New Comment: 
Here's a unified patch for trunk (the one you committed to trunk plus the test case
fixes)<br/>Also branch 20 patch that addresses the two eclipse warnings you found. 


